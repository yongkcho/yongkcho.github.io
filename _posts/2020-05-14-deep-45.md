---
title: "[Deep learning]RNN (2) Feedforward"
date: 2020-05-14
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# Lesson 2 Feedforward

CNN을 처음 몇 layer로 사용해 feature extraction으로 쓰고, 뒤에 RNN을 붙여 memory를 사용하게 만들 수도 있습니다.

이런 구조는 Gesture recognition 등에 사용됩니다.

![rnn08](https://drive.google.com/uc?id=1UFaRGFLsEB40Pem5hEyzup3ODheFJn5C)

더 복잡한 것들을 알아보기 전에, 다시 한번 Feedforward에 대해 살펴볼 필요가 있습니다. 


## 1) Feedforward Nural Network

Feedforward의 간단한 구조는 다음과 같습니다. n개의 x vector input을 받아 k개의 y vector output을 출력합니다.

one to many, many to many 또는 many to one의 관계를 가지고 비선형 예측을 할 수 있습니다.

![rnn09](https://drive.google.com/uc?id=1EJ4nTaJ5pzs4InK7Ol1A-5tq3j3GiWh7)

이런 구조로는 **classification**이나 **regression**을 할 수 있습니다. 

feedforward는 아래와 같은 구조이며, 정적(static)입니다. 같은 데이터를 주어주면 같은 결과가 나옵니다.

![rnn10](https://drive.google.com/uc?id=1iOvDb9r98mc-JJoRDuJUMB9hjvuwXpjK)

신경망을 학습시키는 과정은 크게 **1) Training**과 **2) Evaluation**으로 나눠볼 수 있습니다.

Training은 주어진 데이터를 바탕으로 일반화한 네트워크를 산출하는 과정이고, Evaluation은 새로운 input을 넣어 원하는 결과가 나오는지 확인하는 과정입니다.

이 Training과정은 또다시 ** 1) Feedforward**와 **2) Back Propagation**으로 나눠볼 수 있습니다.

Feedforward에서는 output을 출력하고 error를 측정합니다. 그리고 back prop에서는 error를 줄여가는 과정을 거치게 됩니다.

이제 조금 더 수학적인 배경을 살펴보고자 합니다. 여기서는 [선형대수](http://linear.ups.edu/html/section-LC.html)와 [행렬연산](https://en.wikipedia.org/wiki/Matrix_multiplication)에 대한 이해를 가지고 있는 편이 좋습니다.

하나의 hidden layer를 가진 네트워크를 가정해 보겠습니다 먼저 hidden states의 값을 계산하고 다음으로 output을 계산하게 됩니다.

![rnn11](https://drive.google.com/uc?id=1pFDjkcZnzDkDuMeY2z6olql04aAeNCGJ)

![rnn12](https://drive.google.com/uc?id=1hL_wwRVDaiMI2o7IJjoizn1dkpfO_MK1)

이 과정은 다음과 같습니다. input x에 대해 h를 찾는 과정입니다. 

![rnn13](https://drive.google.com/uc?id=1Ij75gRGYjnL9iqshxBSGI65iCZmKkC3l)

여기서 다양한 activation function을 활용하게 됩니다.

![rnn14](https://drive.google.com/uc?id=1ugq0vo0908jFnjXV0Vmu7IoSeb8gg_rf)

activation function을 통해 input과 output의 비선형적인 관계를 찾을 수 있기에, 실제 데이터를 다루는 것이 용이해 집니다.

하지만 이런 과정은 vasnishing gradient 문제를 낳게 되는데, 이는 뒤에서 더 살펴보겠습니다.

activation function에 대해 더 알고싶다면 [이곳](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)을 참고하시면 좋습니다.

이렇게 h를 찾게 되었다면, 이 다음 과정은 output을 찾아가는 것입니다. 앞서와 유사한 방식으로 얻을 수 있습니다.

![rnn15](https://drive.google.com/uc?id=1tgjpUiZ0bv5NQ0Jh3lfRzw--pil5p-_w)

이 과정에서 반드시 activation function이 필요하지는 않지만, 필요시에는 활용할 수도 있습니다. 

이렇게 출력된 y와 실제 값을 비교해 error을 산출하게 됩니다. 여기서 주로 사용되는 것이 [MSE](https://en.wikipedia.org/wiki/Mean_squared_error)나 [cross entropy](https://www.ics.uci.edu/~pjsadows/notes.pdf)입니다. 


## 2) Back Propagation

위와 같이 Feedforward을 하고 나면 이제는 Back Propagation의 차례입니다. 여기서는 [부분 미분](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)에 대해 알고 계시는 것이 좋습니다.

일반적으로 자주 사용되는 미분식들은 [다음](http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf)을 참고해주세요.

Back Propagation을 통해 신경망의 weights를 조정하고(gradient가 양수라면 weights를 감소시키는 등), error를 최소화 할 수 있습니다.

그런데 여기서 적절한 '최소'를 어떻게 알 수 있을까요? 여기에는 정답은 없지만 몇가지 방법이 있습니다. 이후에 살펴보겠습니다.

이 back prop을 더 자세히 살펴보면 다음과 같습니다.

![rnn16](https://drive.google.com/uc?id=1tKCa9_SqIjPM-n9Hykss8uBzQ4c8T2GA)

즉, back prop이 하는 일을 정리해 보면 이렇습니다.

![rnn17](https://drive.google.com/uc?id=1u5KQnNAAvQl29y3sEc_q62B2aVfmhvnr)

learning rate는 어떻게 조정하는게 좋을까요? 이를 더 알고 싶으신 분들은 아래 링크를 참고해 주세요.

> [참고1](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)

> [참고2](https://cs231n.github.io/neural-networks-3/#loss)

overfitting을 막기 위해서는 **1) training process를 빨리 끝내거나**, ** 2) Dropout을 활용한 regulization**이 있습니다.


