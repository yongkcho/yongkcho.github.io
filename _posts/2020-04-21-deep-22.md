---
title: "[Deep learning] Hidden Layer"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 1 Hidden Layer 

이번에는 신경망에서 multilayer perceptron에 대해서 알아보겠습니다. 이를 위해서는 먼저 벡터와 행렬에 대해 알아볼 필요가 있습니다.

* 칸 아카데미의 [벡터강의](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra)

* 칸 아카데미의 [행렬강의](https://www.khanacademy.org/math/precalculus-2018/precalc-matrices)


## 1) Forward Pass

weights인 $w_{ij}$는 두개의 인자를 가지고 있습니다. i는 input을, j는 hidden node를 의미합니다.

예를 들어 아래의 그림은 x1, x2, x3의 3가지 input이 있고, hidden nodes는 h1, h2가 있습니다.

![nn58](https://drive.google.com/uc?id=1WYTPGYEPQBHg8n-PmiNi1tp2PdAvPvXc)

즉, x1이 h1으로 이동하며 사용하는 weights는 w11이고, h2로 갈때는 w12를 사용하게 됩니다.

이를 쉽게 정리하기 위해 행렬을 사용합니다. 각 열은 input에 대한 weights를 의미하고, 행은 hidden에 대한 weights를 의미합니다.

![nn59](https://drive.google.com/uc?id=1t3pWhZy68mdYjgb5uN6tYoKSQam_Z5W4)

이를 코드를 통해 구현하면 다음과 같습니다. 만약 **featrues**가 2차원 array라면,

```python
# input units과 records
n_records, n_inputs = features.shape

# hidden units의 개수
n_hidden = 2

weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))
```

weights_input_to_hidden라는 n_inputs과 n_hidden을 가지는 2차원 array를 만듦니다.

그렇다면 각각의 hidden layer의 $h_{j}$에 대해서 우리는 다음을 계산할 수 있습니다.

$h_{j} = \sum_{i}w_{ij}x_{i} $

위의 예시에서 우리는 input를 wegiths와 행렬곱을 합니다. 

![nn60](https://drive.google.com/uc?id=1krEWOmBX5MX-elXRA2yaXdjdG999JpR4)

이러한 연산을 NumPy에서는 **np.dot**을 통해서 수행합니다.

```python
hidden_inputs = np.dot(inputs, weights_input_to_hidden)
```

이를 통해 다음과 같은 계산을 수행하는 것입니다.

![nn61](https://drive.google.com/uc?id=1cLr3k2ph6fliOuebLvoUUz9fJx3kx-Jx)

여기서 행렬 곱을 하기 위해서 전치를 해야 할수도 있습니다. 그때는 **.T**를 활용합니다. 

```python
np.array(features, ndmin=2)
# array([[ 0.49671415, -0.1382643 ,  0.64768854]])

np.array(features, ndmin=2).T
# array([[ 0.49671415],
       [-0.1382643 ],
       [ 0.64768854]])
```

## 2) Forward Pass Code

전체를 함께 보면 다음과 같은 코드가 됩니다.

```python
import numpy as np

# Activation을 위해 sigmoid 함수를 정의합니다. 
def sigmoid(x):
    return 1/(1+np.exp(-x))

# Network size를 정합니다.
N_input = 4
N_hidden = 3
N_output = 2

# Seed를 정해 디버깅하기 쉬운 환경을 만들어 놓습니다.
np.random.seed(42)

# 임의의 데이터를 만듦니다.
X = np.random.randn(4)

weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))

# forward pass를 수행합니다. 

hidden_layer_in = np.dot(X, weights_input_to_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)
output_layer_out = sigmoid(output_layer_in)

print('Output-layer Output:')
print(output_layer_out)
```

## 3) Backpropagation

이번에는 error를 줄이기 위핸 Backpropagation을 수행해 보겠습니다. 먼저 수식을 하나 보겠습니다.

$\delta _{j}^{h}=$ $$\sum W_{jk}$$ $\delta _{k}^{o}f'$  $$\left ( h_{j} \right )$$

여기서 ${\delta _{k}}^{o}$는 각 k의 output의 error를 의미합니다. 

그리고 j는 output과 hidden layer사이의 weights을 의미합니다. 

gradient descent는 이전과 같지만 새로운 error를 반영해서 다음과 같이 됩니다.

$\Delta \ w _{j}^{h}=$ $$\eta \delta _{j}^{h}x_{i}$$

이 식은 여러개의 input이 있어도 동일하기 때문에 다음과 같이 표현할 수 있습니다.

$\Delta w_{pq} = \eta \delta$ $$_{output} V_{in}$$

$V_{in}$은 input데이터를 의미하고, $\delta _{output}$는 output의 error를 말합니다.

이제 예시를 통해 더 자세히 살펴보겠습니다.

![nn62](https://drive.google.com/uc?id=1QPFoyQHWli5Fzm8ZwS87zIVU2ONQtqkH)

2진 분류를 수행하는 중이고, y= 1이라고 합시다. 먼저 hidden unit의 input은 다음과 같이 계산됩니다. 

$h = \sum_{i}w_{i}x_{i}$ = 0.1 x 0.4 - 0.2 x 0.3 = -0.02

그리고이 hidden unit의 output은 다음과 같습니다.

a = *f(h)* = sigmoid(-0.02) = 0.495

이를 output unit의 input으로 활용한 결과값은 

$\widehat{y} = f\left ( W\cdot  a\right )$ = sigmoid(0.1 x 0.495) = 0.512

이 output을 가지고 우리는 Backpropagation을 수행하려고 합니다.  sigmoid를 활용하면, error는 다음과 같습니다.

$\delta ^{o} = \left ( y - \widehat{y} \right )f'\left ( W \cdot a \right )$ = (1 - 0.512) x 0.512 x (1 - 0.512) = 0.122

그리고 hidden unit의 error term인 $\delta _{j}^{h} = \sum_{k}W_{jk}\delta _{k}^{o}f'\left ( h_{j} \right )$을 활요아면,

$\delta ^{h} = W\delta ^{o}f'\left ( h \right )$ = 0.1 x 0.122 x 0.495 x (1 - 0.495) = 0.003

이제 error를 계산했기 때문에, 이를 gradient descent에 활용할 수 있습니다. learning rate, error  그리고 activation value를 곱합니다.  

$\Delta W = \eta \delta ^{h}a$ = 0.5 x 0.122 x 0.495 = 0.0302

이제 input에서 hidden에서 적용되는 $w_{i}$을 learning rate와 hidden unit error, input value를 곱해 구합니다.

$\Delta w_{i} = \eta \delta^{h} x_{i}$ = (0.5 x 0.0003 x 0.1, 0.5 x 0.003 x 0.3) = (0.00015, 0.00045)

sigmoid는 이렇게 효과적을 error를 줄여주지만, 때로는 지나치게 만이 줄여버리는  vanishing gradient 문제가 이;어나기도 합니다.


## 4) Backpropagation Code

hidden error에 iunput을 바로 곱해보겠습니다.

```python
hidden_error*inputs
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-22-3b59121cb809> in <module>()
----> 1 hidden_error*x

ValueError: operands could not be broadcast together with shapes (3,) (6,) 
```

이런 오류가 발생합니다. 이는 역시 행렬곱의 조건을 만족시키지 못했기 때문입니다. 제대로 계산하려면 다음과 같이 해야 합니다.

```python
hidden_error*inputs[:,None]
# array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],
        [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],
        [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],
        [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],
        [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],
        [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])
		
#또는 hidden_error*inputs.T을 쓸 수도 있지만, 이경우에는 inputs이 1차원이면 문제가 생깁니다.
```

그러면 한번 코드를 보겠습니다.

```python
import numpy as np


def sigmoid(x):

    return 1 / (1 + np.exp(-x))


x = np.array([0.5, 0.1, -0.2])
target = 0.6
learnrate = 0.5

weights_input_hidden = np.array([[0.5, -0.6],
                                 [0.1, -0.2],
                                 [0.1, 0.7]])

weights_hidden_output = np.array([0.1, -0.3])

## Forward pass
hidden_layer_input = np.dot(x, weights_input_hidden)
hidden_layer_output = sigmoid(hidden_layer_input)

output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)
output = sigmoid(output_layer_in)

## Backwards pass
error = target - output

# output layer의 error term를 계산합니다.
output_error_term = error * output * (1 - output)

# hidden layer의 error term를 계산합니다
hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \
                    hidden_layer_output * (1 - hidden_layer_output)

# hidden layer에서 output layer로 갈 때의 weights를 조정합니다.
delta_w_h_o = learnrate * output_error_term * hidden_layer_output

# input layer에서 hidden layer로 갈 때의 weights를 조정합니다. 
delta_w_i_h = learnrate * hidden_error_term * x[:, None]

print('Change in weights for hidden layer to output layer:')
print(delta_w_h_o)
print('Change in weights for input layer to hidden layer:')
print(delta_w_i_h)

# Change in weights for hidden layer to output layer:
# [0.00804047 0.00555918]
# Change in weights for input layer to hidden layer:
# [[ 1.77005547e-04 -5.11178506e-04]
#  [ 3.54011093e-05 -1.02235701e-04]
#  [-7.08022187e-05  2.04471402e-04]]

```

## 5) Further reading

더 알아보기 위해서는 다음과 같은 글을 읽어보면 좋습니다.

* [Yes, you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)
* [a lecture from Stanford's CS231n course](youtube.com/watch?v=59Hbtz7XgjM)

