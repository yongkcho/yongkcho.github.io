
# Convolutional Autoencoder

MNIST dataset을 활용해 다시한번 encoder를 만들어보겠습니다. 이번에는 convolutional layer를 통해 퍼포먼스를 높여보겠습니다.

> encoder는 convolutional과 pooling layer로 구성되어 있고, decoder는 **transpose convolutional layers**를 사용해 압축된 image를 'upsample'하게 구성해야 합니다.

<img src='notebook_ims/autoencoder_1.png' />


```python
import torch
import numpy as np
from torchvision import datasets
import torchvision.transforms as transforms

# convert data to torch.FloatTensor
transform = transforms.ToTensor()

# load the training and test datasets
train_data = datasets.MNIST(root='data', train=True,
                                   download=True, transform=transform)
test_data = datasets.MNIST(root='data', train=False,
                                  download=True, transform=transform)
```

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Processing...
    Done!
    


```python
# Create training and test dataloaders

num_workers = 0
# how many samples per batch to load
batch_size = 20

# prepare data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)
```

### Visualize the Data


```python
import matplotlib.pyplot as plt
%matplotlib inline
    
# obtain one batch of training images
dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()

# get one image from the batch
img = np.squeeze(images[0])

fig = plt.figure(figsize = (5,5)) 
ax = fig.add_subplot(111)
ax.imshow(img, cmap='gray')
```




    <matplotlib.image.AxesImage at 0x7fdc5c3df0f0>




![png](autoen_2_files/autoen_2_4_1.png)


---
## Convolutional  Autoencoder

#### Encoder
신경망의 encoder는 일반적인 convolution pyramid입니다. 각 convolutional layer는 차원을 줄이기 위해 max-pooling layer와 함께 이어져 있습니다.

#### Decoder
decode는 representation을 더 거대한 reconstructed된 image로 변환합니다. 예를들어, representation이 7 x 7 x 4일 수 있습니다. 이는 encoder에서는 output이고, decoder에서는 input입니다. decoder 이후 28 x 28 x 1의 image를 얻어야 하기 때문에 압축된 이미지를 복원하는 것이 필요합니다. 이 구조는 아래와 같습니다.

<img src='notebook_ims/conv_enc_1.png' width=640px>

마지막 encoder layer의 크기는 7 x 7 x 4 = 196입니다. 원본 이미지의 크기는 28 x 28 = 784입니다. 즉, 원본이미지의 25% 정도 되는 크기입니다. 이 구조의 목적은 input data의 small representation을 찾는 것 입니다. 

### Transpose Convolutions, Decoder

이 decoder는 **transposed convolutional** layers를 사용해 input layer의 길이와 높이를 증가시킵니다. 이는 convolutional layer와 유사하지만, 반대로 작동합니다. 3x3 커널을 사용하는 경우, input layer의 3x3 patch가 convolutional layer의 하나의 unit으로 줄어듭니다. 반대로 input layer의 한 unit은 transposed convolutional에서 3x3으로 확장됩니다. PyTorch에는 이 기능을 수행하는 [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/nn.html#convtranspose2d)가 있습니다. 

Trasnposed convolutional layer는 최종 이미지를 바둑판처럼 만들어버릴 수도 있습니다. 이는 kernel이 오버랩되어 발생하는 현상이며, stride와 kernel 크기를 동일하게 설정하면 피할 수 있습니다. Augustus Odena, *et al*이 저술한 [이 문서](http://distill.pub/2016/deconv-checkerboard/)를 보면 이런 문제를 피하는 방식에 대해 더 잘 알수 있습니다. 가까운 이웃이나 이중선을 사용해 layer크기를 조정하는 방식이나 interpolation을 제시하고 있습니다. 


```python
import torch.nn as nn
import torch.nn.functional as F

# define the NN architecture
class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        ## encoder layers ##
        # conv layer (depth from 1 -> 16), 3 x 3 kernel
        self.conv1 = nn.Conv2d(1, 16, 3, padding = 1)
        # conv layer (depth from 16 --> 4), 3x3 kernels
        self.conv2 = nn.Conv2d(16, 4, 3, padding = 1)
        # pooling layer를 사용해 x-y의 차원을 2로 축소합니다; kernel과 stride는 2입니다.
        self.pool = nn.MaxPool2d(2, 2)
        
        ## decoder layers ##
        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride = 2)
        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride = 2)

    def forward(self, x):
        ## encode ##
        # add hidden layer (relu 사용)
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        # second hidden layer
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        
        ## decode ##
        ## transpose layer를 추가합니다.
        x = F.relu(self.t_conv1(x))
        x = F.sigmoid(self.t_conv2(x))
                
        return x

# NN을 initialize합니다.
model = ConvAutoencoder()
print(model)
```

    ConvAutoencoder(
      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (t_conv2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))
    )
    

---
## Training

이제 신경망을 훈련시켜 보겠습니다. 지난번과 같이 validation까지 보지 않고 training과 test loss만 확인해 보겠습니다. class label은 필요 없고, input과 output 만 비교하면 됩니다. 회귀는 확률값보다 _quantities_ 를 비교합니다. `MSELoss`를 사용해 입력과 출력 이미지를 비교하겠습니다.


```python
# loss function 정의합니다.
criterion = nn.MSELoss()

# optimizer 정의합니다.
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```


```python
# epoch 회수를 정의합니다.
n_epochs = 30

for epoch in range(1, n_epochs+1):
    #  training loss를 모니터링 합니다.
    train_loss = 0.0
    
    ###################
    # train the model #
    ###################
    for data in train_loader:
        # _ 는 label을 의미합니다.
        # flattening은 필요 없습니다.
        images, _ = data
        # 모든 optimized variables에서 gradients를 지웁니다.  
        optimizer.zero_grad()
        # forward pass
        outputs = model(images)
        # loss를 계산합니다. 
        loss = criterion(outputs, images)
        # backward pass
        loss.backward()
        # single optimization step (parameter를 update합니다)
        optimizer.step()
        # training loss를 업데이트합니다.
        train_loss += loss.item()*images.size(0)
            
    # training statistics을 출력합니다.
    train_loss = train_loss/len(train_loader)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
        epoch, 
        train_loss
        ))
```

    Epoch: 1 	Training Loss: 0.472627
    Epoch: 2 	Training Loss: 0.218401
    Epoch: 3 	Training Loss: 0.201917
    Epoch: 4 	Training Loss: 0.192231
    Epoch: 5 	Training Loss: 0.185351
    Epoch: 6 	Training Loss: 0.181281
    Epoch: 7 	Training Loss: 0.178624
    Epoch: 8 	Training Loss: 0.176813
    Epoch: 9 	Training Loss: 0.175137
    Epoch: 10 	Training Loss: 0.173825
    Epoch: 11 	Training Loss: 0.172679
    Epoch: 12 	Training Loss: 0.171734
    Epoch: 13 	Training Loss: 0.170946
    Epoch: 14 	Training Loss: 0.170285
    Epoch: 15 	Training Loss: 0.169695
    Epoch: 16 	Training Loss: 0.169163
    Epoch: 17 	Training Loss: 0.168620
    Epoch: 18 	Training Loss: 0.168071
    Epoch: 19 	Training Loss: 0.167654
    Epoch: 20 	Training Loss: 0.167267
    Epoch: 21 	Training Loss: 0.166892
    Epoch: 22 	Training Loss: 0.166518
    Epoch: 23 	Training Loss: 0.166132
    Epoch: 24 	Training Loss: 0.165693
    Epoch: 25 	Training Loss: 0.165171
    Epoch: 26 	Training Loss: 0.164674
    Epoch: 27 	Training Loss: 0.164209
    Epoch: 28 	Training Loss: 0.163755
    Epoch: 29 	Training Loss: 0.163305
    Epoch: 30 	Training Loss: 0.162873
    

## Checking out the results

결과를 확인해 보겠습니다. 가장자리가 뭉개지는 현상이 약간 발생한 것 같습니다.


```python
# test images에서 한 batch를 가져옵니다.
dataiter = iter(test_loader)
images, labels = dataiter.next()

# sample output을 가져옵니다.
output = model(images)
# images를 표시하기 위한 준비를 합니다. 
images = images.numpy()

# output를 resize해 batch의 images와 일치시킵니다.
output = output.view(batch_size, 1, 28, 28)
output = output.detach().numpy()

# 첫 10개  이미지를 확인합니다.
fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))

# input은 위에, reconsturcted된 이미지는 아래에 plot해 비교합니다.
for images, row in zip([images, output], axes):
    for img, ax in zip(images, row):
        ax.imshow(np.squeeze(img), cmap='gray')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
```


![png](autoen_2_files/autoen_2_11_0.png)

