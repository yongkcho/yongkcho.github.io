---
title: "[Deep learning]Deploying a Model (5) Deployment and Modeling
"
date: 2020-07-20
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson 5 Characteristics of Deployment and Modeling

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

## 1) Characteristics of Deployment and Modeling

![006](https://drive.google.com/uc?id=19LtXm9TGnPWrSqoyt9FCnYXyXqZ4zaTu)

 
production deployment를 쉽게 생각하면 머신러닝 모델을 기존 프로덕션 환경에 통합하여 이 모델에 대한 데이터기반의 의사 결정을 하거나 예측에 모델을 사용할 수 있는 방법으로 생각할 수 있습니다.

또한 프로덕션 환경은 현재 많은 사람들이 사용하는 웹, 모바일 또는 기타 소프트웨어 응용 프로그램으로 생각할 수 있으며 이러한 사용자의 요청에 신속하게 응답할 수 있게 해 줍니다.

이 점을 염두해 두고 이번 포스팅에서 소개할 배포 및 모델링의 여러 가지 특성을 살펴봅시다. 

또한 이런 개념은 고유 코드로 구현 된 것보다 클라우드 플랫폼 서비스에서 보다 쉽게 ​​사용할 수 있습니다.

![]()

## 2) Characteristics of Modeling

**Hyper parameter**

머신러닝에서 하이퍼 파라미터는 데이터에서 값을 설정할 수 없는 매개 변수입니다.

즉, 하이퍼 파라미터는 estimator를 통해 직접 학습되지 않습니다. 따라서  개발자가 직접 값을 설정해야합니다.

이는 최적화를 위한 하이퍼 파라미터 튜닝이 모델 훈련에서 중요한 부분을 차지함을 의미합니다.

클라우드 플랫폼의 머신 러닝 서비스는 종종 모델 훈련에 사용할 튜닝을 자동으로 하게 도와줍니다.

머신 러닝 플랫폼이 자동 하이퍼 파라미터 튜닝을 제공하지 못하는 경우 한 가지 옵션은 하이퍼 파라미터 튜닝을 위해 scikit-learn Python 라이브러리의 메소드를 사용하는 것입니다. Scikit-learn은 하이퍼 파라미터 튜닝에 도움이되는 메소드를 포함하는 무료 라이브러리입니다.

![]()

## 3) Characteristics of Deployment

**모델 버전 관리**

배포에서 가장 큰 특징 중 하나는 배포 할 모델의 버전입니다.

배포 플랫폼은 데이터베이스에 모델 버전을 메타 데이터의 일부로 저장하는 것 외에도 배포 된 모델의 버전을 표시 할 수 있어야합니다.

이렇게하면 배포 된 모델을보다 쉽게 ​​유지 관리, 모니터링 및 업데이트 할 수 있습니다.

**모델 모니터링**

또 다른 특징은 배포 된 모델을 쉽게 모니터링 할 수 있다는 것입니다.

모델이 배포되면 성능 메트릭을 충족할 수 있는지 계속해서 확인해야합니다. 만약 충족하지 않는다면 응용 프로그램을 더 나은 성능의 모델로 업데이트해야 할 수 있습니다.

**모델 업데이트 및 라우팅**
배포 된 모델을 쉽게 업데이트 할 수있는 기능은 또 다른 특징입니다. 성능이 부족하면 모델을 업데이트해야합니다.

모델에 입력되는 데이터에 근본적인 변화가 있다면, 이 입력 데이터를 수집해 모델을 업데이트 할 수 있습니다.

배포 플랫폼은 다른 비율의 사용자 요청을 배포 된 모델로 라우팅하는 것을 지원해야합니다. 이렇게 배포된 모델간의 성능을 비교할 수도 있습니다.

이러한 방식은 다른 모델과 비교하며 성능을 테스트 할 수도 있습니다.

**모형 예측**

배포의 또 다른 특징은 배포 된 모델에서 제공하는 예측의 유형입니다. 대표적으로는  On-demand prediction과 Batch prediction 두 가지가 있습니다.

* On-demand prediction은 

- 온라인,

- 실시간,

- 동기화된 예측이라고 부르기도 합니다.

* 이러한 유형의 예측은 다음과 같은 것들을 기대할 수 있습니다.

- 각 예측 요청에 대한 낮은 응답 대기 시간을 가집니다. 

- 하지만 요청양의 변동성이 높을 수도 있습니다.

* 요청에 대한 응답으로는 예측값이 출력됩니다. 요청 및 응답은 종종 JSON 또는 XML 형식의 문자열을 사용하는 API를 통해 수행됩니다.

* 사용자의 각각의 예측 요청은 예측에 대한 하나 이상의 요청을 포함 할 수 있습니다. 요청은 전송 된 데이터의 크기에 따라 제한될 수도 있습니다. 일반적인 클라우드 플랫폼 주문형 예측 요청 크기 제한은 1.5 (ML 엔진)에서 5MB (SageMaker)사이에서 설정됩니다.

**On-demand prediction**은 일반적으로 배포 된 모델을 기반으로 고객, 사용자 또는 직원에게 실시간 온라인 응답을 제공하는 데 사용됩니다. 이번 웹 애플리케이션 예제에서, 사용자는 이런 형태의 요청을 합니다.

**Batch Predictions**

* Batchd prediction은

- 비동기식,

- 배치 기반 예측이라고 부릅니다.

이러한 유형의 예측을 통해 다음을 기대할 수 있습니다.

* 보다 정기적 인 전달로 많은 양의 요청을 처리할 수 있습니다.

* 대기 시간은 문제가 되지 않습니다.

각 배치 요청은 특정 형식의 요청 데이터 파일을 가리키며, 예측 결과를 파일로 반환합니다. 클라우드 서비스에서는 이러한 파일이 플랫폼 공급자의 클라우드에 저장되어야합니다.

클라우드 서비스는 일반적으로 스토리지 서비스에 저장할 수있는 파일 크기에 대한 제한에 따라 일괄 요청으로 처리 할 수있는 데이터 양에 제한이 있습니다. 예를 들어 Amazon의 SageMaker는 배치 예측 요청을 S3 스토리지 서비스의 객체에 적용하는 크기 제한으로 제한합니다.

배치 예측은 일반적으로 비즈니스 결정을 내리는 데 사용됩니다. 예를 들어, 비즈니스에서 복잡한 모델을 사용하여 여러 제품에 대한 고객 만족도를 예측하고 매주 보고서에 이러한 추정치가 필요하다면, 이를 위해서는 매주 배치 예측 요청을 통해 고객 데이터를 처리해야합니다.