---
title: "[Deep learning]RNN (5) Character-Level LSTM in PyTorch"
date: 2020-05-14
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

#  Lesson 5 Character-Level LSTM in PyTorch

PyTorch를 사용해 문자 단위의 LSTM을 만들어 보겠습니다. 신경망은 영문 텍스트의 문자 단위로 학습하고, 그 뒤에 새로운 텍스트를 역시 문자 단위로 생성합니다. 이 코드에서는 안나 카레리나를 학습시키고, 이 문자를 바탕으로 새로운 텍스트를 만들어 보겠습니다.

이 신경망은 Andrej Karpathy의 [RNN 포스팅](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)과 [implementation in Torch 포스팅](https://github.com/karpathy/char-rnn)을 참고하였습니다.

![rnn30](https://drive.google.com/uc?id=1dKnhavv0k_9HEPV-Cg9mxSeNnGthrXvk)

모델을 만들기 위해 필요한 모듈을 import합니다.


```python
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
```

## 1) Load in Data

안나 카레리나 텍스트를 불러오고 숫자로 변환해 신경망에서 사용할 수 있게 합니다.


```python
# 텍스트 파일을 불러옵니다.
with open('data/anna.txt', 'r') as f:
    text = f.read()
```

먼저 100개의 단어를 확인해 보겠습니다. [American Book Review](http://americanbookreview.org/100bestlines.asp)에서는 책을 시작하는 훌륭한 문장 중 하나로 이를 꼽았다고 합니다.


```python
text[:100]
```




    'Chapter 1\n\n\nHappy families are all alike; every unhappy family is unhappy in its own\nway.\n\nEverythin'



## 2) Tokenization

아래 코드에서 **dictionaries**를 만들어 문자를 integer로 바꿔 신경망에 사용합니다.


```python
# 텍스트를 인코딩하고 각 문자를 integer에 매핑하고, 반대로도 매핑합니다.

# 2개의 dictonary를 매핑합니다:
# 1. int2char는 정수형을 문자열로 매핑합니다.
# 2. char2int는 문자열을 정수형으로 매핑합니다.
chars = tuple(set(text))
int2char = dict(enumerate(chars)) #index와 컬렉션 원소를 반환하기 위해 enumerate 사용
char2int = {ch: ii for ii, ch in int2char.items()}

# encode the text
encoded = np.array([char2int[ch] for ch in text])
```

이제 위의 문장이 정수열로 바뀐 것을 알 수 있습니다.


```python
encoded[:100]
```




    array([60, 18, 56, 75, 59, 64, 24, 68,  7, 57, 57, 57, 53, 56, 75, 75, 71,
           68, 37, 56, 13, 15, 48, 15, 64, 20, 68, 56, 24, 64, 68, 56, 48, 48,
           68, 56, 48, 15, 17, 64, 29, 68, 64, 49, 64, 24, 71, 68,  0, 27, 18,
           56, 75, 75, 71, 68, 37, 56, 13, 15, 48, 71, 68, 15, 20, 68,  0, 27,
           18, 56, 75, 75, 71, 68, 15, 27, 68, 15, 59, 20, 68, 44, 58, 27, 57,
           58, 56, 71, 80, 57, 57, 78, 49, 64, 24, 71, 59, 18, 15, 27])



## 3) Pre-processing the data

RNN 이미지에서 볼 수 있듯이, LSTM은 **one hot encoding**이 되어 정수형으로 변환된 데이터를 input으로 받습니다. 이렇게 해당 정수만 '1'이고 나머지 벡터는 '0'으로 채워지는 열 벡터로 변환해 사용해야 합니다. 이를 수행하는 기능을 만들어보겠습니다.


```python
def one_hot_encode(arr, n_labels):
    
    # encoded array를 초기화합니다.
    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)
    
    # 적절한 값을 '1'로 바꿔줍니다.
    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.
    
    # 다시 Original array로 reshape합니다. 
    one_hot = one_hot.reshape((*arr.shape, n_labels))
    
    return one_hot
```


```python
# 제대로 작동하는지 확인해 보겠습니다.
test_seq = np.array([[3, 5, 1]])
one_hot = one_hot_encode(test_seq, 8)

print(one_hot)
```

    [[[ 0.  0.  0.  1.  0.  0.  0.  0.]
      [ 0.  0.  0.  0.  0.  1.  0.  0.]
      [ 0.  1.  0.  0.  0.  0.  0.  0.]]]
    

## 4) Making training mini-batches


data를 학습하기 위해서 훈련을 위한 mini-batche를 만들어야 합니다. 이는 다음과 같은 모습입니다.

![rnn31](https://drive.google.com/uc?id=1Bi8Qp1TKP8fGrSvPYVPy2ZB_o-k7bDSu)

<br>

위의 예시에서 인코딩된 문자는 ('arr' parameter로 전달되는)

In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long.


## 5) Creating Batches

**1. 먼저 텍스트의 일부를 제거해 mini batch를 만듭니다 **

각 batch는 $N \times M$ 개의 문자가 포함됩니다. $ N $는 batch 크기 (batch의 시퀸스 수)이며, $M$은 시seq_length  즉, 시퀸스 내의 tme stamp 수와 동일합니다. 그리고 배열 'arr'에서 만들  수 있는 batch의 총 개수 $K$를 얻기 위해  `arr` 의 길이를 배치 당 문자 수로 나눕니다. batch의 수를 알면 `arr`, $N * M * K$을 통해 문자의 개수를 얻을 수 있습니다.

**2. 그 뒤 arr을 $N$ 배치로 분할합니다 ** 

`arr.reshape(size)`를 사용해 이 작업을 수행할 수 있습니다. 여기서 `size` 는 재구성된 array의 크기를 포함하는 tuple입니다. 일괄적으로 $N$ 시퀸스가 필요하기 때문에 첫 번째 차원의 차원을 먼저 정합니다. 두번째 차원은 크기에서  `-1` 을 자리를 표현하는 placeholder로 사용할 수 있으며, array에 적절한 데이터를 채웁니다. 이 후를 위해 $N \times (M * K)$.array가 필요합니다. 

**3.이 array를 반복해 mini batch를 얻을 수 있습니다 **

각 batch는 $N \times (M * K)$ 위의 $N \times M$ 의 window입니다. 각 후속 batch에 대해 window는 `seq_length`만큼 움직입니다. 동시에 입력과 타겟 array를 만들어야 합니다. 타겟은 한 문자씩 이동해 만든 input입니다. 이를 위해  `range` 를 사용해 $0$에서 `arr.shape[1]`까지의 `n_steps` 의 크기를 단위로 취합니다.  `range` 에서 얻는 정수는 항상 batch의 시작을 가르키고, 각 window의 너비는 `seq_length`입니다.


```python
def get_batches(arr, batch_size, seq_length):
    '''batches의 size를 반환하는 함수입니다.
       arr의 batch_size x seq_length입니다.
       
       Argument는 다음과 같습니다. 
       ---------
       arr: batch를 만드려는 array
       batch_size: bacth의 크기, batch당 시퀸스의 수
       seq_length: 시퀸스 내 인코딩된 문자 수  
    '''
    
    batch_size_total = batch_size * seq_length
    
    # 전체 batche의 수
    n_batches = len(arr)//batch_size_total
    
    # full batche를 만들기 위해 필요한 문자만 남깁니다
    arr = arr[:n_batches * batch_size_total]
    
    # batch_size의 행으로 resahpe합니다.
    arr = arr.reshape((batch_size, -1))
    
    # array를 한 번에 한 번씩 순서대로 반복합니다.
    for n in range(0, arr.shape[1], seq_length):
        # The features
        x = arr[:, n:n+seq_length]
        # target을 1만큼 이동 
        y = np.zeros_like(x)
        try:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]
        except IndexError:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]
        yield x, y
```

## 6) Test Implementation

8, 50의 batch size를 사용해 어떻게 동작하는지 살펴보겠습니다.


```python
batches = get_batches(encoded, 8, 50)
x, y = next(batches)
```


```python
# printing out the first 10 items in a sequence
print('x\n', x[:10, :10])
print('\ny\n', y[:10, :10])
```

    x
     [[60 18 56 75 59 64 24 68  7 57]
     [20 44 27 68 59 18 56 59 68 56]
     [64 27 28 68 44 24 68 56 68 37]
     [20 68 59 18 64 68 32 18 15 64]
     [68 20 56 58 68 18 64 24 68 59]
     [32  0 20 20 15 44 27 68 56 27]
     [68 46 27 27 56 68 18 56 28 68]
     [43 31 48 44 27 20 17 71 80 68]]
    
    y
     [[18 56 75 59 64 24 68  7 57 57]
     [44 27 68 59 18 56 59 68 56 59]
     [27 28 68 44 24 68 56 68 37 44]
     [68 59 18 64 68 32 18 15 64 37]
     [20 56 58 68 18 64 24 68 59 64]
     [ 0 20 20 15 44 27 68 56 27 28]
     [46 27 27 56 68 18 56 28 68 20]
     [31 48 44 27 20 17 71 80 68 39]]
    

---
## 7) Defining the network with PyTorch

![rnn32](https://drive.google.com/uc?id=1s2-hVniHBR5iGc6O8Q2gQlj4Ct7RQOK5)

PyTorch를 사용해 신경망 구조를정의하겠습니다.

**Model Structure**

`__init__`는 다음과 같은 구조를 가지고 있씁니다:
* 필요한 dictionary를 만들고 저장합니다.
* 입력 크기(문자의 수), 숨겨진 레이어의 크기인 `n_hidden`, 레이어의 수인 `n_layers`, dropout의 확률인 `drop_prob` 그리고 boolean인 batch_first를 LSTM의 parameter로 넣습니다. 
* `dropout_prob`을 사용해 dropout layer를 정의합니다.
* input size `n_hidden` 와 문자열의 개수인 output size를 가지는 fully-connected layer를 정의합니다.
* weights를 초기화합니다.

---
## 8) LSTM Inputs/Outputs

가장 기본적인 LSTM 구조를 만들어 보겠습니다. [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm)

```python
self.lstm = nn.LSTM(input_size, n_hidden, n_layers, 
                            dropout=drop_prob, batch_first=True)
```

* `input_size` 는 이 셀이 순차적 입력으로 받을 문자의 개수를 의미합니다.
* `n_hidden`는 셀에 숨겨진 layer의 단위 수 입니다.
* dropout을 추가할 수도 있습니다.
* `forward` 함수에서 `.view`를 사용해 LSTM 셀을 레이어로 쌓을 수 있습니다.

이렇게 셀 목록을 전달하면 한 셀의 출력이 다음 셀로 전달됩니다.

```python
self.init_hidden()
```


```python
# GPU가 사용가능한지 확인합니다.
train_on_gpu = torch.cuda.is_available()
if(train_on_gpu):
    print('Training on GPU!')
else: 
    print('No GPU available, training on CPU; consider making n_epochs very small.')
```

    Training on GPU!
    


```python
class CharRNN(nn.Module):
    
    def __init__(self, tokens, n_hidden=256, n_layers=2,
                               drop_prob=0.5, lr=0.001):
        super().__init__()
        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.lr = lr
        
        # character dictionaries를 만듭니다.
        self.chars = tokens
        self.int2char = dict(enumerate(self.chars))
        self.char2int = {ch: ii for ii, ch in self.int2char.items()}
        
        ## LSTM을 정의합니다.
        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers,
                           dropout = drop_prob, batch_first = True)
        
        # Dropout layer를 정의합니다.
        self.dropout = nn.Dropout(drop_prob)
        
        # fully-connected layer를 추가합니다.
        self.fc = nn.Linear(n_hidden, len(self.chars))
    
    def forward(self, x, hidden):
                
        ## output과 새로운 hidden state를 얻습니다.
        r_output, hidden = self.lstm(x, hidden)
        
        # dropout layer로 보냅니다.
        out = self.dropout(r_output)
        
        # view를 사용해 LSTM output을 쌓습니다.
        out = out.contiguous().view(-1, self.n_hidden)
        
        # x를 fully connected layer로 전달합니다.
        out = self.fc(out)
        
        # return the final output and the hidden state
        return out, hidden
    
    
    def init_hidden(self, batch_size):
        ''' Initializes hidden state '''
        # n_layers x batch_size x n_hidden 사이즈의 2개의 tensro를 정의합니다.
        # hidden state와 LSTM state를 위해 값을 0으로 초기화 합니다.
        weight = next(self.parameters()).data
        
        if (train_on_gpu):
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
        else:
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())
        
        return hidden
```

**Time to train**

Train function은 epochs 수, learning rate 그리고 기타 매개변수를 설정하는 기능을 사용합니다. 

여기서는 문자 class의 점수가 output이기에 Adam optimizer와 corss entrhopy를 사용합니다. 

batch loop 내에서 hidden state를 history에서 분리하게 딥니다. 이번에는 LSTM의 hidden state와 셀 상태의 tuple인 hidden state가 있으므로 새 **tiple** 변수와 동일하게 설정합니다.

*  [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html)을 사용해 radient에 생길 수 있는 문제를 해결합니다. 


```python
def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):
    ''' Training a network 
    
        Arguments
        ---------
        
        net: CharRNN network
        data: 신경망을 훈련시키기 위한 텍스트 data 
        epochs: 훈련을 반복할 epochs 수
        batch_size: mini-batch 당 mini-sequences의 수
        seq_length: mini-batch 당 문자 수 
        lr: learning rate
        clip: gradient clipping
        val_frac: validation에 사용하기 위한 데이터의 비율
        print_every: training 및 validation loss를 출력 
    
    '''
    net.train()
    
    opt = torch.optim.Adam(net.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    
    # create training and validation data
    val_idx = int(len(data)*(1-val_frac))
    data, val_data = data[:val_idx], data[val_idx:]
    
    if(train_on_gpu):
        net.cuda()
    
    counter = 0
    n_chars = len(net.chars)
    for e in range(epochs):
        # initialize hidden state
        h = net.init_hidden(batch_size)
        
        for x, y in get_batches(data, batch_size, seq_length):
            counter += 1
            
            # One-hot encode our data and make them Torch tensors
            x = one_hot_encode(x, n_chars)
            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)
            
            if(train_on_gpu):
                inputs, targets = inputs.cuda(), targets.cuda()

            # Creating new variables for the hidden state, otherwise
            # we'd backprop through the entire training history
            h = tuple([each.data for each in h])

            # zero accumulated gradients
            net.zero_grad()
            
            # get the output from the model
            output, h = net(inputs, h)
            
            # calculate the loss and perform backprop
            loss = criterion(output, targets.view(batch_size*seq_length))
            loss.backward()
            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(net.parameters(), clip)
            opt.step()
            
            # loss stats
            if counter % print_every == 0:
                # Get validation loss
                val_h = net.init_hidden(batch_size)
                val_losses = []
                net.eval()
                for x, y in get_batches(val_data, batch_size, seq_length):
                    # One-hot encode our data and make them Torch tensors
                    x = one_hot_encode(x, n_chars)
                    x, y = torch.from_numpy(x), torch.from_numpy(y)
                    
                    # Creating new variables for the hidden state, otherwise
                    # we'd backprop through the entire training history
                    val_h = tuple([each.data for each in val_h])
                    
                    inputs, targets = x, y
                    if(train_on_gpu):
                        inputs, targets = inputs.cuda(), targets.cuda()

                    output, val_h = net(inputs, val_h)
                    val_loss = criterion(output, targets.view(batch_size*seq_length))
                
                    val_losses.append(val_loss.item())
                
                net.train() # reset to train mode after iterationg through validation data
                
                print("Epoch: {}/{}...".format(e+1, epochs),
                      "Step: {}...".format(counter),
                      "Loss: {:.4f}...".format(loss.item()),
                      "Val Loss: {:.4f}".format(np.mean(val_losses)))
```

**Instantiating the model**

Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!


```python
# define and print the net
n_hidden= 512
n_layers= 2

net = CharRNN(chars, n_hidden, n_layers)
print(net)
```

    CharRNN(
      (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)
      (dropout): Dropout(p=0.5)
      (fc): Linear(in_features=512, out_features=83, bias=True)
    )
    

**Set your training hyperparameters**


```python
batch_size = 100
seq_length = 100
n_epochs = 20 # start small if you are just testing initial behavior

# train the model
train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)
```

    Epoch: 1/20... Step: 10... Loss: 3.2242... Val Loss: 3.1580
    Epoch: 1/20... Step: 20... Loss: 3.1510... Val Loss: 3.1209
    Epoch: 1/20... Step: 30... Loss: 3.1415... Val Loss: 3.1109
    Epoch: 1/20... Step: 40... Loss: 3.1100... Val Loss: 3.1070
    Epoch: 1/20... Step: 50... Loss: 3.1350... Val Loss: 3.1042
    Epoch: 1/20... Step: 60... Loss: 3.1147... Val Loss: 3.1017
    Epoch: 1/20... Step: 70... Loss: 3.1337... Val Loss: 3.0969
    Epoch: 1/20... Step: 80... Loss: 3.1146... Val Loss: 3.0846
    ...
    Epoch: 20/20... Step: 3520... Loss: 1.1935... Val Loss: 1.2488
    Epoch: 20/20... Step: 3530... Loss: 1.2038... Val Loss: 1.2484
    Epoch: 20/20... Step: 3540... Loss: 1.2018... Val Loss: 1.2477
    Epoch: 20/20... Step: 3550... Loss: 1.2112... Val Loss: 1.2470
    Epoch: 20/20... Step: 3560... Loss: 1.2316... Val Loss: 1.2496
    

## 9) Getting the best model

최고의 성능을 가지는 hyperparamter를 설정하려면 training 및 validation loss를 확인해야 합니다. tarining loss가 validation보다 훨씬 낮으면 overfitting입니다. training loss를 낮추기 위해서는 regularization을 높이거나 (dropout을 더 시키거나) 더 작은 신경망을 만드는 것이 도움이 됩니다. 

** Hyperparameters **

신경망의 hyperparameter는 아래와 같습니다.

* `n_hidden` - Hidden layer의 단위 수 
* `n_layers` - 사용할 hidden LSTM layer의 수

dropout prob과 learning rate는 default로 주어진다고 가정하고 진행하겠습니다. 

* `batch_size` - 한 신경망을 통해 실행되는 시퀸스의 수 
* `seq_length` - 신경망이 훈련한 시퀸스 내의 문자 수. 일반적으로 신경망이 클수록 더 큰범위의 dependency를 학습합니다. 그러나 훈련하는데 시간이 더 걸립니다. 일반적으로 100으로 설정하는게 적합합니다. 
* `lr` - training에 사용할 learning rate입니다. 

[w이 문서](https://github.com/karpathy/char-rnn#tips-and-tricks).의 팁들을 참고하는 것도 도움이 됩니다.

> **Tips and Tricks**

> **Approximate number of parameters**

> 모델을 제어하는 가장 중요한 두 parameter는 `n_hidden`과`n_layers`입니다. 항상 2/3의 `n_layers`를 사용하는 것이 좋습니다.`n_hidden`은 가지고있는 데이터의 양에 따라 조정할 수 있습니다. 다음과 같은 것들을 고려해야 합니다.

> - training이 시작할 때 출력되는 모델의 parameter의 수와 
> - dataset의 크기입니다. 1mb 파일은 약 100만자로 구성되어 있습니다.

> 이 둘은 거의 같은 크기를 가지고 있는 것이 적절합니다. 예를 들면 다음과 같습니다. 

> - 100MB의 데이터가 있고 150k개의 parameter를 출력하고 있는 defalut parameter 세팅을 한다면, 데이터 크기가 상당히 크기 때문에 (100 mil >> 0.15 mil)크게 적합하지 않을 것으로 예상해 볼 수 있습니다. 즉, `n_hidden` 을 더 크게 만들 여유가 있습니다. 

> - 10MB의 데이터가 있고 천만개의 parameter를 실행한다고 가정해 봅시다. validation loss를 모니터링 해 training loss보다 큰 경우에는 dropout을 늘려 validaion loss에 도움이 되는지 확인해 볼 수 있습니다. 

> **Best models strategy**

> 컴퓨팅 파워가 충분할 경우 좋은 모델을 얻는 전략은 신경망을 크게 만들어 오류를 관측한 다음 0~1사이 dropout 값을 시도해 보는 것입니다.training 성능이 가장 좋은 모델 사용하면 됩니다. 딥 러닝에서 많은 hyper parameter 설정으로 시도해 보는 것이 일반적이며, 가장 높은 성능을 가진 것을 선택하게 됩니다.
 
> training과 validation의 크기역시 hyperparameter입니다. 적절한 양의 데이터가 없다면 validation 수행에 노이즈가 많아져 가치가 없을 수도 있습니다. 

**Checkpoint**

훈련 후 모델을 적용해 필요할 경우 다시 로드할 수도 있습니다. 동일한 아키텍처, hidden layer의 hyper parameter 등의 필요한 매개변수를 저장합니다. 


```python
# 여러 파일을 저장하기 위해 이름을 변경합니다.
model_name = 'rnn_x_epoch.net'

checkpoint = {'n_hidden': net.n_hidden,
              'n_layers': net.n_layers,
              'state_dict': net.state_dict(),
              'tokens': net.chars}

with open(model_name, 'wb') as f:
    torch.save(checkpoint, f)
```

---
## 10) Making Predictions

모델 학습이 되었기 떄문에 샘플링 하여 다음 문자를 예측해 보겠습니다. 샘플링을 위해 문자를 전달하고 신경망이 다음 캐릭터를 예측하게 해야 합니다. 이 다음 다음 예상 문자를 얻습니다. 계속 이를 반복하면 문장이 만들어집니다. 

**A note on the `predict`  function**

RNN의 output은 fully-connected layer에서 도출력된 것이며, **다음 문자의 점수 분포**를 출력합니다. 

> 다음 문자를 얻기 위해 softmax 함수를 사용합니다. 다음 문자를 예측할 수 있는 *확률 분포*가 도출됩니다. 

**Top K sampling**

가능한 모든 문자의 확률 분포를 바탕으로 다음 문자를 예측하게 됩니다. 샘플 텍스트를 만들고 $K$값을 고려해 가장 가능성이 높은 문자를 고려해 보다 합리적인 예측을 할 수 있습니다. 이런 방식은 샘플 텍스트에 약간의 노이즈와 임의성을 도입할 수 있도록 허용합니다. [topk 문서] (https://pytorch.org/docs/stable/torch.html#torch.topk)를 참고해 보세요. 


```python
def predict(net, char, h=None, top_k=None):
        ''' Given a character, predict the next character.
            Returns the predicted character and the hidden state.
        '''
        
        # tensor inputs
        x = np.array([[net.char2int[char]]])
        x = one_hot_encode(x, len(net.chars))
        inputs = torch.from_numpy(x)
        
        if(train_on_gpu):
            inputs = inputs.cuda()
        
        # detach hidden state from history
        h = tuple([each.data for each in h])
        # get the output of the model
        out, h = net(inputs, h)

        # get the character probabilities
        p = F.softmax(out, dim=1).data
        if(train_on_gpu):
            p = p.cpu() # move to cpu
        
        # get top characters
        if top_k is None:
            top_ch = np.arange(len(net.chars))
        else:
            p, top_ch = p.topk(top_k)
            top_ch = top_ch.numpy().squeeze()
        
        # select the likely next character with some element of randomness
        p = p.numpy().squeeze()
        char = np.random.choice(top_ch, p=p/p.sum())
        
        # return the encoded value of the predicted char and the hidden state
        return net.int2char[char], h
```

## 11) Priming and generating text 

hidden state을 구축할 수 있도록 신경망을 구성해야 합니다. 그렇지 않으면 신경망은 무작위로 문자열을 생성하게 됩니다. 첫번째 문자는 일반적으로 history가 작기 때문에 약간 어설플 수 있습니다. 


```python
def sample(net, size, prime='The', top_k=None):
        
    if(train_on_gpu):
        net.cuda()
    else:
        net.cpu()
    
    net.eval() # eval mode
    
    # First off, run through the prime characters
    chars = [ch for ch in prime]
    h = net.init_hidden(1)
    for ch in prime:
        char, h = predict(net, ch, h, top_k=top_k)

    chars.append(char)
    
    # Now pass in the previous character and get a new one
    for ii in range(size):
        char, h = predict(net, chars[-1], h, top_k=top_k)
        chars.append(char)

    return ''.join(chars)
```


```python
print(sample(net, 1000, prime='Anna', top_k=5))
```

    Anna asking the contention of was one of them the doctor. The sendent which
    said that they seemed time he could not show herself, and an account the place that she had a side of this wanted, which went out of the same word and there where he came in, the most
    suddenly said.
    
    "You're not an even and to be done, but I'm very glad."
    
    "I have
    liked a child.... I've sone to see harry it about her in, the party, we'll be an acquaintance."
    
    The princess who had not been and a carriage, which had been told him to him about his stream, with a subject of a country. The second man with the
    forest in a share and seemed to him as though he had taken
    her the creature and ask him in some steps and shows
    of it all
    he was to bo doubt. His brother's sight of second man into the significont of his confusion.
    
    "That's not once."
    
    "What are you to do." Alexey Alexandrovitch heard the party at off a truth, because when she heard the same, too. He saw that there was no one and whom
    he had a sense of houseless hou
    

**Loading a checkpoint**


```python
# 20 epoch 이상의 훈련을 거친 `rnn_20_epoch.net` 모델을 로드해 사용해 보겠습니다.
with open('rnn_x_epoch.net', 'rb') as f:
    checkpoint = torch.load(f)
    
loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])
loaded.load_state_dict(checkpoint['state_dict'])
```


```python
# 로드한 모델을바탕으로 샘플링했습니다. 
print(sample(loaded, 2000, top_k=5, prime="And Levin said"))
```

    And Levin said to him, as they considered the same summer than her household which had tried to get up and so strange of all his hands to take the same. "That with you will be stanting for you."
    
    "Yes, yes, it is no spirit in a subject."
    
    "I said he was not always from harvily. I'll see, it's out. I can't say, and how is
    how?"
    
    "I can't be sitting of the cheek of happoness."
    
    "What a door saying?"
    
    "I won't tell you they're such a conversation. What is this?" he said suddenly.
    
    "I have to be to the company?" said Anna, simply silent.
    
    "Then wo like that things is always too. I do your sen and the position. With a principle and side of a moment worse if you would begin to do it,"
    said Levin.
    
    "Yes, it's not the procost in me at that?... I don't understand it, but
    in the child's but the service of it and will be strength in all this thing that. And it seems to the weakness as you would give him on as seemed to married the common, but you will be stopping. It's
    a commind and
    saying it to think of the peasants, is not secret tran as the
    conversation in the sound of that service of is," he asked him, and with significant proper tone, when she saw in her face, still he could not be
    at the standing of anything,
    to see to see. But they was not long. And that they did not care or a little telling his stream of the party of her, and those
    would show about her, so than the ceater to have been the perfectly stare, at the continual step of her eyes, but that
    was in her surprise they were so awfully angry as it was so as to talk, to see what he had been angry from her, and that she had not changed it that he had said it still the
    persuase that her heart with him a feet faster of the children, but she had been said. "Yes," she said in the saying stinging of her eyes, she was not taking him
    by silent with the cold back of their most sides and hold the first painfully without still and delightful anticess
    that his head too would have said that the word when she went to himself to the sound of the 
    
