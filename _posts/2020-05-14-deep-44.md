---
title: "[Deep learning]RNN (1) Introduction"
date: 2020-05-14
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# Lesson 1 Introduction of Recurrent Neural Networks

지금까지는 CNN을 사용해 신경망을 구성하는 작업을 해 보았습니다. CNN은 training data에서 부분적이고 가시적인 패턴을 찾아내는데 적합합니다.

지금부터는 Recurrent Neural Networks 즉, RNN을 살펴보고자 합니다. 이를 통해 신경망에 **메모리를** 사용할 수 있게 해 주고, 연속적인 데이터를 잘 처리할 수 있게 해 줍니다.

text data는 보통 단어의 연속으로 구성되어 있기에 RNN은 text processing이나 generation에 사용할 수 있습니다. 몇 가지 RNN의 활용 사례를 알아보겠습니다.


**Text Generation**

Udacity 과정에서 TV Scropt를 만들어 보는 코드를 짜게 된다고 합니다. 이렇게, 어떤 텍스트던지 활용해서 새로운 텍스트를 만들어 볼 수 있습니다. 

이미지를 넣으면 자동으로 [설명](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)을 써 주는 작업도 할 수 있습니다.


**Sketch RNN**

텍스트 뿐만아니라 이미지에서도 사용할 수 있습니다. 캐치마인드처럼 [스케치](https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html)를 하면 인식하는 프로그램을 만들어 볼 수도 있습니다.

**Speech Recognition**

아마존의 알렉사, 애플의 시리, 구글의 음성 인식도 RNN을 사용합니다.

**Time Series Prediction**

교통 상황을 예측할 수도 있고, 고객의 선호를 파악할 수도 있으며, 주식시장 예측에도 사용됩니다.

**NLP**

구글번역같은 기계번역, 구글 애널리틱스같은 분석, 챗봇에서도 사용할 수 있습니다.

**Gesture Recognition**

사용자가 어떤 동작을 취하는지 역시 분석이 가능합니다.

몇가지 흥미로운 프로젝트를 소개해 보겠습니다.

* [DOTA2 게이밍 봇](https://ucai.com/blog/dota-2/)

* [무성 영화에 소리를 넣은 프로젝트](https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8)

* [손 글씨체를 만드는 프로젝트](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3)

* [Amazon의 S2T 프로젝트](https://aws.amazon.com/ko/lex/faqs/)

* [Facebook이 RNN과 LSTM을 사용해 언어 모델을 만든 프로젝트](https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/)

* [Netflix가 RNN의 모델을 활용하는 방식](https://arxiv.org/pdf/1511.06939.pdf)



## 1) Introduction

RNN의 큰 특징은 **Temporal Depedencies를(또는 dependencies over time)** 가진다는 것입니다. 새로운 input뿐만 아니라, 지난 input들의 영향을 받습니다.

우리가 저녁을 메뉴를 고를 때 전날에 뭘 먹었는지나, 점심에 뭘 먹었는지를 기억하는 것과 유사합니다. 대략적인 RNN의 구조는 다음과 같습니다.

![rnn1](https://drive.google.com/uc?id=1xpqpwkcR6Gt51nkHf_vlmGR3UoLsfkCJ)

'recurrent'는 **반복적**이라는 뜻입니다. RNN은 input sequence에 같은 작업을 반복한다는 것을 의미합니다.

![rnn2](https://drive.google.com/uc?id=1kgEDThtxTpWjm-fo79dGAd0Pa65R_25p)


## 2) History of RNN

1980년대에 Feedforward Neural Networks가 제안되었으나, temporal dependencies를 반영하다는 것이 불가능한 문제가 있었습니다.

![rnn3](https://drive.google.com/uc?id=1rUXEQ93H0WLxjMRmoBqWXTDFSdNzM0qV)

실시간으로 변화하는 시그널이나 영상데이터를 다루려면 이 문제는 꼭 해결되어야 했습니다. 생물학적으로도 신경망은 recurrent한 연결을 가지고 있습니다.

memory를 반영하기 위한 첫 번째 시도는 1989년에 제안된 TDNN(Time Delay Neural Network)였습니다. 과거의 데이터를 축적해 현재 데이터와 함께 다루는 방식입니다.

![rnn4](https://drive.google.com/uc?id=1ZZNyLAqjnXFB_vEgfIfLM1JTZ_IT0S3K)

하지만 반영하기로 정한 일부의 timestamp이외는 활용할 수 없다는 단점이 있었습니다. 1990년대에는 Simple RNN(Elman Network 또는 Jordan Network)가 제안되었습니다.  

![rnn5](https://drive.google.com/uc?id=1mqQVwEht3nlILRLpFrHwIcabti9vRJSJ)

이런 신경망들은 시간이 지남에 따라 기하학적으로 영향이 감소하는 **vanishing gradient problem**을 가지고 있다는 것이 밝혀졌습니다.

![rnn6](https://drive.google.com/uc?id=15Vy3h5iRWChtk0R82PAuXGxyFwuvt_bj)

이를 해결하기 위해 LSTM(Long Short Term Memory)가 제안되었습니다. input을 다음 게이트에 넣거나, 넣지 않는 방식을 통해 문제 해결을 시도했다고 합니다. 

![rnn7](https://drive.google.com/uc?id=1V2oqXhAHKdCTGkfRX9tZuPijfBpZJMZo)

더 자세한 사항은 다음 포스트로 살펴보겠습니다.