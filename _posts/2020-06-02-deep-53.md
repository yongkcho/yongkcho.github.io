---
title: "[Deep learning]GAN (3) Batch Normalization"
date: 2020-06-02
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson ? Batch Normalization

Batch Normalization은 2015년에 Sergey Ioffe과 Christian Szegedy에 의해 [제안](https://arxiv.org/pdf/1502.03167.pdf)되었습니다. 

네트워크의 input을 단순히 정규화하는 대신, 네트워크 안의 모든 레이어의 input을 정규화한다는 것이 핵심 컨셉입니다. 


## 1) Batch Normalization

**훈련** 과정에서 각 batchㅇ 대해 평균과 표준 편차를 활용해서 값을 정규화하는 것을 Batch Normalization이고 합니다. 또는 **batch statistics**고도 합니다.

> 각 값에 대해 batch의 평균을 빼고, 표준편차로 나눠 앞의 layer에 대한 input으로 사용합니다.

컨셉은 간단한 편이지만 이를 왜 사용할까요? input을 정규화하는 것은 네트워크가 학습하고 적합한 솔루션을 도출하는데 도움을 줍니다. 

네트워크는 layer들이 겹쳐져 이뤄진 것입니다. 한 layer의 출력값은 다음 layer의 입력이 됩니다. 

즉, 한 layer는 부분적 네트워크에서 첫 번째 layer로 볼 수 있습니다.


## 2) Normalization at Every Layer

다음과 같은 3개의 레이어로 구성된 네트워크를 생각해 보겠습니다.

![gnn1](https://drive.google.com/uc?id=1cDt2iyFCXyADETPnsTURbWs00vyk9O8y)

이 네트워크를 부분적으로 나눠서 생각해 보겠습니다. layer1을 input으로 생각해 보면 아래와 같이 됩니다.

![gnn2](https://drive.google.com/uc?id=1YtxnAFDXpXaa3s52EQpFe5aiyyVin3Zi)

같은 방식으로 layer2를 input으로 생각해 보면, layer3는 단순한 하나의 레이어로 구성된 네트워크로 생각해 볼 수 있습니다.

![gnn3](https://drive.google.com/uc?id=17ItDnVOnicZ6bdYI9FrMdhn-EdhdycCF)

이렇게 sub network로 생각해 보면, 각 layer의 output을 정규화하는 것이 전체의 성능 향상과 연결되는 것을 쉽게 이해할 수 있습니다.


## 3) Internal Covariate Shift

직관적 이해를 넘어서 batch normalization이 왜 효용이 있는지 수리적으로 생각해 볼 수도 있습니다. 논문의 저자들은 이를 내부 공변량 이동(internal covariate shift)이라 합니다.

> 내부 공변량 이동은 다른 layer로 입력하는 분포의 변화를 나타냅니다. 각 레이어에 대한 입력 분포가 유사할 때 네트워크 훈련이 가장 효과적입니다.

그리고 batch normalization은 각 layer input의 분포를 표준화하는 방식이기도 합니다. 자세한 내용은 [이 문서](https://arxiv.org/pdf/1502.03167.pdf)에서 볼 수 있습니다. 

Ian Goodfellow 등이 저술한 [이 책](http://www.deeplearningbook.org/)도 참고할만 합니다. [챕터8](http://www.deeplearningbook.org/contents/optimization.html)에 batch normalization에 대한 내용이 있습니다.


## 4) The Math

batch normalization 뒤에 있는 수학적 배경에 대해 살펴봅시다. 꼭 알 필요는 없지만 과정을 이해하는데 도움을 줍니다.

정규화를 위해서는 각 batch의 평균과 표준편차를 알아야 합니다. 나중에 코드를 살펴보면 이것이 batch input의 평균값이 아니라 

비선형 activation function을 통과한 이후 input이 되기 전에 특정 layer에서 출력되는 평균 값임을 알 수 있습니다. 

이 평균값을 **mu_B**로 표현하겠습니다.

$\mu_B$

이는 모든 **x_i**의 합을 전체 개수인 **m**으로 나눈 값입니다.

$\mu_B \leftarrow \frac{1}{m}\sum_{i = 1}^{m}x_{i}$

표준 편차 역시 계산해야 합니다.  **$\sigma_{B}^{2}σ$** 입니다.

$\sigma_{B}^{2} \leftarrow \frac{1}{m}\sum_{i=1}^m (x_i - \mu_B)^2$

평균과 표준편차를 계산하면 아래 방정식을 통해서 값을 표준화 할 수 있습니다.

$\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_{B}^{2} + \epsilon}}$

기존의 표준편차와 약간 차이가 있는 것을 볼 수 있습니다. 제곱근 안에 입실론이 들어 있습니다. 

입실론은 0.0001처럼 아주 작은 양의 상수 입니다. 이를 사용해 0으로 나누는 것을 방지해 주지만 각 batch의 분산을 약간 증가시킵니다.

각 batch에 대해 계산을 하면서 모집단에 대한 추정을 하기 때문에 통계적으로 쓰이는 방법이기도 합니다. 모집단의 분산은 _보통_ 샘플보다 크기 때문에

분산을 약간 증가시키는 것은 계산에 도움을 줍니다. 특히 샘플이 작으면 작을수록 이 방식은 더욱 도움이 됩니다. 

이런 방식을 통해 표준화된 값인 $\hat{x_i}$을 얻을 수 있습니다.

이를 적용하기 전에 **감마**값을 곱하고 **베타** 값을 더합니다. 감마와 베타는 네트워크가 학습 가능한 parameter입니다. 정규화된 값을 스케일링하고 이동시키는 역할을 합니다. 

$y_i \leftarrow \gamma \hat{x_i} + \beta$
​	
이제 레이어의 최종 출력을 얻었습니다. 이 출력결과는 sigmoid, tanh, ReLU, Leaky ReLU 등과 같은 비선형 activation function에 전달됩니다.

다음 포스팅에는 이를 코드로 구현해 보겠습니다. 