---
title: "[Deep learning]Nural Networks (12) Sentiment Classification"
date: 2020-04-22
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# 감성 분류 및 신경망을 통한 문제 해결

Andrew Trask는 신경망을 통해 영화 리뷰를 Positive와 Negative로 분류합니다. 

이 내용을 번역하고, 제가 배운 내용을 함께 정리해 두었습니다.


## 1) EDA


```python
def pretty_print_review_and_label(i): #리뷰를 간단하게 살펴보기 위한 함수를 정의합니다. 
    print(labels[i] + "\t\t" + reviews[i][:80] + "...") 
    
g = open('reviews.txt', 'r') #데이터를 불러옵니다. 이미 알고있는 데이터입니다.
reviews = list(map(lambda x:x[:-1], g.readlines())) #list에 map을 사용해 lambda 함수로 한번에 처리합니다. 마지막에 붙는 \n을 빼기 위해 -1까지만 불러옵니다.
g.close()

g = open('labels.txt', 'r') #신경망을 통해 분류해야 하는 데이터입니다.
labels = list(map(lambda x:x[:-1].upper(), g.readlines()))
g.close()
```


```python
len(reviews) #전체 데이터의 개수를 확인합니다. 
```




    25000




```python
reviews[0] #데이터를 확인해 봅니다.
```




    'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '




```python
labels[0] #해당 리뷰의 라벨로 살펴봅니다. 
```




    'POSITIVE'




```python
print("labels.txt \t : \t reviews.txt\n") 
pretty_print_review_and_label(2137) #위에서 정의한 함수를 불러와, 임의의 데이터를 간단히 확인해보겠습니다.
pretty_print_review_and_label(12816)
pretty_print_review_and_label(6267)
pretty_print_review_and_label(21934)
pretty_print_review_and_label(5297)
pretty_print_review_and_label(4998)
```

    labels.txt 	 : 	 reviews.txt
    
    NEGATIVE		this movie is terrible but it has some good effects .  ...
    POSITIVE		adrian pasdar is excellent is this film . he makes a fascinating woman .  ...
    NEGATIVE		comment this movie is impossible . is terrible  very improbable  bad interpretat...
    POSITIVE		excellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...
    NEGATIVE		if you haven  t seen this  it  s terrible . it is pure trash . i saw this about ...
    POSITIVE		this schiffer guy is a real genius  the movie is of excellent quality and both e...
    

[numpy](https://docs.scipy.org/doc/numpy/reference/) 에서 [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) 을 사용해 더 살펴봅시다.


```python
from collections import Counter
import numpy as np
```


```python
# positive, negative, total counts를 세기 위한 Counter 오브젝트를 생성합니다. 
positive_counts = Counter()
negative_counts = Counter()
total_counts = Counter()
```


```python
for i in range(len(reviews)): 
    if(labels[i] == "POSITIVE"):
        for word in reviews[i].split(" "): #먼저 스페이스를 기준으로 단어를 분할하고
            positive_counts[word] += 1 #각 단어의 개수를 세어줍니다.
            total_counts[word] += 1
    else:
        for word in reviews[i].split(" "):
            negative_counts[word] += 1
            total_counts[word] += 1
```


```python
#이제 어떤 단어들이 Positive로 분류되었는지 확인해 봅시다.
positive_counts.most_common()[0:9]
```




    [('', 550468),
     ('the', 173324),
     ('.', 159654),
     ('and', 89722),
     ('a', 83688),
     ('of', 76855),
     ('to', 66746),
     ('is', 57245),
     ('in', 50215),
     ('br', 49235)]




```python
#이제 어떤 단어들이 Negative로 분류되었는지 확인해 봅시다.
negative_counts.most_common()[0:9]
```




    [('', 561462),
     ('.', 167538),
     ('the', 163389),
     ('a', 79321),
     ('and', 74385),
     ('of', 69009),
     ('to', 68974),
     ('br', 52637),
     ('is', 50083)]



Positive나 Negative 모두 빈도가 높은 단어는 큰 의미가 없어보입니다. 영어에서 흔히 쓰이는 the 등이 계속해서 등장하는 것을 볼 수 있습니다. 이런 단어들을 바탕으로 학습을 시킨다면 두 class를 제대로 분류하지 못할 가능성이 있습니다. 그래서 둘의 **비율(ratio)** 을 비교해 보도록 하겠습니다. 


```python
pos_neg_ratios = Counter() #먼저 counter 오브젝트를 만들고

for term,cnt in list(total_counts.most_common()): 
    if(cnt > 100): #전체 카운트 리스트 중 100번 이상 등장한 단어에 대해서 
        pos_neg_ratio = positive_counts[term] / float(negative_counts[term] + 1) #pos와 neg의 비율을 구합니다. 이때, negative_counts가 0일수 있어 1을 더합니다.
        pos_neg_ratios[term] = pos_neg_ratio
```


```python
#몇 단어에 대해서 비율을 살펴봅시다. 
print("Pos-to-neg ratio for 'the' = {}".format(pos_neg_ratios["the"])) #pos와 neg가 유사한 비율로 나타납니다.
print("Pos-to-neg ratio for 'amazing' = {}".format(pos_neg_ratios["amazing"])) #pos에 더 가까운 것 같습니다.
print("Pos-to-neg ratio for 'terrible' = {}".format(pos_neg_ratios["terrible"])) #neg에 더 가까운 듯 합니다. 
```

    Pos-to-neg ratio for 'the' = 1.0607993145235326
    Pos-to-neg ratio for 'amazing' = 4.022813688212928
    Pos-to-neg ratio for 'terrible' = 0.17744252873563218
    

하지만 여기서는 하나의 문제가 있습니다. amazing은 4, terrible은 0.18인데 이 두 값을 어떻게 비교할 수 있을까요?
그렇기 때문에 우리는 log를 취해 비교하게 됩니다.


```python
#로그 변환을 하면 0이 중립적 단어, 1은 긍정적 단어, -1은 부정적 단어가 됩니다.
for word,ratio in pos_neg_ratios.most_common():
    pos_neg_ratios[word] = np.log(ratio)
```


```python
#Positive 리뷰에서 주로 나타나는 단어를 확인해봅시다.
pos_neg_ratios.most_common()[0:14]
```




    [('edie', 4.6913478822291435),
     ('paulie', 4.07753744390572),
     ('felix', 3.152736022363656),
     ('polanski', 2.8233610476132043),
     ('matthau', 2.80672172860924),
     ('victoria', 2.681021528714291),
     ('mildred', 2.6026896854443837),
     ('gandhi', 2.538973871058276),
     ('flawless', 2.451005098112319),
     ('superbly', 2.26002547857525),
     ('perfection', 2.159484249353372),
     ('astaire', 2.1400661634962708),
     ('captures', 2.038619547159581),
     ('voight', 2.030170492673053)]




```python
#Negative 리뷰에서 주로 나타나는 단어를 확인해봅시다.
list(reversed(pos_neg_ratios.most_common()))[0:14]
```




    [('boll', -4.969813299576001),
     ('uwe', -4.624972813284271),
     ('seagal', -3.644143560272545),
     ('unwatchable', -3.258096538021482),
     ('stinker', -3.2088254890146994),
     ('mst', -2.9502698994772336),
     ('incoherent', -2.9368917735310576),
     ('unfunny', -2.6922395950755678),
     ('waste', -2.6193845640165536),
     ('blah', -2.5704288232261625),
     ('horrid', -2.4849066497880004),
     ('pointless', -2.4553061800117097),
     ('atrocious', -2.4259083090260445),
     ('redeeming', -2.3682390632154826)]



## 2) Transforming Text into Numbers<a id='lesson_3'></a>

<br>

[set](https://docs.python.org/3/tutorial/datastructures.html#sets) 을 활용해 작업을 합니다.


```python
from IPython.display import Image #이미지 확인을 위한 모듈을 부러옵니다. 

review = "This was a horrible, terrible movie."  #다음과 같은 리뷰가 있다고 생각해 보면

Image(filename='sentiment_network.png') #이런 형태로 네트워크로 바꿀 수 있습니다.
```




![png](sentimental_files/sentimental_21_0.png)



위의 그림은 신경망에서 Class를 표현하는 방식입니다. 각 단어를 node로 놓고, input값들을 hideen layer를 통과 시켜 분류합니다
만약 positive의 경우에는 어떨까요?


```python
review = "The movie was excellent"

Image(filename='sentiment_network_pos.png')
```




![png](sentimental_files/sentimental_23_0.png)




```python
vocab = set(total_counts.keys()) #set은 순서가 없고, 중복을 허용하지 않는 집합형 자료입니다.
```


```python
vocab_size = len(vocab)
print(vocab_size) #단어의 개수를 살펴봅니다.
```

    74074
    


```python
from IPython.display import Image 
Image(filename='sentiment_network_2.png') #다음과 같이 layer_0은 input, layer_1은 hidden, 그리고 layer_2는 output을 의미합니다.
```




![png](sentimental_files/sentimental_26_0.png)




```python
layer_0 = np.zeros((1, vocab_size))
layer_0.shape #input layer를 만들어줍니다.
```




    (1, 74074)




```python
from IPython.display import Image
Image(filename='sentiment_network.png')
```




![png](sentimental_files/sentimental_28_0.png)




```python
#이제 만들어진 layer에 모든 단어를 넣어야 합니다.
word2index  = {}
for i,word in enumerate(vocab): #enumerate를 통해 몇 번째 반복문인지 확인하고, index와 원소를 tuple로 반환합니다.
    word2index[word] = i 
    
#결과를 확인해 봅시다. 
word2index(list())[0:9]	 
```




    {'': 0,
     'kiwi': 1,
     'janet': 2,
     'caravans': 3,
     'using': 4,
     'clubbed': 5,
     'arado': 6,
     'foundational': 7,
     'yoshida': 8,}



```python
#이제 input layer를 업데이트 해 봅시다. 

def update_input_layer(review):
    
    global layer_0
    
    #layer를 리셋합니다. 
    layer_0 *= 0
    
    #각 단어의 등장 횟수를 파악하고, 이를 layer_0에 저장합니다.
    for word in review.split(" "):
        layer_0[0][word2index[word]] += 1
```


```python
layer_0 #현재레이어를 확인합니다.
```




    array([[0., 0., 0., ..., 0., 0., 0.]])




```python
update_input_layer(reviews[0]) #이제 첫 번째 리뷰에 대해서 정의한 함수를 테스트하겠습니다.
```


```python
layer_0 #첫번째 값이 업데이트 된 것을 확인할 수 있습니다.
```




    array([[18.,  0.,  0., ...,  0.,  0.,  0.]])




```python
def get_target_for_label(label): #positive는 1로, negative는 0으로 변화합니다.
    if(label == 'POSITIVE'):
        return 1
    else:
        return 0
```


```python
labels[0] #첫 번째 label에 대해서 확인해 보겠습니다
```




    'POSITIVE'




```python
get_target_for_label(labels[0])
```




    1




```python
labels[1] #negative도 잘 분류되었는지 확인해보겠습니다
```




    'NEGATIVE'




```python
get_target_for_label(labels[1])
```




    0



## 3) Building a Neural Network


```python
import time
import sys
import numpy as np
```


```python
#  신경망을 encapsulate해 class로 만듦니다.

class SentimentNetwork:
    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1,  hidden_nodes = 10, learning_rate = 0.1):
        # min_count는 최소 등장횟수입니다. 이 이상 단어가 등장해야 추가됩니다. 
        # polarity_cutoff는 positive와 negative의 비율을 바탕으로 이 이상이 되어야만 학습에 사용합니다.
        
        np.random.seed(1) #seed를 설정해 디버깅을 쉽게 합니다.
        
        self.pre_process_data(reviews,labels) #리뷰와 라벨을 trainning전에 정리합니다.
        
        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate) #input node의 개수만큼 인자를 받아 신경망을 초기화 시킵니다.
        
        
    def pre_process_data(self, reviews, labels):
        
        review_vocab = set()
        for review in reviews: #주어진 review에 있는 모든 단어를 바탕으로 review_vocab을 채웁니다.
            for word in review.split(" "):
                review_vocab.add(word)
        
        self.review_vocab = list(review_vocab) #vocabulary 집합을 list로 만들어 index를 바탕으로 접근할 수 있게 만듦니다.
        
        label_vocab = set() #주어진 label역시 review와 같은 작업을 수행합니다.
        for label in labels:
            label_vocab.add(label)
            
        self.label_vocab = list(label_vocab)
        
        self.review_vocab_size = len(self.review_vocab) #review와 label의 size를 저장해 둡니다.
        self.label_vocab_size = len(self.label_vocab)
        
        self.word2index = {} #dictionary를 만들어 label을 index 위치에 mapping합니다.
        for i,word in enumerate(self.review_vocab):
            self.word2index[word] = i
        
        self.label2index = {}
        for i,label in enumerate(self.label_vocab):
            self.label2index[label] = i
    
    
    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        #input, hidden, output layer, learning_rate의 숫자를 정합니다.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        
        #weights를 초기화합니다.
        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes)) #input과 hidden layer 사이의 weights
        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                            (self.hidden_nodes, self.output_nodes))
        
        self.layer_0 = np.zeros((1, input_nodes)) #input layer를 만들어줍니다.
        
    def update_input_layer(self, review):
        
        self.layer_0 *= 0 #input layer를 모두 0으로 초기화합니다.
        
        for word in review.split(" "):
            if(word in self.word2index.keys()):
                self.layer_0[0][self.word2index[word]] += 1
        
        
    def get_target_for_label(self, label): #positive를 1로, negative를 0으로 변환합니다.
        if(label == 'POSITIVE'):
            return 1
        else:
            return 0
        
    def sigmoid(self, x): #sigmoid 함수를 정의합니다.
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_output_2_derivative(self, output):
        return output * (1 - output)
    
    def train(self, training_reviews, training_labels):
        assert(len(training_reviews) == len(training_labels)) #둘이 동일하지 않으면 AssertError를 발생시킵니다
        
        correct_so_far = 0 #trainning 중 맞춘 class를 저장할 변수를 만듦니다.
        
        start = time.time() #시작한 시간을 체크합니다.
        
        for i in range(len(training_reviews)): #모든 review에 대해 forward와 backword pass를 수행합니다. 매번 weights를 수행합니다.
            
            #review와 label을 불러옵니다.
            review = training_reviews[i]
            label = training_labels[i]
            
            #forward pass
            
            self.update_input_layer(review) #input layer
            layer_1 = self.layer_0.dot(self.weights_0_1)
            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
            
            #backward pass
            
            #output error
            layer_2_error = layer_2 - self.get_target_for_label(label)
            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)
            
            #backpropagated error
            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)
            layer_1_delta = layer_1_error #hidden layer의 gradient입니다.
            
            #weights를 업데이트합니다.
            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # hidden에서 output에 적용되는 weights를 조정합니다.
            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate #input에서 hidden에 적용되는 weights를 조정합니다.
            
            if(layer_2 >= 0.5 and label == 'POSITIVE'):
                correct_so_far += 1
            elif(layer_2 < 0.5 and label == 'NEGATIVE'):
                correct_so_far += 1
                
            #디버깅을 위해 예측 정확도와 걸린 시간을 확인합니다.
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(training_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct_so_far) + " #Trained:" + str(i+1) \
                             + " Training Accuracy:" + str(correct_so_far * 100 / float(i+1))[:4] + "%")
            if(i % 2500 == 0):
                print("")
                
    def test(self, testing_reviews, testing_labels):
        correct = 0
        start = time.time()
        
        for i in range(len(testing_reviews)):
            pred = self.run(testing_reviews[i]) #label을 예측합니다.
            if(pred == testing_labels[i]):
                correct += 1
                
            #역시 디버ㄷ싱을 위해 걸린 시간 등을 표현합니다. 
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(testing_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct) + " #Tested:" + str(i+1) \
                             + " Testing Accuracy:" + str(correct * 100 / float(i+1))[:4] + "%")
    
    def run(self, review):
        
        self.update_input_layer(review.lower()) #input layer
        layer_1 = self.layer_0.dot(self.weights_0_1)
        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
        
        if(layer_2[0] >= 0.5):
            return "POSITIVE"
        else:
            return "NEGATIVE"
```


```python
#learning_rate를 0.1로 해서 조금만 테스트 해보겠습니다.
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)
```


```python
mlp.test(reviews[-1000:],labels[-1000:])
```

    Progress:99.9% Speed(reviews/sec):548.7 #Correct:500 #Tested:1000 Testing Accuracy:50.0%


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.8,learning_rate=0.1)
mlp.train(reviews[:-1000],labels[:-1000])
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):64.15 #Correct:1251 #Trained:2501 Training Accuracy:50.0%
    Progress:20.8% Speed(reviews/sec):60.57 #Correct:2501 #Trained:5001 Training Accuracy:50.0%
    Progress:31.2% Speed(reviews/sec):62.41 #Correct:3751 #Trained:7501 Training Accuracy:50.0%
    Progress:41.6% Speed(reviews/sec):60.32 #Correct:5001 #Trained:10001 Training Accuracy:50.0%
    Progress:52.0% Speed(reviews/sec):61.55 #Correct:6251 #Trained:12501 Training Accuracy:50.0%
    Progress:62.5% Speed(reviews/sec):61.32 #Correct:7501 #Trained:15001 Training Accuracy:50.0%
    Progress:72.9% Speed(reviews/sec):62.09 #Correct:8751 #Trained:17501 Training Accuracy:50.0%
    Progress:83.3% Speed(reviews/sec):61.84 #Correct:10001 #Trained:20001 Training Accuracy:50.0%
    Progress:93.7% Speed(reviews/sec):62.37 #Correct:11251 #Trained:22501 Training Accuracy:50.0%
    Progress:99.9% Speed(reviews/sec):62.53 #Correct:12000 #Trained:24000 Training Accuracy:50.0%

Accuracy는 50%로, 실제로 동전 던지기와 확률이 같습니다. 이를 조정하기 위해 learning_rate를 **0.01** 로 조정합니다. 


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.01)
mlp.train(reviews[:-1000],labels[:-1000])
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):57.84 #Correct:1248 #Trained:2501 Training Accuracy:49.9%
    Progress:20.8% Speed(reviews/sec):61.00 #Correct:2498 #Trained:5001 Training Accuracy:49.9%
    Progress:31.2% Speed(reviews/sec):60.95 #Correct:3748 #Trained:7501 Training Accuracy:49.9%
    Progress:41.6% Speed(reviews/sec):59.85 #Correct:4998 #Trained:10001 Training Accuracy:49.9%
    Progress:52.0% Speed(reviews/sec):59.82 #Correct:6248 #Trained:12501 Training Accuracy:49.9%
    Progress:62.5% Speed(reviews/sec):59.44 #Correct:7487 #Trained:15001 Training Accuracy:49.9%
    Progress:72.9% Speed(reviews/sec):59.74 #Correct:8737 #Trained:17501 Training Accuracy:49.9%
    Progress:83.3% Speed(reviews/sec):59.58 #Correct:9987 #Trained:20001 Training Accuracy:49.9%
    Progress:93.7% Speed(reviews/sec):59.48 #Correct:11237 #Trained:22501 Training Accuracy:49.9%
    Progress:99.9% Speed(reviews/sec):60.27 #Correct:11986 #Trained:24000 Training Accuracy:49.9%

이렇게 바꿔봐도 Accuracy는 그다지 변하지 않는 것 같습니다. learning_rate를 **0.001** 로 바꿔보겠습니다.


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.001)
mlp.train(reviews[:-1000],labels[:-1000]) 
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):63.59 #Correct:1258 #Trained:2501 Training Accuracy:50.2%
    Progress:20.8% Speed(reviews/sec):65.39 #Correct:2633 #Trained:5001 Training Accuracy:52.6%
    Progress:31.2% Speed(reviews/sec):63.89 #Correct:4050 #Trained:7501 Training Accuracy:53.9%
    Progress:41.6% Speed(reviews/sec):63.27 #Correct:5547 #Trained:10001 Training Accuracy:55.4%
    Progress:52.0% Speed(reviews/sec):61.27 #Correct:7127 #Trained:12501 Training Accuracy:57.0%
    Progress:62.5% Speed(reviews/sec):61.83 #Correct:8768 #Trained:15001 Training Accuracy:58.4%
    Progress:72.9% Speed(reviews/sec):61.17 #Correct:10398 #Trained:17501 Training Accuracy:59.4%
    Progress:83.3% Speed(reviews/sec):60.82 #Correct:12121 #Trained:20001 Training Accuracy:60.6%
    Progress:93.7% Speed(reviews/sec):60.64 #Correct:13830 #Trained:22501 Training Accuracy:61.4%
    Progress:99.9% Speed(reviews/sec):60.55 #Correct:14897 #Trained:24000 Training Accuracy:62.0%

드디어 네트워크가 뭔가를 배우기는 하는데, 속도가 너무 느린 것 같습니다.
다음에는 이를 어떻게 보완할 수 있을지 알아보겠습니다. 
