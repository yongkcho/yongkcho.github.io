---
title: "[Deep learning] Nural Networks (6)Gradient Descent Algorithm"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lesson 6


## 1) Puesdo Code

Gradient Descent를 구현하기 위해 먼저 로직을 파악해 봅시다. 

![nn39](https://drive.google.com/uc?id=1l-yTr77j7qV3e344zZgkKRny1c7H6lVU)

1. 임의의 weight로 알고리즘을 시작합니다.

2. 모든 데이터 포인트 x에 대해 
		2.1. For i = 1, ..., n 
			2.1.1 w를 업데이트합니다.
			2.1.2 b를 업데이트합니다.
			
3. 에러가 최소화될 때 까지 이를 반복합니다.


## 2)Implementing

그럼 이제 python을 바탕으로 Gradient Descent를 구현해 보겠습니다.

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#필요한 패키지를 불러옵니다.

def plot_points(X, y): #포인트를 플로팅하기 위한 함수를 정의합니다.
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color='g--'): #라인 플롯을 플로팅하기 위한 함수를 정의합니다.
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color)
```

다음으로 데이터를 로드해 시각화하겠습니다.

```python
data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
plt.show()
```
그러면 다음과 같은 결과가 나옵니다.

![nn40](https://drive.google.com/uc?id=1bHS2bF2NaN7t0tEkCEWx_uugnqHAg_uR)

이제, 필요한 함수를 정의하겠습니다.

```python
# Activation (sigmoid) function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Output (prediction) formula
def output_formula(features, weights, bias):
    return sigmoid(np.dot(features, weights) + bias)

# Error (log-loss) formula
def error_formula(y, output):
    return - y*np.log(output) - (1 - y) * np.log(1-output)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    output = output_formula(x, weights, bias)
    d_error = -(y - output)
    weights -= learnrate * d_error * x
    bias -= learnrate * d_error
    return weights, bias
```

그러면 이제 알고리즘을 반복하며 gradient descent function을 트레이닝하겠습니다.

```python
np.random.seed(44)

epochs = 100
learnrate = 0.01

def train(features, targets, epochs, learnrate, graph_lines=False):
    
    errors = []
    n_records, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    bias = 0
    for e in range(epochs):
        del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
            output = output_formula(x, weights, bias)
            error = error_formula(y, output)
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # log-loss error를 트레이닝 셋에서 구해 출력합니다.
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)
        if e % (epochs / 10) == 0:
            print("\n========== Epoch", e,"==========")
            if last_loss and last_loss < loss:
                print("Train loss: ", loss, "  WARNING - Loss Increasing")
            else:
                print("Train loss: ", loss)
            last_loss = loss
            predictions = out > 0.5
            accuracy = np.mean(predictions == targets)
            print("Accuracy: ", accuracy)
        if graph_lines and e % (epochs / 100) == 0:
            display(-weights[0]/weights[1], -bias/weights[1])
            

    # 라인을 플로팅합니다.
    plt.title("Solution boundary")
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    # 데이터를 플로팅합니다.
    plot_points(features, targets)
    plt.show()

    # 에러를 플로팅합니다. 
    plt.title("Error Plot")
    plt.xlabel('Number of epochs')
    plt.ylabel('Error')
    plt.plot(errors)
    plt.show()
```

이렇게 하면 다음과 같은 결과가 나옵니다.

```python
========== Epoch 0 ==========
Train loss:  0.713584519538
Accuracy:  0.4

========== Epoch 10 ==========
Train loss:  0.622583521045
Accuracy:  0.59

========== Epoch 20 ==========
Train loss:  0.554874408367
Accuracy:  0.74

========== Epoch 30 ==========
Train loss:  0.501606141872
Accuracy:  0.84

========== Epoch 40 ==========
Train loss:  0.459333464186
Accuracy:  0.86

========== Epoch 50 ==========
Train loss:  0.425255434335
Accuracy:  0.93

========== Epoch 60 ==========
Train loss:  0.397346157167
Accuracy:  0.93

========== Epoch 70 ==========
Train loss:  0.374146976524
Accuracy:  0.93

========== Epoch 80 ==========
Train loss:  0.354599733682
Accuracy:  0.94

========== Epoch 90 ==========
Train loss:  0.337927365888
Accuracy:  0.94
```

![41](https://drive.google.com/uc?id=1tXmpXYx1s3FinM85BTM42D6gACcTWypO)

![42](https://drive.google.com/uc?id=16ym2rEgG1C0sThOAZBkVXJzx-ykZWuuf)


## 3) Perceptron vs Gradient Descent

지금까지 error를 줄이기 위한 두가지 알고리즘을 보았는데요. 두개는 어떻게 다를까요?

![42](https://drive.google.com/uc?id=1_YFCbbu_FoW5yQ9E1xt5n4xmKa1jGxLw)

큰 차이점 중 하나는 Perceptron의 경우, 예측이 맞았을 때 아무것도 하지 않지만 gradient descent는 error를 계산해 반영한다는 것입니다.