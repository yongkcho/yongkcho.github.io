---
title: "[Deep learning]GAN (4) Batch Normalization with PyTorch"
date: 2020-06-02
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson 3 Batch Normalization with PyTorch


## 1) Introduction

PyTorch를 사용해 batch normalization을 수행해 보겠습니다. 다시한번 MNIST데이터셋을 활용하고자 합니다.


```python
%matplotlib inline

import numpy as np
import torch
import matplotlib.pyplot as plt
```


```python
from torchvision import datasets
import torchvision.transforms as transforms

# number of subprocesses to use for data loading
num_workers = 0
# how many samples per batch to load
batch_size = 64

# convert data to torch.FloatTensor
transform = transforms.ToTensor()

# get the training and test datasets
train_data = datasets.MNIST(root='data', train=True,
                            download=True, transform=transform)

test_data = datasets.MNIST(root='data', train=False,
                           download=True, transform=transform)

# prepare data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,
                                           num_workers=num_workers)

test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,
                                          num_workers=num_workers)
```

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Processing...
    Done!
    

##2) Visualize the data


```python
# obtain one batch of training images
dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()

# get one image from the batch
img = np.squeeze(images[0])

fig = plt.figure(figsize = (3,3)) 
ax = fig.add_subplot(111)
ax.imshow(img, cmap='gray')
```




    <matplotlib.image.AxesImage at 0x7f325b8db208>


![png](https://drive.google.com/uc?id=1ZDnBfU5ASNQ_tKjJKPvqtgbdlGTan-QD)


## 3) Neural network classes for testing

`NeuralNet` 클레스는 **batch normalization이 적용된/적용되지 않은** 네트워크를 만들어 비교해 줍니다. 코드가 약간 복잡해 처음부터 다 이해할 필요는 없고, 점차 설명해 보도록 하겠습니다.

*About the code:*
> 분류문제를 해결하기 위한 간단한 MLP를 만듭니다; 네트워크는 batch norm에 대해 알아보기 위해 디자인되었고, 분류 정확도에는 비교적 공을 덜 들였습니다.

**(Important) Model Details**

`__init__` 함수 안에 batch normalization을 정의하였습니다. 중요한 부분은 다음과 같습니다. 

1. batch normalization이 있는 레이어에는 bias term이 포함되어 있지 **않습니다.**

2. 선형 레이어 출력에서 사용되는 PyTorch의 [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d)함수를 활용하였습니다. 만약 이미지와 같은 2D출력에는 [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)을 사용합니다.

3. activation function을 호출하기 **전에** batch normalization layer를 추가합니다.


```python
import torch.nn as nn
import torch.nn.functional as F

class NeuralNet(nn.Module):
    def __init__(self, use_batch_norm, input_size=784, hidden_dim=256, output_size=10):
        """
        Creates a PyTorch net using the given parameters.
        
        :param use_batch_norm: bool
            Pass True to create a network that uses batch normalization; False otherwise
            Note: this network will not use batch normalization on layers that do not have an
            activation function.
        """
        super(NeuralNet, self).__init__() # init super
        
        # Default layer sizes
        self.input_size = input_size # (28*28 images)
        self.hidden_dim = hidden_dim
        self.output_size = output_size # (number of classes)
        # Keep track of whether or not this network uses batch normalization.
        self.use_batch_norm = use_batch_norm
        
        # define hidden linear layers, with optional batch norm on their outputs
        # layers with batch_norm applied have no bias term
        if use_batch_norm:
            self.fc1 = nn.Linear(input_size, hidden_dim*2, bias=False)
            self.batch_norm1 = nn.BatchNorm1d(hidden_dim*2)
        else:
            self.fc1 = nn.Linear(input_size, hidden_dim*2)
            
        # define *second* hidden linear layers, with optional batch norm on their outputs
        if use_batch_norm:
            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim, bias=False)
            self.batch_norm2 = nn.BatchNorm1d(hidden_dim)
        else:
            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)
        
        # third and final, fully-connected layer
        self.fc3 = nn.Linear(hidden_dim, output_size)
        
        
    def forward(self, x):
        # flatten image
        x = x.view(-1, 28*28)
        # all hidden layers + optional batch norm + relu activation
        x = self.fc1(x)
        if self.use_batch_norm:
            x = self.batch_norm1(x)
        x = F.relu(x)
        # second layer
        x = self.fc2(x)
        if self.use_batch_norm:
            x = self.batch_norm2(x)
        x = F.relu(x)
        # third layer, no batch norm or activation
        x = self.fc3(x)
        return x

```

## 4) Create two different models for testing

* `net_batchnorm` is a linear classification model **with** batch normalization applied to the output of its hidden layers
* `net_no_norm` is a plain MLP, without batch normalization

* `net_batchnorm` 은 hidden layer의 출력에 batch normalization이 적용된 linear classification model입니다.
* `net_no_norm` 은  batch normalization이 적용되지 않은 단순한 MLP입니다. 

batch noramlization을 제외하고 두 모델은 동일합니다.


```python
net_batchnorm = NeuralNet(use_batch_norm=True)
net_no_norm = NeuralNet(use_batch_norm=False)

print(net_batchnorm)
print()
print(net_no_norm)
```

    NeuralNet(
      (fc1): Linear(in_features=784, out_features=512, bias=False)
      (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (fc2): Linear(in_features=512, out_features=256, bias=False)
      (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (fc3): Linear(in_features=256, out_features=10, bias=True)
    )
    
    NeuralNet(
      (fc1): Linear(in_features=784, out_features=512, bias=True)
      (fc2): Linear(in_features=512, out_features=256, bias=True)
      (fc3): Linear(in_features=256, out_features=10, bias=True)
    )
    

---
**Training**

아래의 `train`함수는 몇 번의 epochs를 진행하며 모델을 훈련시킵니다. 최적화를 위해 cross entropy loss와 stochastic gradient descent를 사용했습니다. 이 함수는 loss를 반환하고, 매 epoch를 기록해 비교할 수 있게 해 줍니다. 


```python
def train(model, n_epochs=10):
    # number of epochs to train the model
    n_epochs = n_epochs
    # track losses
    losses = []
        
    # optimization strategy 
    # specify loss function (categorical cross-entropy)
    criterion = nn.CrossEntropyLoss()

    # specify optimizer (stochastic gradient descent) and learning rate = 0.01
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    # set the model to training mode
    model.train()
    
    for epoch in range(1, n_epochs+1):
        # monitor training loss
        train_loss = 0.0

        ###################
        # train the model #
        ###################
        batch_count = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            # clear the gradients of all optimized variables
            optimizer.zero_grad()
            # forward pass: compute predicted outputs by passing inputs to the model
            output = model(data)
            # calculate the loss
            loss = criterion(output, target)
            # backward pass: compute gradient of the loss with respect to model parameters
            loss.backward()
            # perform a single optimization step (parameter update)
            optimizer.step()
            # update average training loss
            train_loss += loss.item() # add up avg batch loss
            batch_count +=1                

        # print training statistics 
        losses.append(train_loss/batch_count)
        print('Epoch: {} \tTraining Loss: {:.6f}'.format(
            epoch, 
            train_loss/batch_count))
    
    # return all recorded batch losses
    return losses
        
      
```

## 5) Comparing Models

training loss를 비교합니다.


```python
# batchnorm model losses
# this may take some time to train
losses_batchnorm = train(net_batchnorm)
```

    Epoch: 1 	Training Loss: 0.560093
    Epoch: 2 	Training Loss: 0.201311
    Epoch: 3 	Training Loss: 0.137414
    Epoch: 4 	Training Loss: 0.102877
    Epoch: 5 	Training Loss: 0.080025
    Epoch: 6 	Training Loss: 0.063420
    Epoch: 7 	Training Loss: 0.050702
    Epoch: 8 	Training Loss: 0.040656
    Epoch: 9 	Training Loss: 0.032748
    Epoch: 10 	Training Loss: 0.026457
    


```python
# *no* norm model losses
# you should already start to see a difference in training losses
losses_no_norm = train(net_no_norm)
```

    Epoch: 1 	Training Loss: 1.604559
    Epoch: 2 	Training Loss: 0.513751
    Epoch: 3 	Training Loss: 0.377659
    Epoch: 4 	Training Loss: 0.330637
    Epoch: 5 	Training Loss: 0.301410
    Epoch: 6 	Training Loss: 0.278804
    Epoch: 7 	Training Loss: 0.259521
    Epoch: 8 	Training Loss: 0.242428
    Epoch: 9 	Training Loss: 0.227059
    Epoch: 10 	Training Loss: 0.213200
    


```python
# compare
fig, ax = plt.subplots(figsize=(12,8))
#losses_batchnorm = np.array(losses_batchnorm)
#losses_no_norm = np.array(losses_no_norm)
plt.plot(losses_batchnorm, label='Using batchnorm', alpha=0.5)
plt.plot(losses_no_norm, label='No norm', alpha=0.5)
plt.title("Training Losses")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f3252b48ef0>




![png](https://drive.google.com/uc?id=1VsyrVjsXXblGFNPmhoF9SW68AN2MRAVP)

---
**Testing**

Batch normalization이 적용된 모델이 더 낮은 loss로 시작하고, 최종 결과 역시 더 낮은 loss를 보이는 것을 알 수 있습니다. 이번에는 test의 성능에 서 비교해 보겠습니다.  `test`함수에는 `train` 파라미터를 통해 training과 evaulation 모드를 바꿀 수 있습니다. 또한 전달된 모델의 전체 테스트 정확도를 포함해 일부 테스트의 통계를 계산합니다.


```python
def test(model, train):
    # initialize vars to monitor test loss and accuracy
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))
    test_loss = 0.0

    # set model to train or evaluation mode
    # just to see the difference in behavior
    if(train==True):
        model.train()
    if(train==False):
        model.eval()
    
    # loss criterion
    criterion = nn.CrossEntropyLoss()
    
    for batch_idx, (data, target) in enumerate(test_loader):
        batch_size = data.size(0)
        # forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)
        # calculate the loss
        loss = criterion(output, target)
        # update average test loss 
        test_loss += loss.item()*batch_size
        # convert output probabilities to predicted class
        _, pred = torch.max(output, 1)
        # compare predictions to true label
        correct = np.squeeze(pred.eq(target.data.view_as(pred)))
        # calculate test accuracy for each object class
        for i in range(batch_size):
            label = target.data[i]
            class_correct[label] += correct[i].item()
            class_total[label] += 1

    print('Test Loss: {:.6f}\n'.format(test_loss/len(test_loader.dataset)))

    for i in range(10):
        if class_total[i] > 0:
            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
                str(i), 100 * class_correct[i] / class_total[i],
                np.sum(class_correct[i]), np.sum(class_total[i])))
        else:
            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
        100. * np.sum(class_correct) / np.sum(class_total),
        np.sum(class_correct), np.sum(class_total)))
```

**Training and Evaluation Mode**

batch nomalization이 포함된 모델의 경우 evaluation mode로 설정하는 것이 필요합니다.

> * training mode는 batch normalization layer가 **batch statistic**을 사용해 계산하는 것을 의미합니다.
> * 반면에 evaluation mode는 전체 training set에서 추정된 **모집단의 statistic**을 사용하므로 테스트 데이터에서 성능이 향상합니다.


```python
# test batchnorm case, in *train* mode
test(net_batchnorm, train=True)
```

    Test Loss: 0.086511
    
    Test Accuracy of     0: 98% (969/980)
    Test Accuracy of     1: 99% (1124/1135)
    Test Accuracy of     2: 96% (1001/1032)
    Test Accuracy of     3: 97% (987/1010)
    Test Accuracy of     4: 96% (948/982)
    Test Accuracy of     5: 97% (870/892)
    Test Accuracy of     6: 96% (927/958)
    Test Accuracy of     7: 96% (996/1028)
    Test Accuracy of     8: 97% (945/974)
    Test Accuracy of     9: 95% (965/1009)
    
    Test Accuracy (Overall): 97% (9732/10000)
    


```python
# test batchnorm case, in *evaluation* mode
test(net_batchnorm, train=False)
```

    Test Loss: 0.073191
    
    Test Accuracy of     0: 98% (968/980)
    Test Accuracy of     1: 99% (1125/1135)
    Test Accuracy of     2: 97% (1011/1032)
    Test Accuracy of     3: 97% (986/1010)
    Test Accuracy of     4: 97% (957/982)
    Test Accuracy of     5: 97% (871/892)
    Test Accuracy of     6: 97% (932/958)
    Test Accuracy of     7: 97% (1000/1028)
    Test Accuracy of     8: 96% (942/974)
    Test Accuracy of     9: 97% (986/1009)
    
    Test Accuracy (Overall): 97% (9778/10000)
    


```python
# for posterity, test no norm case in eval mode
test(net_no_norm, train=False)
```

    Test Loss: 0.204425
    
    Test Accuracy of     0: 98% (966/980)
    Test Accuracy of     1: 98% (1114/1135)
    Test Accuracy of     2: 91% (944/1032)
    Test Accuracy of     3: 93% (941/1010)
    Test Accuracy of     4: 93% (919/982)
    Test Accuracy of     5: 93% (831/892)
    Test Accuracy of     6: 95% (911/958)
    Test Accuracy of     7: 92% (953/1028)
    Test Accuracy of     8: 91% (889/974)
    Test Accuracy of     9: 93% (943/1009)
    
    Test Accuracy (Overall): 94% (9411/10000)
    

## 6) Which model has the highest accuracy?


traning 및 evaluation 모드에서 batch normalizaiton 모델의 정확도를 비교해 보면 약간 개선된 것을 알 수 있습니다. 또한 batch normalization layer를 사용하는 모델이 정규화되지 않은 모델과 비교할 때 전체 정확도가 크게 향상되었음을 알 수 있습니다.

---
## 7) Considerations for other network types

여기서는 fully connected layer에 적용한 것만 봤지만 다른 네트워크에서도 특정 조건을 만족시키면 batch normalization을 적용할 수 있습니다.

**ConvNets**

Convolution layer는 여러 feature map으로 구성됩니다. Convolution layer의 깊이는 feature map의 수를 나타냅니다. 각 feature map의 가중치는 layer에 공급되는 모든 입력에서 공유됩니다. 이러한 차이로 인해 Convolution layer에 적용할 때는 layer의 node 당이 아니라 feature map당 배치 / 모집단의 평균 및 분산을 사용해야 합니다.

> [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)을 사용해 적용할 수 있습니다. 

**RNNs**

2016 년 논문 [Recurrent Batch Normalization] (https://arxiv.org/abs/1603.09025)에서 볼 수 있듯이 batch normalization은 RNN에서도 작동 할 수 있습니다. 구현하는 데 약간의 작업이 필요하지만 기본적으로 layer 대신 time stamp 당 평균 및 분산을 계산해야합니다. PyTorch에서 [this GitHub 리포지토리] (https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)에서 구현한 것을 볼 수 있습니다.
