---
title: "[Deep learning] Nural Networks (5)Gradient Descent"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 5 Gradient Descent

이전에 Gradient Descent의 컨셉에 대해서만 알아보았다면, 이번에는 좀더 수학적인 원리를 살펴보겠습니다. 

![nn33]()

위의 그림을 같이 보면, 처음의 y hat의 예측이 별로일 때, 이을 풀어쓰면 둘째 줄과 같고, 여기서 각각의 error를 계산하는 것이 세번째 줄입니다.

learning rate를 0.1이라 할 때(4번째 줄), w와 b가 업데이트 되고, error가 줄어든 새로운 식이 도출됩니다.


## 1) Gradient Calculation

컴퓨터에게 시키기 전에 직접 계산해 보며 익숙해져 봅시다.

먼저, sigmoid function을 다음과 같이 정리해 볼 수 있습니다.

σ ′(x)=σ(x)(1−σ(x))

이렇게 정리되는 이유는 다음과 같습니다.

![nn34]()

만약 m개의 데이터가 있다면 우리는 