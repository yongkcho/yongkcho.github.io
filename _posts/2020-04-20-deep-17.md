---
title: "[Deep learning] Nural Networks (5)Gradient Descent"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lesson 5 Gradient Descent

이전에 Gradient Descent의 컨셉에 대해서만 알아보았다면, 이번에는 좀더 수학적인 원리를 살펴보겠습니다. 

![nn33](https://drive.google.com/uc?id=1sF2AB1BHBfvtWRtR8gwcMhFy6Zcbzb55)

위의 그림을 같이 보면, 처음의 y hat의 예측이 별로일 때, 이을 풀어쓰면 둘째 줄과 같고, 여기서 각각의 error를 계산하는 것이 세번째 줄입니다.

learning rate를 0.1이라 할 때(4번째 줄), w와 b가 업데이트 되고, error가 줄어든 새로운 식이 도출됩니다.


## 1) Gradient Calculation

컴퓨터에게 시키기 전에 직접 계산해 보며 익숙해져 봅시다.

먼저, sigmoid function을 다음과 같이 정리해 볼 수 있습니다.

$\sigma '\left ( x \right ) = \sigma\left ( x \right )\left (1 - x \right )$

이렇게 정리되는 이유는 다음과 같습니다.

![nn34](https://drive.google.com/uc?id=1KmrCB7ZYUjMQ5Ijhv6y1eGik1N_I1j66)

만약 m개의 데이터가 있다면 우리는 x를 1, 2, 3, .. , m까지 라벨링 할 수 있으며, 이 때의 error function은 다음과 같습니다. 

예측 함수가 $\widehat{y_{i}}  = \sigma\left ( Wx^{\left ( i \right )} + b \right )$임을 고려할 때,

$E = -\frac{1}{m}\sum_{i = 1}^{m}\left (y_{i}ln\left (\widehat{y_{i}}  \right ) + \left ( 1 - y_{i} \right ) ln\left ( 1 -  \widehat{y_{i}} \right )  \right )$

목표는 E의 gradient를 계산하는 것이며, 데이터 포인트 x가 1부터 n까지 있다면 아래와 같이 계산할 수 있습니다.

$\triangledown E = \left ( \frac{\partial }{\partial w_{1}}E, \cdots , \frac{\partial }{\partial w_{n}}E, \frac{\partial }{\partial b}E  \right )$

계산을 단순하게 하기 위해서 각 데이터 포인트 error를 계산하고 이를 미분합니다. Error의 초합은 전체 error의 평균가 같아 집니다. 

$E = -yln\left ( {\widehat{y}}  \right ) - \left ( 1 - y \right )ln\left (1 -  {\widehat{y}}\right )$

이 error를 weights를 반영해 미분하기 위해 우리는 먼저 $\frac{\partial }{\partial w_{j}}\widehat{y}$을 계산해야 합니다. $\widehat{y} = \sigma \left ( Wx +  b \right )$식을 반영하면,

![nn35](https://drive.google.com/uc?id=1ksOAIalkvYXQhupbT31EdVBCDTIVeG9G)

마지막 방정식은 $w_{j}$와 관련해 상수가 아닌 합이 x_{j}을 미분한 $w_{j}x_{j}$이기 때문입니다.

이제 우리는 데이터 포인트 x에 E를 미분할 수 있습니다. 

![nn36](https://drive.google.com/uc?id=1XhX_X9VLctV8GBirW88tR3tC72R3P3uA)

유사한 계산을 수행하면 다음과 같은 값을 얻습니다.

![nn37](https://drive.google.com/uc?id=1_D_aEiZf3Roi6Cs8SFZZ6lvaMb8374S7)

이 계산식은 우리에게 중요한 것을 말해 줍니다. 각각의 데이터 포인트에 대해 라벨은 y, 예측값은 y hat이며, error function을 고려하면 gradient는 다음과 같습니다.

$\triangledown E = -\left ( y - \widehat{y} \right )\left ( x_{1}, \cdots  x_{n}, 1 \right )$

즉, 예측을 잘할 수록 gradient는 줄어들고, 예측이 잘되지 않을 수록 gradient는 증가한다는 것입니다.

이렇게 얻어진 값을 바탕으로 다음과 같이 값을 업데이트 합니다.

![nn38](https://drive.google.com/uc?id=1g3abHSdikFqakF8OCaNBE3EOSMbI6cFL) 