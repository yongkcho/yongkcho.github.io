---
title: "[Deep learning]Nural Networks (11) Under and Overfitting "
date: 2020-04-21
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 11 Under and Overfitting 

지난번에 error와 그를 잡는 gradient descent에 대해서 다뤘습니다. 

여기서 의문이 하나 생깁니다. 과연 error를 **가장 작게** 만드는게 항상 좋은 모델일까요? 

BertrAIND Russell 은 이런 말을 했다고 합니다. 

> "The whole problem with Artificial Inetelligence is that bad models ars so certain of themselves, and good models so full of doubts"

> "AI의 문제는 나쁜 모델은 너무 정확해 보이고, 좋은 모델은 의심스러워 보인다는 것이다."

오늘은 그런 fitting문제에 대해 살펴보겠습니다.


## 1) Early Stopping

![nn63](https://drive.google.com/uc?id=1VSp6tkcqxT54Sx1QcPjB8XmWonxgsTSY)

여기 세 가지 모델이 있습니다. 가장 왼쪽의 모델은 선 하나로 분류를 하고 있는데, 성능이 좋지 않아보입니다. 이런 모델을 아직 피팅이 덜 되었다 해서 under fitting이라고 합니다.

반면 가장 오른쪽 모델은 완벽하게 데이터를 분류해 냅니다. 하지만 만약 새로운 데이터가 들어와도 잘 분류할 수 있을까요? 이런 모델을 너무 피팅이 됐다 해서 over fitting이라고 합니다.

우리의 목표는 가운데에 있는 것 처럼 적절한 수준의 fitting을 만들어 데이터를 잘 분류하게 만드는 것입니다.

불행히도, 모델을 trainning 시키면 시킬수록 데이터를 잘 분류하지만 동시에 overfitting 됩니다.

아래의 예시를 보면, epoch가 20 정도일 때 가장 분류가 잘 된 것을 확인할 수 있습니다. 그 이후로는 overfitting이 일어납니다. 

![nn64](https://drive.google.com/uc?id=1pBr_54H2UoadEMZwmgKCsBkuuVgK_VCN)

이런 overfitting을 방지하는 방법 중 하나는, 적정한 시점에서 멈추는 early stopping입니다.

아래 그래프처럼 이를 알기 위해서는 trainning error와 testing error를 파악해 적정 시점에서 학습을 중지해야 합니다.

![nn65](https://drive.google.com/uc?id=18GOiUpIXp5TOB3kBoi4Nkx6JPlKX7pOx)

큰 coefficient는 overfitting을 낳는 경향이 있습니다. 그래서 weights에 패널티를 주는 것이 필요합니다. 

다음과 같은 방법들을 통해 Regularization을 수행할 수 있습니다. 

![nn66](https://drive.google.com/uc?id=1NZJ7YFxiJCgwjdEnvH5zv0wvMeMOy0Zx)

L1은 보통 feature selection에 적합합니다.sparsity가 높아 작은 weights는 0으로 수렴하기 때문입니다.

반면, L2는 sparsity가 적어 다양한 weights를 반영할 수 있어, Model Trainning에 적합하다고 합니다. 


## 2) Drop out

신경망의 node 중 일부는 너무 잘 훈련되고, 나머지는 훈련되지 않는 경우가 있습니다. 이런 문제는 overfitting을 야기할 수도 있습니다.

이럴 경우에 Drop out을 사용합니다. 전체 nodes중 일부를 tranning에 사용하지 않는 과정을 반복하는 것입니다.

![nn67](https://drive.google.com/uc?id=1sgz6ptMq7Uxknegv5AnZf_EtzEsJZHy3)


## 3) Various Method

Gradient descent가 global minimum이 아닌 local minimum에 빠질 수도 있다는 것을 보았습니다. 

3.1) Random Restart

이런 문제를 해결하기 위한 방법 중 하나가 Random restart입니다. 여러번 trainning을 반복해 보는 것입니다.

![nn68](https://drive.google.com/uc?id=19chxm1jfcad5JpKYv6irDgS_LOXwMwEZ)

하지만, 몇 번을 반복해도 그것이 local minimum이라는 확신을 가지기는 쉽지 않습니다.

3.2) Other Activaiton Function

Sigmoid가 보통 값들을 작게 변환해 vanishing grdient문제를 발생시키기도 합니다. 

이럴 때 Activation Function을 바꿈으로서 문제를 해결해 볼 수 있습니다.

첫 번째 Activation은 Hyperbolic Tangent입니다. 본 프로그램에 따르면 보통 신경망의 성능이 상승한다고 합니다. (believe it or not)

![nn69](https://drive.google.com/uc?id=1ldeQ-PC4UxFWTOCFxzL_UoO-Sw58QFlu)

두 번째는 ReLU입니다. 상당히 간단한데요. 1과 0으로 구분합니다.

![nn70](https://drive.google.com/uc?id=1mZUuN1UPD_tKa1fKnuWHMQoo5Tj4rDMt)


3.3) Batch vs stochastic gradient descent

우리가 가진 모든 데이터를 집어 넣고, gradient descent를 반복하면 매우 정확한 방법일 수 있지만 데이터가 커질수록 어마어마한 연산을 하게 된다는 문제가 있습니다. 

Batch는 데이터를 분할 해서 신경망을 훈련하고, gradient descent를 수행하는 것을 의미합니다. batch size와 반복횟수에 따라 성능 달라집니다.

Stochastic은 추출된 데이터에 대해서 gradient descent를 구하는 것입니다. 당연히 빠르지만, 적정한 값을 찾지 못할 가능성이 있습니다.


3.4) Momentum 

0~1사이의 베타 값을 gradient descent의 각 스탭에 곱해 global minimum을 극복하려는 방법입니다.

![nn71](https://drive.google.com/uc?id=1Qfbk_7r67iaqR3EPxJYDSSYCgMHMruUX)



