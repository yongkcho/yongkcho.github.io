---
title: "[Deep learning]Transfer Learning (1) AWS overview"
date: 2020-05-11
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study AWS Cloud
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# Lesson 1 Transfer Learning

CNN을 활용한 VGN-16이나 ResNet같은 훌륭하게 디자인된 네트워크들이 있습니다.

그런데 이런 네트워크들을 가져와서 필요한 분류에 사용할 수는 없을까요? **Transfer Learning**은 바로 이 아이디어에서 출발했습니다.


## 1)Useful Layers

VGG는 1,000개의 다른 class를 분류하는 네트워크입니다. 각 layer는 class를 구분하기 위한 feature들을 가지고 있습니다. 

[지난번]에 살펴본 것 처럼, 각 filter는 선의 형태, 모양 등을 인지하고, 분류합니다. 

![cnn35](https://drive.google.com/uc?id=1SSqlFpqAD6qbF2iX4VLRALruDcGxlCtF)

이런 filter들을 활용해 원하는 분류에 사용해 볼 수 있게 사용하며, 마지막에 1~2개의 linear layer를 추가해 분류에 활용하는 것이 기본적 아이디어 입니다.

![cnn36](https://drive.google.com/uc?id=1pveIm9ODe0zM3sTer3RnrR4_Oh_-0g59)

dataset의 사이즈와, 데이터베이스의 유사성에 기초해 다른 네트워크들을 사용해 볼 수도 있습니다. 


## 2) Overview of Transfer Learning

Udacity의 공동 창업자 중 한명인 Sebastian Thrun은 논문에서 CNN을 사용해 피부암을 진단하는 네트워크를 만들었습니다.

이 네트워크를 만드는 과정에서 ImageNet에서 훈련된 네트워크를 Transfer Learning을 통해 사용하였다고 합니다. 

그리고 전체의 weights를 변형해가며 새롭게 훈련을 하는 **Fine Tunning**을 수행했다고 합니다.

![cnn37](https://drive.google.com/uc?id=1eQO-TTM2G2BeguPqDM8w4TxKeAc3NEVD)

Transfer Learning은 다음과 같은 것들을 먼저 생각해 보아야 합니다.

* 새롭게 분류하고자 하는 dataset의 크기

* 기존의 dataset과 새로운 dataset의 유사성에

Transfer Learning에는 보통 4가지의 케이스가 있습니다.

* 1) 새로운 dataset이 작고 기존의 dataset과 유사한 경우

* 2) 새로운 dataset이 작지만 기존의 dataset과 다른 경우

* 3) 새로운 dataset이 크고 기존의 dataset과 유사한 경우

* 4) 새로운 dataset이 크고 기존의 dataset과 다른 경우

큰 dataset의 수백만개의 이미지를 가질 수 있고, 작은 dataset은 2,000개의 이미지를 가질 수도 있습니다. 

dataset의 크기는 주관적이지만, overfitting의 가능성 때문에 생각해 볼 필요가 있습니다. 

늑대와 개의 이미지의 dataset은 유사하지만, 개와 꽃은 유사하지 않습니다. 

이 4가지 경우에는 각기 다른 접근법이 있습니다. 각 방식에 대해 살펴보겠습니다.

![cnn38](https://drive.google.com/uc?id=1oniT417SICbPO3bc4OHmGxV17wdHA2TH)

이 방식들을 이해하기 위해서 먼저 훈련된 네트워크를 가지고 와서 각 경우에 적용해 보겠습니다. 

이 예시에는 convolutional network 3개와 fully connected layer 3개가 있습니다.

![cnn39](https://drive.google.com/uc?id=1uF7t4hUL_izmc4z24XxUSo3I7zMaAW5w)

convolutional networks는 다음과 같은 일을 합니다.

* 첫번째 layer는 이미지의 경계를 탐색합니다.
 
* 두번째 layer는 이미지의 모양을 탐색합니다.

* 마지막 layer는 higher level feature를 탐색합니다.


## 3) Small Data Set, Similar Data

![cnn40](https://drive.google.com/uc?id=1Na5Dw-tW1ra3Pb-XY_lp45J0zqbdNYJh)

만약 새로운 데이터가 기존과 유사하고 작다면,

* 신경망의 마지막을 잘라냅니다.

* 새로운 데이터셋의 class의 숫자와 맞는 fully connected layer를 추가합니다.

* 새로운 layer의 weights를 랜덤하게 설정합니다. / 기존 네트워크의 weights는 유지합니다.

* 신경망을 다시 훈련시키며 새로운 layer의 weights를 최적화합니다.

작은 dataset의 overfitting을 막기 위해서 기존 신경망의 weights를 유지합니다.

dataset이 유사하기 때문에 기존과 higher level feature역시 유사합니다. 이를 시각화하면 다음과 같습니다.

![cnn41](https://drive.google.com/uc?id=1b1Y5Tm5hfrFLs1HD1mo4RJ2AqcSiIsZt)


## 4) Small Data Set, Different Data

![cnn42](https://drive.google.com/uc?id=1S7dawza3J5E59qDoP0MNePP83gLlReQF)

새로운 데이터가 기존과 다르다면,

* 신경망의 시작 부분 근처에서 훈련된 레이어를 제외한 모든 레이어를 잘라냅니다.

* 나머지 레이어에 새로운 dataset의 class 수와 일치하는 fully connected 레이어를 추가합니다.

* fully conntected된 새로운 layer의 weights를 랜덤하게 설정합니다. 기존 신경망의 weights는 고정합니다. 

* 신경망을 훈련하며 새로운 layer에 weights를 업데이트 합니다.

dataset의 크기가 작기 때문에 overfitting에 주의해야 합니다. 이를 방지하기 위해 여전히 기존 weights를 유지합니다.

하지만 higher level feature들을 공유하지는 않습니다. 그렇기 때문에 기존 신경망은 lower level feature들에서만 활용됩니다.

![cnn43](https://drive.google.com/uc?id=1lh8RtTDjNX-2P_4l-uacNH56ScAksN2f)


## 5) Large Data Set, Similar Data

![cnn44](https://drive.google.com/uc?id=1-8nnTLJFDEc_OTCiNMtlaAEJZPtXJNib)

새로운 dataset이 크고 기존과 유사하다면, 

* 마지막 fully connected layer를 삭제하고 새로운 dataset의 class와 일치하는 새로운 layer를 추가합니다.

* 새로운 fully connected layer에 weight를 랜덤하게 설정합니다.

* 기존 신경망의 weights를 초기화 합니다.

* 전체 신경망을 새롭게 훈련시킵니다.

dataset의 규모가 크기 때문에 overfitting에 대해서는 덜 걱정해도 되고, 전체 weights를 재설정 해도 됩니다.

기존과 새로운 데이터가 higher level feature를 고용하기에, 신경망은 그대로 유지해도 됩니다.

![cnn45](https://drive.google.com/uc?id=1d23QZu1kQk2SOAqvzjgN1b6zWmuJ6dFg)


## 6) Large Data Set, Different Data

![cnn46](https://drive.google.com/uc?id=13ZXlBB6qmcayWQa7fowj18Sq1qLcorm5)

새로운 dataset이 크고 기존과 다르다면,

* 마지막 fully connected layer를 삭제하고 새로운 dataset의 class와 일치하는 새로운 layer를 추가합니다.

* 신경망을 랜덤한 weights를 활용해 다시 훈련합니다.

* 또는, 5번과 동일한 방법으로 접근할 수도 이습니다.

기존 data와 다름에도 불구하고, 기존 신경망의 weight를 초기화 하는 것이 훈련을 빠르게 만들어줍니다. 

만약 weights를 초기화 해 재 훈련 시키는 것이 성공적이지 않을 경우에는 5번의 방법을 활용해 볼 수도 있습니다.

![cnn47](https://drive.google.com/uc?id=1bm9uGZll-LQ-Jc-63MBc0Ib-eavAclx8)


## 7) 더 알아보기

* [이 논문](https://arxiv.org/pdf/1411.1792.pdf)을 참고해 훈련된 CNN을 어떻게 활용하는지 참고해 볼 수 있습니다.

* [이 논문](http://www.nature.com/articles/nature21056.epdf?referrer_access_token=_snzJ5POVSgpHutcNN4lEtRgN0jAjWel9jnR3ZoTv0NXpMHRAJy8Qn10ys2O4tuP9jVts1q2g1KBbk3Pd3AelZ36FalmvJLxw1ypYW0UxU7iShiMp86DmQ5Sh3wOBhXDm9idRXzicpVoBBhnUsXHzVUdYCPiVV0Slqf-Q25Ntb1SX_HAv3aFVSRgPbogozIHYQE3zSkyIghcAppAjrIkw1HtSwMvZ1PXrt6fVYXt-dvwXKEtdCN8qEHg0vbfl4_m&tracking_referrer=edition.cnn.com)에서 Sebastian Thrun이 imageNet을 활용해 피부암을 발견하는 솔루션을 만든 과정을 확인할 수 있습니다.