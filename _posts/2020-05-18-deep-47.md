---
title: "[Deep learning]RNN (3) Basic Structure of RNN"
date: 2020-05-14
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# Lesson 3 Basic Structure of RNN

이제 본격적으로 RNN에 대해 살펴보겠습니다. RNN은 input sequence에 대해 같은 작업을 반복하기에 붙여진 이름입니다.

![rnn19]()


## 1) Introduction

states라고 하는 internal memory element를 활용하는 것이 특징입니다. 

RNN은 문장 속에서 다음 단어를 예측하거나, 감성분석, sepeach recognition, NLP, Gesture Recognition등 다양한 분야에서 사용됩니다.

RNN은 Feed Forward 신경망 안에 있는 다양한 규칙을 활용합니다. 그렇기 때문에 이 과정에 대해 다시 살펴본 것입니다.

RNN과 Feed Forward의 차이점은 2가지가 있습니다. 

* input과 output의 정의가 약간 다릅니다. RNN은 single input만 받는것이 아니라 앞의 input까지 sequence로 받습니다.

* 현재의 input과 뉴런은 다음 timestamp에서 input으로 사용됩니다. feedforward에서는 선형적으로 진행되어 feedback이 없지만, RNN은 feedback을 받아 활용합니다.

![rnn20]()

가장 기본적인 RNN 구조는 3개의 layer를 활용하는 **Elamn Network(또는 Simple RNN)**입니다. 

![rnn21]()

RNN의 역사에서 살펴봤듯이 이는 1990년에 [제안](https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1)되었습니다. 

더 알고 싶다면 [이곳]()을 참고하시면 됩니다. 


## 2) Visualize RNN

foled model

undofled model