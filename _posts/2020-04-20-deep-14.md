---
title: "[Deep learning] Nural Networks (3) Error, Activation function"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 3 Error Function 

현실에서 데이터는 선형으로 분류되지 않는 경우가 많습니다. 우리는 두 선을 긋거나, 원을 그릴수도 있고, 아니면 곡선을 택할 수도 있을 것입니다.

하지만 perceptron은 처음부터 곡선을 그릴 수는 없습니다. 다양한 선을 그어 데이터를 잘 분류하는 것을 찾아내는 과정이 필요합니다.

이 과정에서 어디로 가야할지를 계산해 모델에게 알려주는 것이 바로 error function입니다.


## 1) Log-loss Error Function

만약 우리가 정상에서 하산해야 한다고 상상해 봅시다. 우리는 가장 많이 내려갈 수 있는 효율적인 동선을 택할 것입니다.

![nn14](https://drive.google.com/uc?id=1c7LlD6rRMH0Mpbl3DAmX9dsJF71jh2gu)

gradient descent라고도 부르는 이 방법은 모델의 error를 측정하고, 점진적으로 줄일 수 있게 모델을 개선합니다. 

이 간단해 보이는 컨셉을 어떻게 컴퓨터에게 전달해 볼 수 있을까요?

아래 예시에서 우리의 선형회귀식은 2개의 error를 가지고 있습니다. 선을 약간 바꾸면 정확한 분류가 가능할 것입니다. 

![nn15](https://drive.google.com/uc?id=1PYL-gYAwq9ntbWJqe9zYsj0LQ4sTit1Q)

여기서 에러를 줄이는 방법은 이산적(discrete)으로 할 수도, 연속적(continuos)으로 할 수도 있습니다.

만약 이산적으로 구분했을 경우에는, 점을 조금 옮겨도 여전히 error의 개수가 동일할 겨웅 개선이 멈출 수 있는 위험이 있습니다.

![nn16](https://drive.google.com/uc?id=1VCgshVVmhXlBSjB95REHVu2m7BaX7Uea)

하지만 잘못 분류된 point의 error에 가중치를 주는 방식으로 분류한다면 훨씬 효과적인 분류가 될 것입니다.

![nn17](https://drive.google.com/uc?id=13A3uB-V_poFi7KQn4ESUiwcc1SLKYEe-)

이렇게 error를 줄이는 방식으로 계속해 반복하며 모델의 성능을 높일 수 있습니다.


## 2) Activation Function

이산과 연속에 대해 조금 더 살펴보겠습니다. 둘의 차이를 다시 살펴보면 아래와 같습니다. 

![nn18](https://drive.google.com/uc?id=1abM5nTCLTBdpPcymCmEbck9uw53ZC_Na)

이산형 분류는 yes나 no로 답을 내는 반면, 연속적 분류는 얼마나 잘 분류가 되고 있는지를 확률적으로 표현한다는 것입니다.

이런 분류를 하기 위해 Activation function은 어떻게 설정해야 할까요? 

![nn19](https://drive.google.com/uc?id=1WpJKCo0vRIECEMYB6eDELjtOL5z2JJ-a)

좌측의 Step function은 이산적으로 1과 0을 구분합니다. 반면, 우측의 Sigmoid function은 연속적으로 값을 나타내고 있습니다. 

이를 예측값과 함께 살펴보면 다음과 같습니다. Wx + b에 따라, 예측값인 y hat의 확률이 변화하는 것을 볼 수 있습니다. 

이러한 sigmoid가 step 대신 activation function으로 쓰인다면 어떨까요?

![nn20](https://drive.google.com/uc?id=1_NJ63SfGGmn7a4R4BBeH6wjgPw1koF9I)

이와 같이 단순히 '1'과 '0'이 아닌, 확률값을 연속적으로 반환하게 됩니다. 이를 gradient descent에 효과적으로 활용할 수 있습니다. 


## 3) Softmax Function

지금까지는 1과 0을 분류하는 문제만을 살펴봤습니다. 그런데, 3가지 이상을 분류하고 싶으면 어떻게 해야할까요?

여기 오리, 비버 그리고 물개가 있습니다. 우리의 모델은 sigmoid를 써서 다음과 같은 확률을 계산해 주고, 여기에 점수를 부여했습니다.

![n21](https://drive.google.com/uc?id=1TELJTI46ZhwoFgsECMXR8yKiVXERhDbC)

이 값들을 어떻게 최종 결과로 바꿔 볼 수 있을까요? 전체를 더해, 값이 차지하는 비율을 확인해 볼 수도 있을 것 같습니다.

![n22](https://drive.google.com/uc?id=1EwqHO_GaEROXFeFOOrcGTChgMN1M0uJi)

하지만 만약 값 중에 음수가 있다면 문제가 발생합니다. 하지만 이 아이디어 자체는 나쁘지 않은데요. 어떻게 음수를 양수로 바꿀 수 있을까요?

우리는 **exp**를 통해 음수를 양수로 전환해 계싼해 볼 수 있습니다.

![n23](https://drive.google.com/uc?id=10aYK-Nirsncht3i6CV-W8NOE-gFvWNuT)

이렇게 exp를 통해 3개 이상의 classification을 해 주는 softmax를 파이썬에서 구현해 봅시다.

```python
import numpy as np

def softmax(L):
    expL = np.exp(L)
    sumExpL = sum(expL)
    result = []
    for i in expL:
        result.append(i*1.0/sumExpL)
    return result
    
    # Note: np.divide를 활용할 수도 있습니다.
    # def softmax(L):
    #     expL = np.exp(L)
    #     return np.divide (expL, expL.sum())
```


## 4) One Hot Encoding

이렇게 세 가지 이상의 분류가 있는 데이터를 처리할 때, 각 데이터에 1, 2, 3처럼 값을 줄 수도 있겠습니다.

하지만 그것보다는 각 분류의 항목을 만드는 **One Hot Encoding**이 더 효율적이고, 많이 사용됩니다.

이러한 one hot encoding은 다음과 같은 식으로 이뤄집니다.

![n24](https://drive.google.com/uc?id=1j4fJb8ep7XaKM7P9UxEwkqyhBeE2evzD)