---
title: "[Deep learning]RNN (6) Hyperparameters"
date: 2020-05-21
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson 6 Hyperparameters


Hyperparameter는 dataset에 알고리즘을 적용하기 위해 설정하는 수치들입니다. 대표적으로 다음과 같은 것들이 있습니다.

```python
# Hyperparameters

learning_rate = 0.01
minibatch_size = 32
epoches = 12
```

이런 값들을 어떻게 정해야 할까요? 정해진 답은 없지만 좀 더 살펴보겠습니다.

Hyperparameter에는 2가지 유형이 있습니다. 첫 번째는 **Optimizer Hyperparameter**입니다. 

모델 보다는 훈련과정에서의 최적화를 위해 사용되는 값들입니다. 다음과 같은 것들이 포함됩니다.

* learning_rate

* minibatch_size

* epoches

![rnn33](https://drive.google.com/uc?id=1_D2iPZjJ0YYhDU94n3mT9V1-TPqMiMte)

다음 유형은 **Model Hyperparameter**입니다. 보다 모델의 구조와 관련이 있습니다.

* Number of Layers, Hidden Layers

* Hyperparameters for architecture

![rnn34](https://drive.google.com/uc?id=1rjd-MR8HVkR9CZhqswFcM0mpqFk32FMn)

먼저 가장 중요하다고 볼 수 있는 **learning_rate**에 대해 살펴봅시다.


## 1) Learning Rate

> "The single most important hyperparameter and one should always make sure that it has been tuned" - Yoshua Bengio

어떤 모델을 사용하던지 가장 먼저 하게될 일은 learning rate를 튜닝하는 것입니다.

training error가 줄어들지만 그 값이 너무 작다면, 신경망이 무엇을 배우고 있긴 하다는 걸 알 수 있습니다. 이 때는 learning rate를 높이는 것이 필요합니다.

```python
Epoch 1, Batch 1, Training Error: 8.4181
Epoch 1, Batch 2, Training Error: 8.4177
Epoch 1, Batch 3, Training Error: 8.4177
Epoch 1, Batch 4, Training Error: 8.4173
Epoch 1, Batch 5, Training Error: 8.4169
```

training error가 들쭉날쭉하다면 신경망이 배우기는 하지만, error의 변동폭이 높기 때문에 learning rate를 줄이는 것이 필요합니다. 

```python
Epoch 1, Batch 1, Training Error: 8.71
Epoch 1, Batch 2, Training Error: 3.25
Epoch 1, Batch 3, Training Error: 4.93
Epoch 1, Batch 4, Training Error: 3.30
Epoch 1, Batch 5, Training Error: 4.82
```

만약 dataset이 정규화되어 있다면 좋은 스타팅 포인트는 **0.01**입니다. 여기서 더 큰값이나(0.1) 더 작은 값(0.001)을 적용하며 적절한 값을 찾아가야 합니다. 

이 때 참고해야 할 지표가 바로 **training error**입니다. Gradient Decent에서 loss function을 사용해 모델을 최적화합니다. 

learning rate는 여기서 얼마나 error를 반영할지 결정하는 hyperparameter입니다. 

learning rate가 너무 크다면 loss가 갈수록 커질 것이고, 너무 작다면 지나치게 오랜 시간동안 training을 할 수도 있습니다. 

![rnn35](https://drive.google.com/uc?id=1-8tLpJ308RiXethGsoz-oTJfk3vumNPh)

문제를 더 어렵게 만드는 것은 error curve가 항상 곡선은 아니라는 것입니다. 최적점이 어디에 있을지 찾는것이 쉽지 않습니다. 

![rnn36](https://drive.google.com/uc?id=1FL_iyrDpOqd0sJubxs3YMBcEGl_9hiZ2)

learning rate에 따라 모델이 최적 학습을 하지 못할 수도 있습니다. 이런 문제를 **Learning Rate Decay**라고 합니다.  

가장 직관적으로 이를 해결하는 방법은 선형적으로 learning rate를 줄여 나가는 것입니다. 5번의 epoch마다 learning rate를 줄여볼 수 있습니다.

![rnn37](https://drive.google.com/uc?id=1brAICb4sJc2IIVoe_xSyxv3RoO7u03yw)

또는 expotential 하게 줄여볼 수도 있습니다.

![rnn38](https://drive.google.com/uc?id=1f2aa8nqpL7y-aKwMWwS2G4zy4hBfYHeH)

이보다 더 나은 방식은 **Adaptive Learning**이 있습니다. learning rate가 크고 작은지를 파악해 수정하는 알고리즘입니다.


## 2) Minibatch Size

이미 몇번 봤던 Minibatch Size입니다. 신경망 훈련에 있어서 아래와 같은 두 방식이 제시되었고 어떤 것이 더 적합한지는 아직도 논쟁 중이라고 합니다.

![rnn39](https://drive.google.com/uc?id=1yTnIPktjDq2YBnbAaIj527M5_xz7cEn0)

한 데이터마다 forward pass, carculate error, backprop을 반복하는 것이 ***online(또는 stocastic)**이고, 

전체 중 일정한 size의 데이터를 batch로 넣어 해당 과정을 거치는 것이 **stocastic**입니다.

batch가 없이 training을 할 때는 accuracy가 떨어지나, batch로 할 경우에는 비교적 안정적인 것을 확인할 수 있습니다.

![rnn40](https://drive.google.com/uc?id=1IWPhP96pvpEJe6j9HqmTwYN2sqNLY_QM)

위 그래프는 ImageNet에서 CNN을 활용해 계산한 것으로 [여기](https://arxiv.org/abs/1606.02228)서 확인해 보실 수 있습니다.

batch가 너무 작으면 느리게 배우고 너무 크면 정확도가 떨어질 수 있습니다. **32, 64, 128, 256**사이에서 정하는 것이 일반적입니다.


## 3) Number of Epochs

얼마나 학습을 진행해야 할지는 validation error를 통해 알 수 있습니다.

언제 epoch를 멈추는 것이 좋을지 파악해 훈련을 멈추는 것을 **early stopping**이라고 합니다.

![rnn41](https://drive.google.com/uc?id=1Lfg_TEKfeiBuqv9jJA_ecmcL4dXThh3w)

