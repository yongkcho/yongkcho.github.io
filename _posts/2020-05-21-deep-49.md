---
title: "[Deep learning]RNN (6) Hyperparameters"
date: 2020-05-21
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson 6 Hyperparameters


Hyperparameter는 dataset에 알고리즘을 적용하기 위해 설정하는 수치들입니다. 대표적으로 다음과 같은 것들이 있습니다.

```python
# Hyperparameters

learning_rate = 0.01
minibatch_size = 32
epoches = 12
```

이런 값들을 어떻게 정해야 할까요? 정해진 답은 없지만 좀 더 살펴보겠습니다.

Hyperparameter에는 2가지 유형이 있습니다. 첫 번째는 **Optimizer Hyperparameter**입니다. 

모델 보다는 훈련과정에서의 최적화를 위해 사용되는 값들입니다. 다음과 같은 것들이 포함됩니다.

* learning_rate

* minibatch_size

* epoches

![rnn33](https://drive.google.com/uc?id=1_D2iPZjJ0YYhDU94n3mT9V1-TPqMiMte)

다음 유형은 **Model Hyperparameter**입니다. 보다 모델의 구조와 관련이 있습니다.

* Number of Layers, Hidden Layers

* Hyperparameters for architecture

![rnn34](https://drive.google.com/uc?id=1rjd-MR8HVkR9CZhqswFcM0mpqFk32FMn)

먼저 가장 중요하다고 볼 수 있는 **learning_rate**에 대해 살펴봅시다.


## 1) Learning Rate

> "The single most important hyperparameter and one should always make sure that it has been tuned" - Yoshua Bengio

어떤 모델을 사용하던지 가장 먼저 하게될 일은 learning rate를 튜닝하는 것입니다.

training error가 줄어들지만 그 값이 너무 작다면, 신경망이 무엇을 배우고 있긴 하다는 걸 알 수 있습니다. 이 때는 learning rate를 높이는 것이 필요합니다.

```python
Epoch 1, Batch 1, Training Error: 8.4181
Epoch 1, Batch 2, Training Error: 8.4177
Epoch 1, Batch 3, Training Error: 8.4177
Epoch 1, Batch 4, Training Error: 8.4173
Epoch 1, Batch 5, Training Error: 8.4169
```

training error가 들쭉날쭉하다면 신경망이 배우기는 하지만, error의 변동폭이 높기 때문에 learning rate를 줄이는 것이 필요합니다. 

```python
Epoch 1, Batch 1, Training Error: 8.71
Epoch 1, Batch 2, Training Error: 3.25
Epoch 1, Batch 3, Training Error: 4.93
Epoch 1, Batch 4, Training Error: 3.30
Epoch 1, Batch 5, Training Error: 4.82
```

만약 dataset이 정규화되어 있다면 좋은 스타팅 포인트는 **0.01**입니다. 여기서 더 큰값이나(0.1) 더 작은 값(0.001)을 적용하며 적절한 값을 찾아가야 합니다. 

이 때 참고해야 할 지표가 바로 **training error**입니다. Gradient Decent에서 loss function을 사용해 모델을 최적화합니다. 

learning rate는 여기서 얼마나 error를 반영할지 결정하는 hyperparameter입니다. 

learning rate가 너무 크다면 loss가 갈수록 커질 것이고, 너무 작다면 지나치게 오랜 시간동안 training을 할 수도 있습니다. 

![rnn35](https://drive.google.com/uc?id=1-8tLpJ308RiXethGsoz-oTJfk3vumNPh)

문제를 더 어렵게 만드는 것은 error curve가 항상 곡선은 아니라는 것입니다. 최적점이 어디에 있을지 찾는것이 쉽지 않습니다. 

![rnn36](https://drive.google.com/uc?id=1FL_iyrDpOqd0sJubxs3YMBcEGl_9hiZ2)

learning rate에 따라 모델이 최적 학습을 하지 못할 수도 있습니다. 이런 문제를 **Learning Rate Decay**라고 합니다.  

가장 직관적으로 이를 해결하는 방법은 선형적으로 learning rate를 줄여 나가는 것입니다. 5번의 epoch마다 learning rate를 줄여볼 수 있습니다.

![rnn37](https://drive.google.com/uc?id=1brAICb4sJc2IIVoe_xSyxv3RoO7u03yw)

또는 expotential 하게 줄여볼 수도 있습니다.

![rnn38](https://drive.google.com/uc?id=1f2aa8nqpL7y-aKwMWwS2G4zy4hBfYHeH)

이보다 더 나은 방식은 **Adaptive Learning**이 있습니다. learning rate가 크고 작은지를 파악해 수정하는 알고리즘입니다.
