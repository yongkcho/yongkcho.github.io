---
title: "[Deep learning]Nural Networks (13) Sentiment Classification(3)"
date: 2020-04-28
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# 감성 분류 및 신경망을 통한 문제 해결

Andrew Trask는 신경망을 통해 영화 리뷰를 Positive와 Negative로 분류합니다. 

이 내용을 번역하고, 제가 배운 내용을 함께 정리해 두었습니다.

## 5)  Reducing Noise by Strategically Reducing the Vocabulary

[지난번 코드](https://yongkcho.github.io/python/udacity/study/deeplearning/ai/nural/network/deep-25/)를 한 번 더 수정해 보겠습니다. 성능 증진을 위해 단어들 중 상대적으로 쓸모가 적은 것들을 줄여보도록 하겠습니다. 

이 작업을 수행하기 위해 이전 코드에서 SnetimentalNetwork 내 pre_process_data를 수정하겠습니다.

이번에는 bokeh를 통해 시각화하고, 성능을 높여보겠습니다.


```python
g = open('reviews.txt','r') # What we know!
reviews = list(map(lambda x:x[:-1],g.readlines()))
g.close()

g = open('labels.txt','r') # What we WANT to know!
labels = list(map(lambda x:x[:-1].upper(),g.readlines()))
g.close()
```


```python
import time
import sys
import numpy as np
from collections import Counter #Counter 오브젝트를 만들기 위해 불러옵니다
```


```python
#bokeh를 pip로 인스톨하고, import합니다. 
from bokeh.models import ColumnDataSource, LabelSet
from bokeh.plotting import figure, show, output_file
from bokeh.io import output_notebook
output_notebook()
```

```python
#먼저 positive와 negative word를 선별하겠습니다.
positive_counts = Counter()
negative_counts = Counter()
total_counts = Counter()

for i in range(len(reviews)):
    if(labels[i] == 'POSITIVE'):
        for word in reviews[i].split(" "):
            positive_counts[word] += 1
            total_counts[word] += 1
    else:
        for word in reviews[i].split(" "):
            negative_counts[word] += 1
            total_counts[word] += 1

pos_neg_ratios = Counter()

for term,cnt in list(total_counts.most_common()):
    if(cnt >= 50):
        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)
        pos_neg_ratios[term] = pos_neg_ratio

for word,ratio in pos_neg_ratios.most_common():
    if(ratio > 1):
        pos_neg_ratios[word] = np.log(ratio)
    else:
        pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))
```


```python
#positive와 negative word의 ratio 빈도를 histogram을 통해 나타내겠습니다.
hist, edges = np.histogram(list(map(lambda x:x[1],pos_neg_ratios.most_common())), density=True, bins=100, normed=True)

p = figure(tools="pan,wheel_zoom,reset,save",
           toolbar_location="above",
           title="Word Positive/Negative Affinity Distribution")
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color="#555555")
show(p)
```

![](https://drive.google.com/uc?id=1iMPAAv-FtCib4jYttHB2RuaPlq7ncSEr)

```python
#한번 단순 빈도수를 체크해 보도록 하겠습니다. 
frequency_frequency = Counter()

for word, cnt in total_counts.most_common():
    frequency_frequency[cnt] += 1
```


```python
hist, edges = np.histogram(list(map(lambda x:x[1],frequency_frequency.most_common())), density=True, bins=100, normed=True)

p = figure(tools="pan,wheel_zoom,reset,save",
           toolbar_location="above",
           title="The frequency distribution of the words in our corpus")
p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color="#555555")
show(p) #zipvian distribution으로 치우쳐저있음 

```
![](https://drive.google.com/uc?id=1vFoeLNxehBBXOeObe2_f1d-wZiDvvfQ2)

단순 빈도수를 보니 완전히 한쪽으로 치우쳐져 있는 것을 알 수 있습니다. 

조금더 processing을 한다면 성능이 올라갈 것 같습니다. 


```python
#  신경망을 encapsulate해 class로 만듦니다.

class SentimentNetwork:
    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1,  hidden_nodes = 10, learning_rate = 0.1):
        # min_count는 최소 등장횟수입니다. 이 이상 단어가 등장해야 추가됩니다. 
        # polarity_cutoff는 positive와 negative의 비율을 바탕으로 이 이상이 되어야만 학습에 사용합니다.
        
        np.random.seed(1) #seed를 설정해 디버깅을 쉽게 합니다.
        
        self.pre_process_data(reviews,labels, polarity_cutoff, min_count) #리뷰와 라벨을 trainning전에 정리합니다.
        #polarity_cutoff와 min_count를 추가하였습니다.
        
        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate) #input node의 개수만큼 인자를 받아 신경망을 초기화 시킵니다.
        
        
    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):
        #역시 polarity_cutoff와 min_count를 추가하였습니다.
        
        #이번에 추가된 코드입니다.
        positive_counts = Counter()
        negative_counts = Counter()
        total_counts = Counter()
        
        for i in range(len(reviews)):
            if(labels[i] == 'POSITIVE'):
                for word in reviews[i].split(" "):
                    positive_counts[word] += 1
                    total_counts[word] += 1
            else:
                for word in reviews[i].split(" "):
                    negative_counts[word] += 1
                    total_counts[word] += 1
                    
        pos_neg_ratios = Counter()
        
        for term,cnt in list(total_counts.most_common()):
            if(cnt >= 50):
                pos_neg_ratio = positive_counts[term] / float(negative_counts[term] + 1)
                pos_neg_ratios[term] = pos_neg_ratio
        
        for word,ratio in pos_neg_ratios.most_common():
            if(ratio > 1):
                pos_neg_ratios[word] = np.log(ratio)
            else:
                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))
        #여기까지가 새로 추가되었습니다.
        
        review_vocab = set()
        for review in reviews: #주어진 review에 있는 모든 단어를 바탕으로 review_vocab을 채웁니다.
            for word in review.split(" "):
                #새롭게 추가된 부분입니다. polarity_cutoff를 넘고, pos_neg ratio가 의미있는 것만 추가합니다.
                if(total_counts[word] > min_count):
                    if(word in pos_neg_ratios.keys()):
                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):
                            review_vocab.add(word)
                        else:
                            review_vocab.add(word)
        
        self.review_vocab = list(review_vocab) #vocabulary 집합을 list로 만들어 index를 바탕으로 접근할 수 있게 만듦니다.
        
        label_vocab = set() #주어진 label역시 review와 같은 작업을 수행합니다.
        for label in labels:
            label_vocab.add(label)
            
        self.label_vocab = list(label_vocab)
        
        self.review_vocab_size = len(self.review_vocab) #review와 label의 size를 저장해 둡니다.
        self.label_vocab_size = len(self.label_vocab)
        
        self.word2index = {} #dictionary를 만들어 label을 index 위치에 mapping합니다.
        for i,word in enumerate(self.review_vocab):
            self.word2index[word] = i
        
        self.label2index = {}
        for i,label in enumerate(self.label_vocab):
            self.label2index[label] = i
    
    
    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        #input, hidden, output layer, learning_rate의 숫자를 정합니다.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        
        #weights를 초기화합니다.
        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes)) #input과 hidden layer 사이의 weights
        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                            (self.hidden_nodes, self.output_nodes))
        
        self.layer_0 = np.zeros((1, input_nodes)) #input layer를 만들어줍니다.
        
    def update_input_layer(self, review):
        
        self.layer_0 *= 0 #input layer를 모두 0으로 초기화합니다.
        
        for word in review.split(" "):
            if(word in self.word2index.keys()):
                #self.layer_0[0][self.word2index[word]] += 1 이전 코드에서는 이렇게 했습니다.
                self.layer_0[0][self.word2index[word]] = 1 #코드를 이렇게 변경하였습니다. 
        
        
    def get_target_for_label(self, label): #positive를 1로, negative를 0으로 변환합니다.
        if(label == 'POSITIVE'):
            return 1
        else:
            return 0
        
    def sigmoid(self, x): #sigmoid 함수를 정의합니다.
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_output_2_derivative(self, output):
        return output * (1 - output)
    
    def train(self, training_reviews, training_labels):
        assert(len(training_reviews) == len(training_labels)) #둘이 동일하지 않으면 AssertError를 발생시킵니다
        
        correct_so_far = 0 #trainning 중 맞춘 class를 저장할 변수를 만듦니다.
        
        start = time.time() #시작한 시간을 체크합니다.
        
        for i in range(len(training_reviews)): #모든 review에 대해 forward와 backword pass를 수행합니다. 매번 weights를 수행합니다.
            
            #review와 label을 불러옵니다.
            review = training_reviews[i]
            label = training_labels[i]
            
            #forward pass
            
            self.update_input_layer(review) #input layer
            layer_1 = self.layer_0.dot(self.weights_0_1)
            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
            
            #backward pass
            
            #output error
            layer_2_error = layer_2 - self.get_target_for_label(label)
            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)
            
            #backpropagated error
            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)
            layer_1_delta = layer_1_error #hidden layer의 gradient입니다.
            
            #weights를 업데이트합니다.
            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # hidden에서 output에 적용되는 weights를 조정합니다.
            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate #input에서 hidden에 적용되는 weights를 조정합니다.
            
            if(layer_2 >= 0.5 and label == 'POSITIVE'):
                correct_so_far += 1
            elif(layer_2 < 0.5 and label == 'NEGATIVE'):
                correct_so_far += 1
                
            #디버깅을 위해 예측 정확도와 걸린 시간을 확인합니다.
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(training_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct_so_far) + " #Trained:" + str(i+1) \
                             + " Training Accuracy:" + str(correct_so_far * 100 / float(i+1))[:4] + "%")
            if(i % 2500 == 0):
                print("")
                
    def test(self, testing_reviews, testing_labels):
        correct = 0
        start = time.time()
        
        for i in range(len(testing_reviews)):
            pred = self.run(testing_reviews[i]) #label을 예측합니다.
            if(pred == testing_labels[i]):
                correct += 1
                
            #역시 디버ㄷ싱을 위해 걸린 시간 등을 표현합니다. 
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(testing_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct) + " #Tested:" + str(i+1) \
                             + " Testing Accuracy:" + str(correct * 100 / float(i+1))[:4] + "%")
    
    def run(self, review):
        
        self.update_input_layer(review.lower()) #input layer
        layer_1 = self.layer_0.dot(self.weights_0_1)
        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
        
        if(layer_2[0] >= 0.5):
            return "POSITIVE"
        else:
            return "NEGATIVE"
```


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)
mlp.train(reviews[:-1000],labels[:-1000]) 
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):785.9 #Correct:1825 #Trained:2501 Training Accuracy:72.9%
    Progress:20.8% Speed(reviews/sec):805.7 #Correct:3817 #Trained:5001 Training Accuracy:76.3%
    Progress:31.2% Speed(reviews/sec):805.6 #Correct:5905 #Trained:7501 Training Accuracy:78.7%
    Progress:41.6% Speed(reviews/sec):808.7 #Correct:8040 #Trained:10001 Training Accuracy:80.3%
    Progress:52.0% Speed(reviews/sec):806.3 #Correct:10189 #Trained:12501 Training Accuracy:81.5%
    Progress:62.5% Speed(reviews/sec):809.1 #Correct:12310 #Trained:15001 Training Accuracy:82.0%
    Progress:67.7% Speed(reviews/sec):810.7 #Correct:13366 #Trained:16266 Training Accuracy:82.1%Progress:72.9% Speed(reviews/sec):797.6 #Correct:14416 #Trained:17501 Training Accuracy:82.3%
    Progress:83.3% Speed(reviews/sec):802.2 #Correct:16597 #Trained:20001 Training Accuracy:82.9%
    Progress:93.7% Speed(reviews/sec):803.0 #Correct:18777 #Trained:22501 Training Accuracy:83.4%
    Progress:99.9% Speed(reviews/sec):804.8 #Correct:20095 #Trained:24000 Training Accuracy:83.7%


```python
mlp.test(reviews[-1000:],labels[-1000:])
```

    Progress:67.9% Speed(reviews/sec):601.7 #Correct:584 #Tested:680 Testing Accuracy:85.8%Progress:99.9% Speed(reviews/sec):499.6 #Correct:855 #Tested:1000 Testing Accuracy:85.5%

앞선 코드보다 성능이 개선된 것을 확인할 수 있습니다. 

조금더 weights에서 어떤 일이 발생했는지 살펴봅시다. 

어떤 단어가 비슷한 weights를 가지고 반영이 되었을까요?


```python
#한번 mlp_full을 훈련시켜 보겠습니다. 
mlp_full = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=0,polarity_cutoff=0,learning_rate=0.01)
```


```python
mlp_full.train(reviews[:-1000],labels[:-1000])
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:9.25% Speed(reviews/sec):739.7 #Correct:1721 #Trained:2221 Training Accuracy:77.4%Progress:10.4% Speed(reviews/sec):672.1 #Correct:1941 #Trained:2501 Training Accuracy:77.6%
    Progress:20.8% Speed(reviews/sec):718.4 #Correct:3977 #Trained:5001 Training Accuracy:79.5%
    Progress:31.2% Speed(reviews/sec):710.3 #Correct:6081 #Trained:7501 Training Accuracy:81.0%
    Progress:41.6% Speed(reviews/sec):727.9 #Correct:8239 #Trained:10001 Training Accuracy:82.3%
    Progress:52.0% Speed(reviews/sec):736.1 #Correct:10395 #Trained:12501 Training Accuracy:83.1%
    Progress:62.5% Speed(reviews/sec):743.3 #Correct:12529 #Trained:15001 Training Accuracy:83.5%
    Progress:72.9% Speed(reviews/sec):750.8 #Correct:14622 #Trained:17501 Training Accuracy:83.5%
    Progress:83.3% Speed(reviews/sec):753.3 #Correct:16783 #Trained:20001 Training Accuracy:83.9%
    Progress:93.7% Speed(reviews/sec):755.6 #Correct:18963 #Trained:22501 Training Accuracy:84.2%
    Progress:99.9% Speed(reviews/sec):756.4 #Correct:20275 #Trained:24000 Training Accuracy:84.4%


```python
#비슷한 단어를 찾을 수 있는 함수를 정의해 사용해 보겠습니다.
def get_most_similar_words(focus = "horrible"):
    most_similar = Counter()

    for word in mlp_full.word2index.keys():
        most_similar[word] = np.dot(mlp_full.weights_0_1[mlp_full.word2index[word]],mlp_full.weights_0_1[mlp_full.word2index[focus]])
    
    return most_similar.most_common()
```


```python
get_most_similar_words("excellent")[1:10] #excellent와 유사한 단어를 체크합니다. 
```




    [('perfect', 0.12772202922182546),
     ('amazing', 0.0962964613982854),
     ('today', 0.09235499385945072),
     ('wonderful', 0.09185286324691552),
     ('great', 0.09183161135873238),
     ('fun', 0.09117340788569482),
     ('best', 0.08981683905415982),
     ('liked', 0.07978138943110683),
     ('definitely', 0.07975284361926761)]




```python
get_most_similar_words("terrible")[1:10] #terrible과 유사한 단어를 체크합니다. 
```




    [('awful', 0.12676760565619083),
     ('waste', 0.12329605907911924),
     ('terrible', 0.09646009731527341),
     ('poor', 0.09525036890389288),
     ('dull', 0.08724323954461477),
     ('disappointment', 0.08268554727282224),
     ('poorly', 0.08207625648169954),
     ('fails', 0.08097672627184332),
     ('unfortunately', 0.08087101810993402)]



하지만 이렇게 한 단어, 한 단어를 살펴볼 수는 없습니다.

한번 시각화를 시켜보겠습니다.


```python
import matplotlib.colors as colors

words_to_visualize = list()
for word, ratio in pos_neg_ratios.most_common(500):
    if(word in mlp_full.word2index.keys()):
        words_to_visualize.append(word)
    
for word, ratio in list(reversed(pos_neg_ratios.most_common()))[0:500]:
    if(word in mlp_full.word2index.keys()):
        words_to_visualize.append(word)
```


```python
pos = 0
neg = 0

colors_list = list()
vectors_list = list()
for word in words_to_visualize:
    if word in pos_neg_ratios.keys():
        vectors_list.append(mlp_full.weights_0_1[mlp_full.word2index[word]])
        if(pos_neg_ratios[word] > 0):
            pos+=1
            colors_list.append("#00ff00")
        else:
            neg+=1
            colors_list.append("#000000")
```


```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=0)
words_top_ted_tsne = tsne.fit_transform(vectors_list)
```


```python
p = figure(tools="pan,wheel_zoom,reset,save",
           toolbar_location="above",
           title="vector T-SNE for most polarized words")

source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],
                                    x2=words_top_ted_tsne[:,1],
                                    names=words_to_visualize,
                                    color=colors_list))

p.scatter(x="x1", y="x2", size=8, source=source, fill_color="color")

word_labels = LabelSet(x="x1", y="x2", text="names", y_offset=6,
                  text_font_size="8pt", text_color="#555555",
                  source=source, text_align='center')
p.add_layout(word_labels)

show(p)

# green indicates positive words, black indicates negative words
```

![](https://drive.google.com/uc?id=1o6aIQMule2KFgAuuFPum1pzOLgQHH7xO)

유사한 단어들이 인접해서 나타나는 것을 확인해 볼 수 있습니다.

지금까지 감성분석을 위해 SnetimentalNetwork을 만들어 보는 실습이었습니다.

이를 한글에 적용해 보는 것은 개인 프로젝트로 따로 포스팅하도록 하겠습니다. 