---
title: "[Deep learning]Weight Initialization"
date: 2020-05-12
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

#  Lesson 1 Weight Initialization

신경망에서 중요한 요소 중 하나는 initial weights를 정하는 것입니다. 신경망을 훈련하기 전에 Weights initialization을 하게 됩니다. 이를 잘 설정하면 적합한 솔루션을 만드는 데 도움이 됩니다.


## 1) Initial Weights and Observing Training Loss

다른 weighits가 어떻게 작동하는지를 확인하기 위한 실험을 해 보겠습니다. 같은 dataset과 신경망을 바탕으로 weights를 변경하면서 어떻게 변경되는지 확인해 보겠습니다. 

![cnn51](https://drive.google.com/uc?id=19-myULa9OvbMIyusp_PZaDx15v0fMI1Y)

## 2) Dataset and Model

MLP를 훈련해 [Fashion-MNIST database](https://github.com/zalandoresearch/fashion-mnist)를 분류해 보겠습니다. FashionMNIST는 의류 사진을 바탕으로 `classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']`. 를 분류하게 됩니다. 이미지는 정규화되어 있어 픽셀은 0.0~1.0의 값을 가집니다.

라이브러리와 [데이터](http://pytorch.org/docs/stable/torchvision/datasets.html)를 import 하겠습니다.


```python
import torch
import numpy as np
from torchvision import datasets
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler

# data 로드에 사용할 subprocess의 수를 정합니다.
num_workers = 0
# 한 batch에서 불러올 샘플 수를 정합니다.
batch_size = 100
# validation에 사용할 traningset의 비율을 정합니다.
valid_size = 0.2

# 데이터를 torch.FloatTensor로 변환합니다.
transforms = transforms.ToTensor()

# training과 test dataset을 정합니다.
train_data = datasets.FashionMNIST(root = 'data', train = True,
                                  download = True, transform = transforms)
test_data = datasets.FashionMNIST(root = 'data', train = False,
                                  download = True, transform = transforms)

# validation에 사용할 training 인덱스를 가져옵니다.
num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx, valid_idx = indices[split:], indices[:split]

# training과 validation batch size를 위해 샘플러를 정의합니다.
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

# dataloader를 준비합니다.
train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,
                                          sampler = train_sampler, num_workers = num_workers)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,
                                          sampler = valid_sampler, num_workers = num_workers)
test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size,
                                         num_workers = num_workers)

# image class를 정의합니다.
classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
```


## 3) Visualize Some Training Data


```python
import matplotlib.pyplot as plt
%matplotlib inline

# tarining image의 batch를 하나 가져옵니다.
dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()

# 해당 batch의 image를 라벨과 함께 plot합니다. 
fig = plt.figure(figsize = (25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks = [], yticks = [])
    ax.imshow(np.squeeze(images[idx]), cmap = 'gray')
    ax.set_title(classes[labels[idx]])
```

![](https://drive.google.com/uc?id=16CGHQJQDFUyqE64AsP4d4lVmc3d1kHh3)


## 4) Define the Model Architecture

dataset의 class를 분류하기 위한 MLP를 정의합니다.

**Neural Network**

![](https://drive.google.com/uc?id=1k3Fgkh9kZrLTt2Smd28Jvptwk6ARTEqa)

* hidden dimensions으로 256, 128을 가지는 3개의 MLP 레이어를 만듭니다. 

* MLP는 flattened된 image (784개의 값을 가진 long vector)를 input으로 받아 output으로 10개의 class를 출력합니다.


 ReLU activations과 Adam optimizer를 사용하는 이 신경망에 다른 initial weights값을 적용해 봅니다.


## 5) All Zeros or Ones

[오컴의 면도날](https://en.wikipedia.org/wiki/Occam's_razor)에 따라서 모든 weights를 0이나 1로 두는 것이 가장 좋은 해법일 수도 있습니다. 하지만 이 경우에는 모든 weigths가 동일하기 때문에 모든 뉴런은 동일한 output을 출력합니다. 이는 어떤 weights를 조정해야 하는지 알기 어렵게 만듭니다. 한번 모두 1과 0으로 해서 loss를 비교해 보겠습니다.

이를 위해 PyTorch의  [nn.init](https://pytorch.org/docs/stable/nn.html#torch-nn-init)를 사용해 보겠습니다. init library에는 weights initialization을 layer 타입에 따라 할 수 있는 다양한 함수를 제공합니다. 

만약 linear layer일 경우에는 weights를 bias가 0인 `constant_weight`로 만들겠습니다. 이 과정은 다음과 같습니다.

>```
if isinstance(m, nn.Linear):
    nn.init.constant_(m.weight, constant_weight)
    nn.init.constant_(m.bias, 0)
```

`constant_weight`는 모델을 instantiate할 때 전달되는 값입니다.


```python
import torch.nn as nn
import torch.nn.functional as F

# NN 아키텍처를 정의합니다.
class Net(nn.Module):
    def __init__(self, hidden_1 = 256, hidden_2 = 128, constant_weight = None):
        super(Net, self).__init__()
        # linear layer (784 -> hidden_1)
        self.fc1 = nn.Linear(28 * 28, hidden_1)
        # linear layer (hidden_1 -> hidden_2)
        self.fc2 = nn.Linear(hidden_1, hidden_2)
        # linear layer(hidden_2 -> 10 classes)
        self.fc3 = nn.Linear(hidden_2, 10)
        #dropout layer (p = 0.2)
        self.dropout = nn.Dropout(0.2)
        
        # weights를 상수로 초기화 합니다.
        if(constant_weight is not None):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.constant_(m.weight, constant_weight)
                    nn.init.constant_(m.bias, 0)
                    
                    
    def forward(self, x):
        # image input을 flatten 합니다.
        x = x.view(-1, 28 * 28)
        # hidden layer를 추가합니다.
        x = F.relu(self.fc1(x))
        # dropout layer를 추가합니다.
        x = self.dropout(x)
        # hidden layer를 추가합니다.
        x = F.relu(self.fc2(x))
        # dropout layer를 추가합니다.
        x = self.dropout(x)
        # output layer를 추가합니다.
        x = self.fc3(x)
        return x
```

**Compare Model Behavior**

`helpers.compare_init_weights`를 사용해 `model_0` 과 `model_1`의 training과 validation loss를 비교해 보겠습니다.이 함수는 model, plot의 이름, training과 validation dataset loaders 들을 list로 받습니다. 그리고 각 모델에 대해 첫 100개의 batches에 대해 training loss를 plot하고 2번의 training epochs 이후 validation accuracy를 출력합니다.


```python
# initialize two NN's with 0 and 1 constant weights
model_0 = Net(constant_weight=0)
model_1 = Net(constant_weight=1)
```


```python
import helpers

# put them in list form to compare
model_list = [(model_0, 'All Zeros'),
              (model_1, 'All Ones')]


# plot the loss over the first 100 batches
helpers.compare_init_weights(model_list, 
                             'All Zeros vs All Ones', 
                             train_loader,
                             valid_loader)
```


![png](https://drive.google.com/uc?id=1Nej2DI73-iqF3yWS_5-yB7rvdP9NdFIp)


    After 2 Epochs:
    Validation Accuracy
        9.558% -- All Zeros
        9.575% -- All Ones
    Training Loss
        2.302  -- All Zeros
      859.303  -- All Ones
    

모두 0으로 하나 1으로 하나 정확도는 10% 정도인 것을 알 수 있습니다. 

학습과정에서 뉴런들이 layer마다 같은 output를 출력하기 때문에 아마 어떤 weights를 바꿔야 할지 알기 어려웠을 것입니다. 뉴런들이 같은 output을 출력하는걸 막기 위해서, 특정한 값들을 활용하는 것이 필요합니다. 하지만 local minimum에 빠지지 않기 위해서 weights들을 랜덤하게 선택하는 것이 필요하고, 이를 위해 uniform distribution을 활용할 수 있습니다.


## 6) Uniform Distribution

[uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous%29)은 특정한 숫자를 선택할 확률이 동일합니다. 연속 분포에서 같은 숫자를 뽑을 확률은 매우 낮습니다. NumPy의 `np.random.uniform` 함수를 활용해 uniform distribution에서 임의의 숫자를 선택합니다.

>#### [`np.random_uniform(low=0.0, high=1.0, size=None)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html)
> uniform distribution에서 임의의 수를 출력합니다.

> 생성된 값은  [low, high) 범위에서 균일한 분포를 따릅니다. low는 범주에 포함되고, high는 범주에서 제외됩니다. 

>- **low:** 범주의 최소값으로 Defaults값은 0입니다.
- **high:** 범주의 최대값으로 Defaults값은 1입니다.
- **size:** int나 int의 tuple로 output array의 shape를 결정합니다.

uniform distribution에 대해 더 알아보기 위해 이를 histogram으로 그려보겠습니다. `np.random_uniform(-3, 3, [1000])`을 `helper.hist_dist` 함수를 사용해 시각화하겠습니다.`-3`이상 `3`미만인 임의의  `1000`개 실수를 가지고 분포를 살펴봅니다.


```python
helpers.hist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))
```


![png](https://drive.google.com/uc?id=1bm_fu6GPhuskzhLHjf7XhEtCFn7O_xY5)


이 histogram은 1,000개의 값을 500개의 bucket을 사용해 표현했습니다. 각 bucket에는 2개의 값이 있어야 합니다 uniform function에 대해 대략적으로 이해했으니, PyTorch의 `nn.init`을 사용해 적용해 보겠습니다.



**Uniform Initialization, Baseline**

 `low=0.0`과 `high=1.0`인 uniform weight initialization을 활용해 신경망의 성능이 어떻게 변하는지 확인해보겠습니다. 모델 정의 외부에서 weights를 설정하기 위해서 다음과 같은 방법을 시도해 볼 수 있습니다.

>1. 네트워크 layer의 유형에 따라 weights를 할당하는 함수를 정의합니다. 
>2. 그 weights들을 initialized된 model에 `model.apply(fn)`를 사용해 적용해 봅니다.

여기서는 `weight.data.uniform_` 을 사용해 model의 weights를 초기화해 보겠습니다.


```python
# 모듈을 받아서 weights initialization을 적용합니다. 
def weights_init_uniform(m):
    classname = m.__class__.__name__
    # 모델의 모든 Linear layer에 대해
    if classname.find('Linear') != -1:
        # uniform distribution을 weights로, bias=0으로 적용합니다.
        m.weight.data.uniform_(0.0, 1.0)
        m.bias.data.fill_(0)
```


```python
# weights를 적용한 새로운 모델을 만듭니다.
model_uniform = Net()
model_uniform.apply(weights_init_uniform)
```




    Net(
      (fc1): Linear(in_features=784, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=128, bias=True)
      (fc3): Linear(in_features=128, out_features=10, bias=True)
      (dropout): Dropout(p=0.2)
    )




```python
# 평가해 보겠습니다.
helpers.compare_init_weights([(model_uniform, 'Uniform Weights')], 
                             'Uniform Baseline', 
                             train_loader,
                             valid_loader)
```


![png](https://drive.google.com/uc?id=106hqhbJrLsNsRui54yeVZaV_eKGC-PaI)


    After 2 Epochs:
    Validation Accuracy
       35.183% -- Uniform Weights
    Training Loss
        4.238  -- Uniform Weights
    

---
loss graph를 보면 신경망이 배우기 시작했다는 것을 알 수 있습니다.

## 7) General rule for setting weights

일반적으로 신경망의 weights는 0에 가까운 값이지만, 너무 작지 않게 설정합니다. 

> $y=1/\sqrt{n}$  일 때, $[-y, y]$ 의 범주 내에서 선택하는 것이 일반적입니다. 
($n$ 은 뉴런이 받는 input 개수입니다.).

baseline을 만들어 테스트 해 보겠습니다. unifom distribution을 0.5으로 이동해 [-0.5, 0.5)내에서 수행해 보겠습니다.


```python
# 모듈을 받아서 weights initialization을 적용합니다. 
def weights_init_uniform_center(m):
    classname = m.__class__.__name__
    # 모델의 모든 Linear layer에 대해
    if classname.find('Linear') != -1:
        # uniform distribution을 weights로, bias=0으로 적용합니다.
        m.weight.data.uniform_(-0.5, 0.5)
        m.bias.data.fill_(0)

# weights를 적용한 새로운 모델을 만듭니다.
model_centered = Net()
model_centered.apply(weights_init_uniform_center)
```




    Net(
      (fc1): Linear(in_features=784, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=128, bias=True)
      (fc3): Linear(in_features=128, out_features=10, bias=True)
      (dropout): Dropout(p=0.2)
    )



그리고 general rule에 따라 분포와 모델을 만들어 보겠습니다. $ [-y, y] $ 범위를 사용하고, 여기서 $ y = 1 / \ sqrt {n} $입니다.


```python
# 모듈을 받아서 weights initialization을 적용합니다. 
def weights_init_uniform_rule(m):
    classname = m.__class__.__name__
    # 모델의 모든 Linear layer에 대해
    if classname.find('Linear') != -1:
        # input의 수를 가져옵니다.
        n = m.in_features
        y = 1.0/np.sqrt(n)
        m.weight.data.uniform_(-y, y)
        m.bias.data.fill_(0)

# weights를 적용한 새로운 모델을 만듭니다.
model_rule = Net()
model_rule.apply(weights_init_uniform_rule)
```




    Net(
      (fc1): Linear(in_features=784, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=128, bias=True)
      (fc3): Linear(in_features=128, out_features=10, bias=True)
      (dropout): Dropout(p=0.2)
    )




```python
# 두 모델을 비교하고 
model_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), 
              (model_rule, 'General Rule [-y, y)')]

# 평가해 보겠습니다.
helpers.compare_init_weights(model_list, 
                             '[-0.5, 0.5) vs [-y, y)', 
                             train_loader,
                             valid_loader)
```


![png](https://drive.google.com/uc?id=1VJEJIH_568KNgDS-u7MZsEEDMOMi0-aT) 


    After 2 Epochs:
    Validation Accuracy
       76.458% -- Centered Weights [-0.5, 0.5)
       85.408% -- General Rule [-y, y)
    Training Loss
        0.454  -- Centered Weights [-0.5, 0.5)
        0.359  -- General Rule [-y, y)
    

훨씬 loss가 감소한 것을 확인할 수 있습니다. epoch를 2번 수행했는데도 높은 정확도를 보입니다.

uniform distribution이 범위 내의 수를 뽑을 확률이 모두 같은데, 만약에 0에 가까운 값을 뽑을 확률이 더 높으면 어떨까요? normal distribution을 활용해 실험해 보겠습니다.


## 8) Normal Distribution

uniform distribution과 다르게 [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)은 평균과 가까운 수를 뽑을 확률이 더 높습니다. 이를 시각화 하기 위해 NumPy의 `np.random.normal` 함수와 히스토그램을 사용해 보겠습니다. 

>[np.random.normal(loc=0.0, scale=1.0, size=None)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html)


```python
helpers.hist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))
```


![png](https://drive.google.com/uc?id=1oYMR5tHdyf_XZUyfLTaQW7Nc0GcA1qxP)


rule를 사용하고, uniform distribution을 적용한 결과와 비교해 보겠습니다.

이제 한번 weights를 정의해 보겠습니다. normal distribution은 평균이 0이고 표준 편차가  $y=1/\sqrt{n}$인 분포입니다.


```python
# 모듈을 받아서 weights initialization을 적용합니다. 
def weights_init_normal(m):
    classname = m.__class__.__name__
    #  모델의 모든 Linear layer에 대해
    if classname.find('Linear') != -1:
        # input의 수를 가져옵니다.
        n = m.in_features
        y = (1.0/np.sqrt(n))
        m.weight.data.normal_(0, y)
        m.bias.data.fill_(0)
```


```python
# rule와 uniform weights를 사용한 모델을 만듭니다.
model_uniform_rule = Net()
model_uniform_rule.apply(weights_init_uniform_rule)

# rule와 정규분포를 사용한 모델을 만듭니다.
model_normal_rule = Net()
model_normal_rule.apply(weights_init_normal)
```




    Net(
      (fc1): Linear(in_features=784, out_features=256, bias=True)
      (fc2): Linear(in_features=256, out_features=128, bias=True)
      (fc3): Linear(in_features=128, out_features=10, bias=True)
      (dropout): Dropout(p=0.2)
    )




```python
# 두 모델을 비교합니다.
model_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), 
              (model_normal_rule, 'Normal Distribution')]

# 결과를 평가합니다.
helpers.compare_init_weights(model_list, 
                             'Uniform vs Normal', 
                             train_loader,
                             valid_loader)
```


![png](https://drive.google.com/uc?id=1rD6wIUnm4iBq4KC7wmWrnHU1CUz6dX93)


    After 2 Epochs:
    Validation Accuracy
       85.217% -- Uniform Rule [-y, y)
       84.433% -- Normal Distribution
    Training Loss
        0.357  -- Uniform Rule [-y, y)
        0.354  -- Normal Distribution
    

이 경우에 정규분포와 uniform 분포는 매우 유사해 보입니다. 이는 신경망이 작기 때문일 수도 있습니다. 일반적으로 정규분포는 모델의 성능을 향상시킨다고 알려져 있습니다.

---

## 9) Automatic Initialization

한번 weight initilalizaiton을 실행하지 않고 자동화시켜서 시험해 보겠습니다.


```python
model_no_initialization = Net()
```


```python
model_list = [(model_no_initialization, 'No Weights')]

# 결과를 평가합니다.
helpers.compare_init_weights(model_list, 
                             'No Weight Initialization', 
                             train_loader,
                             valid_loader)
```


![png](https://drive.google.com/uc?id=13Rl6N4HRCZ55jiNRg0ynCjR7BNlu-fqq)


    After 2 Epochs:
    Validation Accuracy
       84.917% -- No Weights
    Training Loss
        0.544  -- No Weights
    

### Default initialization

재밌게도 weights에 대해 특별한 작업을 하지 않아도 유사한 결과를 가지고 있습니다. PyTorch는 모든 유형의 layer에 대해 기본 weights initialization을 가지고 있습니다. [코드](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html)를 통해 어떤 작업이 일어나는 지를 확인해 볼 수 있습니다. 

경우에 따라서 unifom distribution이 나을 수도, 정규분포가 나을 수도 있고, [common initialization distributions](https://pytorch.org/docs/stable/nn.html#torch-nn-init)을 사용해 볼 수도 있습니다.
