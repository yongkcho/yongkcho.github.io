---
title: "[Deep learning]Nural Networks (13) Sentiment Classification(2)"
date: 2020-04-28
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# 감성 분류 및 신경망을 통한 문제 해결

Andrew Trask는 신경망을 통해 영화 리뷰를 Positive와 Negative로 분류합니다. 

이 내용을 번역하고, 제가 배운 내용을 함께 정리해 두었습니다.

## 4)  Understanding Neural Noise

[지난번 코드](https://yongkcho.github.io/python/udacity/study/deeplearning/ai/nural/network/deep-24/)를 수정해 보겠습니다. 다른 여러가지 기법을 사용하기 전에, 먼저 데이터를 다시 살펴봅시다.

지난 코드에서 모든 단어 node는 빈도수에 따른 weights를 받았습니다. 하지만 이런 방식이 과연 효과적일까요?

조금 살펴보면, 아무것도 없는 ' '이 큰 빈도수를 가지고 있는 것을 볼 수 있습니다. 

그리고 이를 반영한 weights 때문에 코드는 weighted sum을 계산하게 되는 것입니다.

그렇다면 update_input_layer을 바꾸면 어떻게 될까요?


```python
g = open('reviews.txt','r') # What we know!
reviews = list(map(lambda x:x[:-1],g.readlines()))
g.close()

g = open('labels.txt','r') # What we WANT to know!
labels = list(map(lambda x:x[:-1].upper(),g.readlines()))
g.close()
```


```python
import time
import sys
import numpy as np
```


```python
#  신경망을 encapsulate해 class로 만듦니다.

class SentimentNetwork:
    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1,  hidden_nodes = 10, learning_rate = 0.1):
        # min_count는 최소 등장횟수입니다. 이 이상 단어가 등장해야 추가됩니다. 
        # polarity_cutoff는 positive와 negative의 비율을 바탕으로 이 이상이 되어야만 학습에 사용합니다.
        
        np.random.seed(1) #seed를 설정해 디버깅을 쉽게 합니다.
        
        self.pre_process_data(reviews,labels) #리뷰와 라벨을 trainning전에 정리합니다.
        
        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate) #input node의 개수만큼 인자를 받아 신경망을 초기화 시킵니다.
        
        
    def pre_process_data(self, reviews, labels):
        
        review_vocab = set()
        for review in reviews: #주어진 review에 있는 모든 단어를 바탕으로 review_vocab을 채웁니다.
            for word in review.split(" "):
                review_vocab.add(word)
        
        self.review_vocab = list(review_vocab) #vocabulary 집합을 list로 만들어 index를 바탕으로 접근할 수 있게 만듦니다.
        
        label_vocab = set() #주어진 label역시 review와 같은 작업을 수행합니다.
        for label in labels:
            label_vocab.add(label)
            
        self.label_vocab = list(label_vocab)
        
        self.review_vocab_size = len(self.review_vocab) #review와 label의 size를 저장해 둡니다.
        self.label_vocab_size = len(self.label_vocab)
        
        self.word2index = {} #dictionary를 만들어 label을 index 위치에 mapping합니다.
        for i,word in enumerate(self.review_vocab):
            self.word2index[word] = i
        
        self.label2index = {}
        for i,label in enumerate(self.label_vocab):
            self.label2index[label] = i
    
    
    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        #input, hidden, output layer, learning_rate의 숫자를 정합니다.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        
        #weights를 초기화합니다.
        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes)) #input과 hidden layer 사이의 weights
        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                            (self.hidden_nodes, self.output_nodes))
        
        self.layer_0 = np.zeros((1, input_nodes)) #input layer를 만들어줍니다.
        
    def update_input_layer(self, review):
        
        self.layer_0 *= 0 #input layer를 모두 0으로 초기화합니다.
        
        for word in review.split(" "):
            if(word in self.word2index.keys()):
                #self.layer_0[0][self.word2index[word]] += 1 이전 코드에서는 이렇게 했습니다.
                self.layer_0[0][self.word2index[word]] = 1 #코드를 이렇게 변경하였습니다. 
        
        
    def get_target_for_label(self, label): #positive를 1로, negative를 0으로 변환합니다.
        if(label == 'POSITIVE'):
            return 1
        else:
            return 0
        
    def sigmoid(self, x): #sigmoid 함수를 정의합니다.
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_output_2_derivative(self, output):
        return output * (1 - output)
    
    def train(self, training_reviews, training_labels):
        assert(len(training_reviews) == len(training_labels)) #둘이 동일하지 않으면 AssertError를 발생시킵니다
        
        correct_so_far = 0 #trainning 중 맞춘 class를 저장할 변수를 만듦니다.
        
        start = time.time() #시작한 시간을 체크합니다.
        
        for i in range(len(training_reviews)): #모든 review에 대해 forward와 backword pass를 수행합니다. 매번 weights를 수행합니다.
            
            #review와 label을 불러옵니다.
            review = training_reviews[i]
            label = training_labels[i]
            
            #forward pass
            
            self.update_input_layer(review) #input layer
            layer_1 = self.layer_0.dot(self.weights_0_1)
            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
            
            #backward pass
            
            #output error
            layer_2_error = layer_2 - self.get_target_for_label(label)
            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)
            
            #backpropagated error
            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)
            layer_1_delta = layer_1_error #hidden layer의 gradient입니다.
            
            #weights를 업데이트합니다.
            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate # hidden에서 output에 적용되는 weights를 조정합니다.
            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate #input에서 hidden에 적용되는 weights를 조정합니다.
            
            if(layer_2 >= 0.5 and label == 'POSITIVE'):
                correct_so_far += 1
            elif(layer_2 < 0.5 and label == 'NEGATIVE'):
                correct_so_far += 1
                
            #디버깅을 위해 예측 정확도와 걸린 시간을 확인합니다.
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(training_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct_so_far) + " #Trained:" + str(i+1) \
                             + " Training Accuracy:" + str(correct_so_far * 100 / float(i+1))[:4] + "%")
            if(i % 2500 == 0):
                print("")
                
    def test(self, testing_reviews, testing_labels):
        correct = 0
        start = time.time()
        
        for i in range(len(testing_reviews)):
            pred = self.run(testing_reviews[i]) #label을 예측합니다.
            if(pred == testing_labels[i]):
                correct += 1
                
            #역시 디버ㄷ싱을 위해 걸린 시간 등을 표현합니다. 
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(testing_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct) + " #Tested:" + str(i+1) \
                             + " Testing Accuracy:" + str(correct * 100 / float(i+1))[:4] + "%")
    
    def run(self, review):
        
        self.update_input_layer(review.lower()) #input layer
        layer_1 = self.layer_0.dot(self.weights_0_1)
        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))
        
        if(layer_2[0] >= 0.5):
            return "POSITIVE"
        else:
            return "NEGATIVE"
```


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)
mlp.train(reviews[:-1000],labels[:-1000])  #BUT I WANT DO IT MORE FASTER
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):65.65 #Correct:1830 #Trained:2501 Training Accuracy:73.1%
    Progress:20.8% Speed(reviews/sec):61.78 #Correct:3790 #Trained:5001 Training Accuracy:75.7%
    Progress:31.2% Speed(reviews/sec):63.54 #Correct:5884 #Trained:7501 Training Accuracy:78.4%
    Progress:41.6% Speed(reviews/sec):66.38 #Correct:8021 #Trained:10001 Training Accuracy:80.2%
    Progress:52.0% Speed(reviews/sec):67.43 #Correct:10155 #Trained:12501 Training Accuracy:81.2%
    Progress:62.5% Speed(reviews/sec):66.38 #Correct:12287 #Trained:15001 Training Accuracy:81.9%
    Progress:72.9% Speed(reviews/sec):66.35 #Correct:14404 #Trained:17501 Training Accuracy:82.3%
    Progress:83.3% Speed(reviews/sec):66.65 #Correct:16579 #Trained:20001 Training Accuracy:82.8%
    Progress:93.7% Speed(reviews/sec):66.85 #Correct:18771 #Trained:22501 Training Accuracy:83.4%
    Progress:99.9% Speed(reviews/sec):66.86 #Correct:20091 #Trained:24000 Training Accuracy:83.7%

지난번에 비해서 속도가 상당히 빨라진 것을 확인할 수 있습니다. 

하지만 아직 성능을 더 높일 수 있을 여지가 있어 보입니다. 


## 5) Analyzing Inefficiencies in our Network

현재는 의미가 없는 데이터까지 모두 계산에 사용합니다. 실제로는 '0'이어서 아무런 영향을 미치지 못해도 행렬 곱과 합에 사용되는 것입니다.

이러한 것들을 한번 수정해 보겠습니다.

* 이번에는 update_input_layer를 삭제하겠습니다.

* 또한, init_network에서 input layer였던 self.layer_0 역시 삭제하겠습니다.

* train과 run을 수정하겠습니다. 


```python
#  신경망을 encapsulate해 class로 만듦니다.

class SentimentNetwork:
    def __init__(self, reviews, labels, min_count = 10, polarity_cutoff = 0.1,  hidden_nodes = 10, learning_rate = 0.1):
        # min_count는 최소 등장횟수입니다. 이 이상 단어가 등장해야 추가됩니다. 
        # polarity_cutoff는 positive와 negative의 비율을 바탕으로 이 이상이 되어야만 학습에 사용합니다.
        
        np.random.seed(1) #seed를 설정해 디버깅을 쉽게 합니다.
        
        self.pre_process_data(reviews,labels) #리뷰와 라벨을 trainning전에 정리합니다.
        
        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate) #input node의 개수만큼 인자를 받아 신경망을 초기화 시킵니다.
        
        
    def pre_process_data(self, reviews, labels):
        
        review_vocab = set()
        for review in reviews: #주어진 review에 있는 모든 단어를 바탕으로 review_vocab을 채웁니다.
            for word in review.split(" "):
                review_vocab.add(word)
        
        self.review_vocab = list(review_vocab) #vocabulary 집합을 list로 만들어 index를 바탕으로 접근할 수 있게 만듦니다.
        
        label_vocab = set() #주어진 label역시 review와 같은 작업을 수행합니다.
        for label in labels:
            label_vocab.add(label)
            
        self.label_vocab = list(label_vocab)
        
        self.review_vocab_size = len(self.review_vocab) #review와 label의 size를 저장해 둡니다.
        self.label_vocab_size = len(self.label_vocab)
        
        self.word2index = {} #dictionary를 만들어 label을 index 위치에 mapping합니다.
        for i,word in enumerate(self.review_vocab):
            self.word2index[word] = i
        
        self.label2index = {}
        for i,label in enumerate(self.label_vocab):
            self.label2index[label] = i
    
    
    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        #input, hidden, output layer, learning_rate의 숫자를 정합니다.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        
        #weights를 초기화합니다.
        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes)) #input과 hidden layer 사이의 weights
        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                            (self.hidden_nodes, self.output_nodes))
        
        #self.layer_0 = np.zeros((1, input_nodes)) 이전 코드는 이랬습니다.
        self.layer_1 = np.zeros((1, hidden_nodes))
        
    #update_input_layer를 통채로 삭제합니다.        
        
    def get_target_for_label(self, label): #positive를 1로, negative를 0으로 변환합니다.
        if(label == 'POSITIVE'):
            return 1
        else:
            return 0
        
    def sigmoid(self, x): #sigmoid 함수를 정의합니다.
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_output_2_derivative(self, output):
        return output * (1 - output)
    
    #train 함수를 변경하겠습니다. 
    def train(self, training_reviews_raw, training_labels):
        training_reviews = list()
        #0이 아닌 input만 받기 위해 training reviews를 직접 가져옵니다. 
        for review in training_reviews_raw:
            indices = set()
            for word in review.split(" "):
                if(word in self.word2index.keys()):
                    indices.add(self.word2index[word])
            training_reviews.append(list(indices))
        
        assert(len(training_reviews) == len(training_labels)) #둘이 동일하지 않으면 AssertError를 발생시킵니다
        
        correct_so_far = 0 #trainning 중 맞춘 class를 저장할 변수를 만듦니다.
        
        start = time.time() #시작한 시간을 체크합니다.
        
        for i in range(len(training_reviews)): #모든 review에 대해 forward와 backword pass를 수행합니다. 매번 weights를 수행합니다.
            
            #review와 label을 불러옵니다.
            review = training_reviews[i]
            label = training_labels[i]
            
            #forward pass
            
            #이제 update_input_layer는 사용하지 않습니다. 
            self.layer_1 *= 0 #hidden layer
            for index in review:
                self.layer_1 += self.weights_0_1[index]
            
            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2)) #output layer    
            
            #backward pass
            
            #output error
            layer_2_error = layer_2 - self.get_target_for_label(label)
            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)
            
            #backpropagated error
            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)
            layer_1_delta = layer_1_error #hidden layer의 gradient입니다.
            
            #weights를 업데이트합니다.
            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # hidden에서 output에 적용되는 weights를 조정합니다.
            
            # forward pass에서 사용한것에서만 weights를 업데이트합니다.
            for index in review:
                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate #input에서 hidden으로 가는 weights를 업데이트합니다. 
            
            if(layer_2 >= 0.5 and label == 'POSITIVE'):
                correct_so_far += 1
            elif(layer_2 < 0.5 and label == 'NEGATIVE'):
                correct_so_far += 1
                
            #디버깅을 위해 예측 정확도와 걸린 시간을 확인합니다.
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(training_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct_so_far) + " #Trained:" + str(i+1) \
                             + " Training Accuracy:" + str(correct_so_far * 100 / float(i+1))[:4] + "%")
            if(i % 2500 == 0):
                print("")
                
    def test(self, testing_reviews, testing_labels):
        correct = 0
        start = time.time()
        
        for i in range(len(testing_reviews)):
            pred = self.run(testing_reviews[i]) #label을 예측합니다.
            if(pred == testing_labels[i]):
                correct += 1
                
            #역시 디버ㄷ싱을 위해 걸린 시간 등을 표현합니다. 
            elapsed_time = float(time.time() - start)
            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0
            sys.stdout.write("\rProgress:" + str(100 * i/float(len(testing_reviews)))[:4] \
                             + "% Speed(reviews/sec):" + str(reviews_per_second)[0:5] \
                             + " #Correct:" + str(correct) + " #Tested:" + str(i+1) \
                             + " Testing Accuracy:" + str(correct * 100 / float(i+1))[:4] + "%")
    
    def run(self, review):
        
        #실제로 사용된 index를 찾아 이것들만을 layer_1에 더합니다. 
        self.layer_1 *= 0
        unique_indices = set()
        for word in review.lower().split(" "):
            if word in self.word2index.keys():
                unique_indices.add(self.word2index[word])
        for index in unique_indices:
            self.layer_1 += self.weights_0_1[index]
        
        # Output layer
        ## New for Project 5: changed to use self.layer_1 instead of local layer_1
        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))
        if(layer_2[0] >= 0.5):
            return "POSITIVE"
        else:
            return "NEGATIVE"
```


```python
mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)
mlp.train(reviews[:-1000],labels[:-1000]) #빨리질수록 더 많이 반복돌릴 수 있음 
```

    Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%
    Progress:10.4% Speed(reviews/sec):1067. #Correct:1798 #Trained:2501 Training Accuracy:71.8%
    Progress:20.8% Speed(reviews/sec):1008. #Correct:3787 #Trained:5001 Training Accuracy:75.7%
    Progress:31.2% Speed(reviews/sec):955.7 #Correct:5887 #Trained:7501 Training Accuracy:78.4%
    Progress:41.6% Speed(reviews/sec):964.1 #Correct:8030 #Trained:10001 Training Accuracy:80.2%
    Progress:45.2% Speed(reviews/sec):947.6 #Correct:8749 #Trained:10850 Training Accuracy:80.6%Progress:52.0% Speed(reviews/sec):940.3 #Correct:10169 #Trained:12501 Training Accuracy:81.3%
    Progress:62.5% Speed(reviews/sec):966.4 #Correct:12306 #Trained:15001 Training Accuracy:82.0%
    Progress:72.9% Speed(reviews/sec):980.9 #Correct:14432 #Trained:17501 Training Accuracy:82.4%
    Progress:83.3% Speed(reviews/sec):993.5 #Correct:16611 #Trained:20001 Training Accuracy:83.0%
    Progress:93.7% Speed(reviews/sec):1003. #Correct:18792 #Trained:22501 Training Accuracy:83.5%
    Progress:99.9% Speed(reviews/sec):1009. #Correct:20109 #Trained:24000 Training Accuracy:83.7%


```python
mlp.test(reviews[-1000:],labels[-1000:])
```

    Progress:99.9% Speed(reviews/sec):1473. #Correct:847 #Tested:1000 Testing Accuracy:84.7%


속도도 훨씬 상승하고, 성능도 좋아진 것을 확인할 수 있습니다.

다음에는 성능을 좀 더 높일 수 있는 방법에 대해 살펴보도록 하겠습니다.

bokeh를 통해 시각화하고, 이를 반영해 pre processing을 수행하고자 합니다.