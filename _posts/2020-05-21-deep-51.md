---
title: "[Deep learning]RNN (7) Hyperparameters(2)"
date: 2020-05-22
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity  study deeplearning ai nural network
---

#  Lesson 6 Hyperparameters


[지난 번](https://yongkcho.github.io/python/udacity/study/deeplearning/ai/nural/network/deep-50/)에는 학습에 필요한 hyperparameter들에 대해 알아보았습니다. 이번에는 모델 자체에서 설정하는 것들에 대해 살펴보겠습니다.


## 1) Number of Hidden Units / Layers

machine learning에서 가장 고민되는 주제 중 하나는 '몇 개의 hidden layer를 넣을 것인가?'인 것 같습니다. 

hidden layer가 많을 수록 학습이 잘 되지만, 학습 데이터를 모두 외어 버리는 overfitting이 발생할 수도 있습니다.

![rnn42](https://drive.google.com/uc?id=1D28zhIgLAfIAsO2TaXWr5ipZwSRUi5Sz)

모델이 훈련되지 않으면 hidden layer를 더 추가해 보고 validation error를 확인하며 overfitting을 방지하는 것이 필요합니다. 

만약 overfitting이 발생하면 Dropout이나 L2 Regularization등의 방식을 활용할 수도 있습니다. 

![rnn43](https://drive.google.com/uc?id=1b53f3RYpvhOeKWE0PF3-LhF9U_hb6vP2)

일반적으로 3개의 hidden layer가 적절한 경우가 많다고 합니다. 다만 CNN에서는 깊을 수록 유리할 수 있습니다. 


## 2) RNN Hyperparameters

보통 주어진 문제를 해결하기 위해 LSTM과 GRU를 모두 실험하게 됩니다. 아래의 글들을 참고해 보세요.

"These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task."

[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio

"The GRU outperformed the LSTM on all tasks with the exception of language modelling"

[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskever

"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN."

[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei

"Which of these variants is best? Do the differences matter? [Greff, et al. (2015)](https://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that they’re all about the same. [Jozefowicz, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks."

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah

"In our *Neural Machine Translation* experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells. Somewhat to our surprise, we found that the vanilla decoder is unable to learn nearly as well as the gated variant."

[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906v2) by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le


## 3) Sources & References

Hyperparameter에 대해 더 알아보고 싶은 분들은 다음을 참고해 주세요.

* [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533) by Yoshua Bengio

* [Deep Learning book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html) by Ian Goodfellow, Yoshua Bengio, Aaron Courville

* [Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters) by Michael Nielsen

* [Efficient BackProp (pdf)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun

More specialized sources:

* [How to Generate a Good Word Embedding?](https://arxiv.org/abs/1507.05523) by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao

* [Systematic evaluation of CNN advances on the ImageNet](https://arxiv.org/abs/1606.02228) by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas

* [Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078)by Andrej Karpathy, Justin Johnson, Li Fei-Fei