---
title: "[Deep learning]CNN (7) Convolutional Neural Networks"
date: 2020-05-08
toc: true
toc_sticky: true
header:
  teaser: 
use_math: true
categories: python udacity study deeplearning ai nural network
---

*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  

# Lesson 7 Convolutional Neural Networks

이번에는 CIFAR-10 데이터를 활용해 이미지를 분류하는  **CNN** 모델을 만들고자 합니다.  .

이 데이터는 10개의 class를 가지고 있는 작은 색상 이미지입니다. 몇 예시는 아래에 있습니다.


![](https://drive.google.com/uc?id=1ht1SufxepUrdKL_q0FSUWXGpTuURUSRW)

## 1)Test for [CUDA](http://pytorch.org/docs/stable/cuda.html)

이미지의 크기가 크기 때문에(32x32x3), GPU를 사용해 훈련시켜 보겠습니다. CUDA는 병렬처리를 가능하게 해 주며, tensor를 활용할 수 있습니다. 먼저 GPU를 사용할 수 있는지 확인해 보겠습니다.


```python
import torch
import numpy as np

# check if CUDA is available
train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')
```

    CUDA is available!  Training on GPU ...
    

---
## 2)Load the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)

데이터를 다운로드하고 training data를 training과 validation set으로 나눠보겠습니다. 그리고 각 데이터마다 DataLoader를 만들어 보겠습니다.


```python
from torchvision import datasets
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler

# 데이터 로드에 사용할 subprocesses의 개수를 정의합니다.
num_workers = 0
# 각 batch에 로드할 samples의 개수를 정의합니다.
batch_size = 20
# validation으로 활용할 비율을 정합니다.
valid_size = 0.2

# 데이터를 정규화된 torch로 변환합니다.
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

# training과 test dataset을 만듭니다.
train_data = datasets.CIFAR10('data', train=True,
                              download=True, transform=transform)
test_data = datasets.CIFAR10('data', train=False,
                             download=True, transform=transform)

# training 인덱스 중 validation에 사용할 것을 선택합니다. 
num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx, valid_idx = indices[split:], indices[:split]

# training과 validation batches를 얻기 위한 sampler를 정의합니다.
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

# data loaders를 정의합니다.
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,
    sampler=train_sampler, num_workers=num_workers)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, 
    sampler=valid_sampler, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
    num_workers=num_workers)

# image classes를 정의합니다.
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck']
```

    Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz
    Files already downloaded and verified
    

## 3)Visualize a Batch of Training Data


```python
import matplotlib.pyplot as plt
%matplotlib inline

# normalize를 해제하고image를 보여주기 위한 함수입니다.
def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image
```


```python
# training images의 batch를 가져옵니다. 
dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # 이미지를 numpy로 변환해 표시합니다.

# batch내에서 image와 라벨을 plot합니다. 
fig = plt.figure(figsize=(25, 4))
# 20개의 이미지를 표시합니다.
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(classes[labels[idx]])
```


![png](https://drive.google.com/uc?id=1MamdW_8zrSeehsP5358iQjXCpfUXsooI)


## 4)View an Image in More Detail

이번에는 정규화된 RGB 색상 채널을 구분한 grayscale 이미지를 확인해보겠습니다.


```python
rgb_img = np.squeeze(images[3])
channels = ['red channel', 'green channel', 'blue channel']

fig = plt.figure(figsize = (36, 36)) 
for idx in np.arange(rgb_img.shape[0]):
    ax = fig.add_subplot(1, 3, idx + 1)
    img = rgb_img[idx]
    ax.imshow(img, cmap='gray')
    ax.set_title(channels[idx])
    width, height = img.shape
    thresh = img.max()/2.5
    for x in range(width):
        for y in range(height):
            val = round(img[x][y],2) if img[x][y] !=0 else 0
            ax.annotate(str(val), xy=(y,x),
                    horizontalalignment='center',
                    verticalalignment='center', size=8,
                    color='white' if img[x][y]<thresh else 'black')
```


![png](https://drive.google.com/uc?id=12pi_IjTJfkeLhuVBw6dDxxFJrE9UBnvT)


---
## 5)네트워크 [구조](http://pytorch.org/docs/stable/nn.html) 정의하기

CNN 네트워크의 아키텍처를 짜 보겠습니다. 이번에는 선형적이고, fully-connected인 MLP는 사용하지 않고 다음과 같은 것들을 사용합니다.
* [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), filtered images의 stack으로 생각해 볼 수 있습니다.
* [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), input data의 x-y size를 조정해앞선 layer에서 가장 _active_ 한 pixels 을 찾아냅니다.
* 보통 사용하는 Linear + Dropout layers는 overfitting을 막고 10차원의 결과를 얻기 위해서 사용하지 않습니다.

이 작업을 위해서는 선행 작업을 참고하면 좋습니다. [PyTorch classification 예시](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py) 나 [Keras를 사용한 더 복잡한 예시](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) 

## 6)Output volume for a convolutional layer

주어진 convolutional layer의 output size를 계산하기 위해서 [Stanford's cs231n course](http://cs231n.github.io/convolutional-networks/#layers)에서 소개된 다음과 같은 작업을 해 볼 수 있습니다.
>  input volume size (W), kernel/filter size (F), stride (S), zero padding used (P) 를 통해 다음과 같은 계산을 수행합니다. `(W−F+2P)/S+1`. 


```python
import torch.nn as nn
import torch.nn.functional as F

# CNN 아키텍처를 정의합니다.
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # convolutional layer (32x32x3 image tensor)
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        # convolutional layer (16x16x16 tensor)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        # convolutional layer (8x8x32 tensor)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        # max pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        # linear layer (64 * 4 * 4 -> 500)
        self.fc1 = nn.Linear(64 * 4 * 4, 500)
        # linear layer (500 -> 10)
        self.fc2 = nn.Linear(500, 10)
        # dropout layer (p=0.25)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # convolutional, max pooling layers 시퀸스를 추가합니다. 
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        # image input을 flatten 합니다.
        x = x.view(-1, 64 * 4 * 4)
        # dropout layer를 추가합니다.
        x = self.dropout(x)
        # hidden layer를 relu activation function와 함께 추가합니다.
        x = F.relu(self.fc1(x))
        # dropout layer를 추가합니다.
        x = self.dropout(x)
        # 두번째 hidden layer를 relu activation function와 함께 추가합니다.
        x = self.fc2(x)
        return x

# CNN 모델을 만듭니다.
model = Net()
print(model)

# CUDA가 사용가능하면, GPU를 사용해 훈련합니다.
if train_on_gpu:
    model.cuda()
```

    Net(
      (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (fc1): Linear(in_features=1024, out_features=500, bias=True)
      (fc2): Linear(in_features=500, out_features=10, bias=True)
      (dropout): Dropout(p=0.25)
    )
    

## 7)[Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) 과 [Optimizer](http://pytorch.org/docs/stable/optim.html) 정의하기

loss와 optimization function을 정의하는 것은 이번 프로젝트에서 중요합니다. [이 예시](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py)나 [Keras를 활용한 예시](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)를 참고해 보세요. 


```python
import torch.optim as optim

# loss function을 정의합니다.
criterion = nn.CrossEntropyLoss()

# optimizer를 정의합니다.
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

---
## 8)모델 훈련하기

validation loss에 집중하며 overfitting이 일어나지 않게 모델을 훈련해 봅시다. 


```python
n_epochs = 30

valid_loss_min = np.Inf # validation loss의 변화를 측정합니다. 

for epoch in range(1, n_epochs+1):

    # training과 validation loss를 초기화시킵니다.
    train_loss = 0.0
    valid_loss = 0.0
    
    ###############
    # 모델 훈련하기 #
    ###############
    model.train()
    for data, target in train_loader:
        # 가능할 경우GPU에서 훈련을 시킵니다. 
        if train_on_gpu:
            data, target = data.cuda(), target.cuda()
        # 모든 optimized 변수의 gradients를 제거합니다.
        optimizer.zero_grad()
        # forward pass
        output = model(data)
        # batch loss를 계산합니다. 
        loss = criterion(output, target)
        # backward pass
        loss.backward()
        # optimization을 수행합니다.
        optimizer.step()
        # training loss를 업데이트 합니다.
        train_loss += loss.item()*data.size(0)
        
    #################   
    # 모델 테스트하기 #
    #################
    model.eval()
    for data, target in valid_loader:
        # 가능할 경우GPU에서 훈련을 시킵니다. 
        if train_on_gpu:
            data, target = data.cuda(), target.cuda()
        # forward pass
        output = model(data)
        # batch loss를 계산합니다. 
        loss = criterion(output, target)
        # validation loss 평균을 업데이트합니다. 
        valid_loss += loss.item()*data.size(0)
    
    # average losses를 계산합니다.
    train_loss = train_loss/len(train_loader.sampler)
    valid_loss = valid_loss/len(valid_loader.sampler)
        
    # training/validation statistics을 출력합니다.
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
        epoch, train_loss, valid_loss))
    
    #  validation loss가 줄어들면 저장합니다. 
    if valid_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
        valid_loss_min,
        valid_loss))
        torch.save(model.state_dict(), 'model_cifar.pt')
        valid_loss_min = valid_loss
```

    Epoch: 1 	Training Loss: 2.066796 	Validation Loss: 1.761704
    Validation loss decreased (inf --> 1.761704).  Saving model ...
    Epoch: 2 	Training Loss: 1.647399 	Validation Loss: 1.501653
    Validation loss decreased (1.761704 --> 1.501653).  Saving model ...
    Epoch: 3 	Training Loss: 1.473001 	Validation Loss: 1.460549
    Validation loss decreased (1.501653 --> 1.460549).  Saving model ...
    Epoch: 4 	Training Loss: 1.353678 	Validation Loss: 1.261224
    Validation loss decreased (1.460549 --> 1.261224).  Saving model ...
    Epoch: 5 	Training Loss: 1.252662 	Validation Loss: 1.196791
    Validation loss decreased (1.261224 --> 1.196791).  Saving model ...
    Epoch: 6 	Training Loss: 1.167813 	Validation Loss: 1.193532
    Validation loss decreased (1.196791 --> 1.193532).  Saving model ...
    Epoch: 7 	Training Loss: 1.094948 	Validation Loss: 1.046578
    Validation loss decreased (1.193532 --> 1.046578).  Saving model ...
    Epoch: 8 	Training Loss: 1.036050 	Validation Loss: 0.988301
    Validation loss decreased (1.046578 --> 0.988301).  Saving model ...
    Epoch: 9 	Training Loss: 0.976925 	Validation Loss: 0.973023
    Validation loss decreased (0.988301 --> 0.973023).  Saving model ...
    Epoch: 10 	Training Loss: 0.927858 	Validation Loss: 0.929191
    Validation loss decreased (0.973023 --> 0.929191).  Saving model ...
    Epoch: 11 	Training Loss: 0.883305 	Validation Loss: 0.912895
    Validation loss decreased (0.929191 --> 0.912895).  Saving model ...
    Epoch: 12 	Training Loss: 0.839808 	Validation Loss: 0.893773
    Validation loss decreased (0.912895 --> 0.893773).  Saving model ...
    Epoch: 13 	Training Loss: 0.804758 	Validation Loss: 0.860323
    Validation loss decreased (0.893773 --> 0.860323).  Saving model ...
    Epoch: 14 	Training Loss: 0.770316 	Validation Loss: 0.851216
    Validation loss decreased (0.860323 --> 0.851216).  Saving model ...
    Epoch: 15 	Training Loss: 0.735329 	Validation Loss: 0.814373
    Validation loss decreased (0.851216 --> 0.814373).  Saving model ...
    Epoch: 16 	Training Loss: 0.702789 	Validation Loss: 0.823827
    Epoch: 17 	Training Loss: 0.679131 	Validation Loss: 0.802137
    Validation loss decreased (0.814373 --> 0.802137).  Saving model ...
    Epoch: 18 	Training Loss: 0.649355 	Validation Loss: 0.796728
    Validation loss decreased (0.802137 --> 0.796728).  Saving model ...
    Epoch: 19 	Training Loss: 0.626002 	Validation Loss: 0.773395
    Validation loss decreased (0.796728 --> 0.773395).  Saving model ...
    Epoch: 20 	Training Loss: 0.590561 	Validation Loss: 0.799329
    Epoch: 21 	Training Loss: 0.573112 	Validation Loss: 0.781874
    Epoch: 22 	Training Loss: 0.547988 	Validation Loss: 0.767140
    Validation loss decreased (0.773395 --> 0.767140).  Saving model ...
    Epoch: 23 	Training Loss: 0.519777 	Validation Loss: 0.759594
    Validation loss decreased (0.767140 --> 0.759594).  Saving model ...
    Epoch: 24 	Training Loss: 0.495779 	Validation Loss: 0.784253
    Epoch: 25 	Training Loss: 0.479645 	Validation Loss: 0.773368
    Epoch: 26 	Training Loss: 0.458019 	Validation Loss: 0.788102
    Epoch: 27 	Training Loss: 0.436813 	Validation Loss: 0.790022
    Epoch: 28 	Training Loss: 0.421288 	Validation Loss: 0.760412
    Epoch: 29 	Training Loss: 0.403445 	Validation Loss: 0.785253
    Epoch: 30 	Training Loss: 0.383578 	Validation Loss: 0.780198
    
## 9)가장 적은 Validation Loss를 가진 모델을 불러옵니다.

```python
model.load_state_dict(torch.load('model_cifar.pt'))
```

---
## 10)훈련이 끝난 네트워크를 테스트합니다.


```python
# test loss를 트래킹합니다.
test_loss = 0.0
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))

model.eval()
# test data에 반복해 적용합니다.
for data, target in test_loader:
    # 가능할 경우GPU에서 훈련을 시킵니다. 
    if train_on_gpu:
        data, target = data.cuda(), target.cuda()
    # forward pass
    output = model(data)
    # batch loss를 계산합니다.
    loss = criterion(output, target)
    # test loss를 업데이트 합니다.
    test_loss += loss.item()*data.size(0)
    # output을 예상 class의 확률로 바꿔줍니다.
    _, pred = torch.max(output, 1)    
    # 예측 결과와 실제 label을 비교합니다.
    correct_tensor = pred.eq(target.data.view_as(pred))
    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
    # 각각의 class에 대해 정확도를 측정합니다.
    for i in range(batch_size):
        label = target.data[i]
        class_correct[label] += correct[i].item()
        class_total[label] += 1

# test loss의 평균을 구합니다.
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))

for i in range(10):
    if class_total[i] > 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            classes[i], 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
    100. * np.sum(class_correct) / np.sum(class_total),
    np.sum(class_correct), np.sum(class_total)))
```

    Test Loss: 0.753790
    
    Test Accuracy of airplane: 81% (816/1000)
    Test Accuracy of automobile: 88% (881/1000)
    Test Accuracy of  bird: 62% (629/1000)
    Test Accuracy of   cat: 57% (570/1000)
    Test Accuracy of  deer: 66% (663/1000)
    Test Accuracy of   dog: 66% (666/1000)
    Test Accuracy of  frog: 84% (849/1000)
    Test Accuracy of horse: 77% (774/1000)
    Test Accuracy of  ship: 83% (839/1000)
    Test Accuracy of truck: 76% (764/1000)
    
    Test Accuracy (Overall): 74% (7451/10000)
    

### 11)Visualize Sample Test Results


```python
# test images의 batch를 하나 가져옵니다.
dataiter = iter(test_loader)
images, labels = dataiter.next()
images.numpy()

# 가능할 경우GPU에서 훈련을 시킵니다. 
if train_on_gpu:
    images = images.cuda()

# sample outputs을 가져옵니다.
output = model(images)
# output을 예상 class의 확률로 바꿔줍니다.
_, preds_tensor = torch.max(output, 1)
preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())

# 예시 이미지를 plot합니다.
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]),
                 color=("green" if preds[idx]==labels[idx].item() else "red"))
```


![png](https://drive.google.com/uc?id=1rqaXWZjwKhOQxYN1m-ZsUlAGesfXjNDD)

