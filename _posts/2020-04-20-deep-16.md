---
title: "[Deep learning] Nural Networks (4)Likelihood, Probabilities"
date: 2020-04-20
toc: true
toc_sticky: true
header:
  teaser: 
categories: python udacity study deeplearning ai nural network
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 4 Likelihood, Probabilities

우리가 만든 모델은 어떻게 평가할 수 있을까요? 어떤 모델이 다른 모델보다 더 좋다고 말하기 위해 무엇이 필요할까요?

이번에는 likelihood와 probability를 통해 모델의 성능을 측정하는 법에 대해 살펴 보겠습니다.


## 1) Maximum Likelihood

likelihood는 **가능도** 또는 **우도**로 번역되는 수학적 표현입니다. 확률과는 약간 다른 개념인데요.

수리적 개념에 대해 살펴보기 보다는 컨셉에 대해 살펴보도록 하겠습니다.

![](https://drive.google.com/uc?id=1Tifc3JfF2ELxh9pxzqq7AZV6eCLB1ZTF)

위와 같은 두 모델이 있다고 생각해 봅시다. 어느 모델이 더 성능이 좋을까요?

직관적으로 우측이 모든 points를 제대로 분류했기 때문에 성능이 좋다고 생각할 수 있습니다. 하지만 이를 숫자로 나타내 비교할 수는 없을까요?

위 그림에서 각 points 밑에는 확률값이 계산되어 있습니다. 해당 선형회귀에 의해 얼마나 잘 분류되었는지를 나타내는 것입니다.

잘 분류되어 있다면, 확률 역시 높을 것입니다. 그렇다면 확률들의 곱을 계산하면 각 모델의 성능을 평가할 수 있지 않을까요?

이런 아아디어에서 출발한 것이 바로 Maximum Likelihood입니다.


## 2)Cross Entropy

그렇다면 어떻게 확률 값을 최대화시킬 수 있을까요? Error Function과 확률값은 어떻게 연결되어 있을까요?

위의 모델에서는 데이터가 4개뿐이어서 계산이 용이합니다. 하지만 만약 데이터가 1,000개, 2,000개 등 많아진다면 어떻게 될까요?

모든 확률을 곱하기 보다는 더해서 계산하는 것이 더 효과적일 수 있을 것입니다. 그리고 우리는 **log**를 통해 이런 계산을 할 수 있습니다.

![](https://drive.google.com/uc?id=1WmVHcA-5i1eE6PnBGtY9Kc6wDqCdK2xi)

하지만 log를 씌우면 1보다 작은 값들은 모두 음수가 되어 버립니다. 이런 문제를 해결하기 위해 우리는 부호를 바꿔줄 수 있는데요.

이렇게 해서 모델의 성능을 측정하는 것을 **cross entropy**라고 합니다. 모델의 성능이 좋을수록, cross entropy는 낮아지는 것을 볼 수 있습니다.

일어날 확률이 높다면 cross entropy는 작고, 확률이 작다면 cross entropy는 높아지는 것입니다.

cross entropy에  대해 더 알아보기 위해서, 우리 앞에 3개의 문이 있다고 생각해 봅시다.

왼쪽문 뒤에 선물이 있을 확률은 0.8, 가운데는 0.7, 우측문은 0.1입니다. 이런 조건을 아래와 같은 표로 만들어 볼 수 있습니다.

![](https://drive.google.com/uc?id=1znhK0s-MUbR1l3V7CRELVDR-CIaWxwOF)

발생 확률이 높을 수록, cross entropy값은 작아지고, 반대의 경우에는 커지는 것을 알 수 있습니다. 이를 조금 더 정리해 보면

![](https://drive.google.com/uc?id=1YdvuQEigSDWEWveZyo8oF1DQZXjGXbs8)

yᵢ는 박스가 있을 경우는 '1'  없을 경우는 '0'입니다. 선물이 있을 경우의 확률과, 그렇지 못할 확률을 더하고 부호를 변환해 cross entropy를 계산해 볼 수 있습니다.

즉, 우측 하단부에 있는 두 가지 case중 위의 상황이 아래 상황보다 발생할 가능성이 높다는 것을 알 수 있습니다. 이를 파이썬으로 구현해 보면 다음과 같습니다.

```python
import numpy as np

# Y는 카테고리이고, P는 확률입니다. 

def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
```



## 3) Multi-Class Cross Entropy

다시 3개의 문이 있습니다. 그런데 이번에는 문 뒤에 선물이 있는 것이 아니라, 오리나 수달 물개 중 하나가 있습니다. 

이렇게 class가 3개 이상일 경우에는 어떻게 계산할 수 있을까요? 다음과 같습니다. 위에서 살펴본 것을 약간 확장해 적용한 것입니다. 

![nn29](https://drive.google.com/uc?id=1YdvuQEigSDWEWveZyo8oF1DQZXjGXbs8)


## 4) Error Function

이제 다시 error function으로 돌아올 차례입니다. 컨셉만 살펴봤던 지난번과는 다르게, 이번에는 수식으로 생각해 보겠습니다.

![nn30](https://drive.google.com/uc?id=1_ZDzDLWXlIWrTbeeoQOYxm6mKMe9IQ-P)

식을 살펴보면, y가 1일 때와 1이 아닐 때로 나누어, 각각의 예측값과 에러를 계산하는 방식에 대해 다루고 있습니다.

Error의 경우, y = 1일 때는 붉은 글씨로 써져있는 부분이 0이 되고, 반대의 경우에는 푸른 글씨로 써진 부분이 0이 되어서 계산합니다.

우리는 이런 값들의 총합을 계산함으로서 error function을 다룰 수 있습니다. 

역시 class가 3개인 경우에 대해서도 생각해 보면 다음과 같습니다. 식을 잘 살펴보면, 같은 표현이라는 것을 알 수 있습니다. 

![nn31](https://drive.google.com/uc?id=1KF8AVM34_LTuXtJ9ugZBkKUIyPZZARHl)

선형회귀식에서 error function은 다음과 같습니다.

![nn32](https://drive.google.com/uc?id=1QXwEI_zcCPyyy4MUYuLFapsKn3iPNTik)