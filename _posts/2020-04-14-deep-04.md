---
title: "[Deep learning] Introduction to Deep Learning (3.3) Flappy Bird"
date: 2020-04-14
toc: true
toc_sticky: true
header:
  teaser: https://drive.google.com/uc?id=1kIcvwUeQaiQrxfq_Kt1YBWxFF8w7k56f
categories: python udacity study deeplearning ai
---


*이 포스팅은 Udacity의 Bertelsmann Technology Scholarship을 정리한 자료입니다.*  


# Lession 3.3 Flappy Bird

## 1) Introduction  

이 프로그램의 세번째 프로젝트는 [Flappy Bird](https://flappybird.io/) 입니다.

사실 제가 Deep Learning, 더 나아가 강화학습에 매우 매력을 느꼈던 것 중 하나는 게임 때문이었습니다.

강화학습을 거친 벽돌깨기 게임을 보면서 재미난 것들을 만들 수 있겠다 싶어 공부해 보게 되었습니다.

![게임](https://drive.google.com/uc?id=1GZGIRYZwnGkR7aL6sa9RaPbdomsZclWp)

마침 이번에는 한 대 높은 난이도로 악명을 떨쳤던 [Flappy Bird 프로젝트](https://github.com/yenchenlin/DeepLearningFlappyBird) 를 리뷰하게 되었습니다. 

이 게임은 터치하거나 버튼을 눌러 새의 높이를 조정하며 장애물을 통과하는 플랫포머 게임인데요.

조작이 어렵기로 악명이 높았는데, 만약 컴퓨터가 조작하면 어떻게 될지 기대됩니다. 

![게임2](https://drive.google.com/uc?id=1kIcvwUeQaiQrxfq_Kt1YBWxFF8w7k56f)


## 2) 프로젝트 실행해보기

2.1)가상환경 만들기

오늘도 역시 시작하기 전에, 환경을 만들고 필요한 패키지를 설치해 보겠습니다. 다음과 같은 패키지가 필요합니다.

* opencv
* pygame
* tensorflow==0.12

```python
Mac/Linux : conda create -n flappybird python=2.7
Window : conda create -n flappybird python=3.5

conda activate flappybird

conda install opencv
pip install pygame
pip install tensorflow==0.12
```

2.2) 프로젝트 clone한 뒤 경로 이동, 실행 

그리고 github repo에 있는 flappybird 프로젝트를 clone하고 경로를 이동합니다.

이동한 경로에서 파이썬 프로그램을 실행하면, flappybird가 죽지 않고 계속 나는 것을 볼 수 있습니다.

```python
git clone https://github.com/yenchenlin/DeepLearningFlappyBird.git
cd DeepLearningFlappyBird
python deep_q_network.py

```

![게임3](https://drive.google.com/uc?id=1zIAGYMS8pzTu7LbnA-T-UGJRf9N9vfjB)

udacity의 강좌는 여기까지지만, 프로젝트를 조금 더 이해해보도록 하겠습니다.
모든 내용은 [프로젝트 repo](https://github.com/yenchenlin/DeepLearningFlappyBird) 에서 따 왔습니다.


## 3) Deep Q Network 

Deep Q Network(이하 DQN)은 일종의 CNN입니다. 픽셀들을 input으로 받아, 

미래의 리워드를 측정하는 value function을 output으로 도출합니다. 

DQN의 pseudo code

```python
Initialize replay memory D to size N
Initialize action-value function Q with random weights
for episode = 1, M do
    Initialize state s_1
    for t = 1, T do
        With probability ϵ select random action a_t
        otherwise select a_t=max_a  Q(s_t,a; θ_i)
        Execute action a_t in emulator and observe r_t and s_(t+1)
        Store transition (s_t,a_t,r_t,s_(t+1)) in D
        Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D
        Set y_j:=
            r_j for terminal s_(j+1)
            r_j+γ*max_(a^' )  Q(s_(j+1),a'; θ_i) for non-terminal s_(j+1)
        Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^2 with respect to θ
    end for
end for
```

네트워크의 아키텍처는 다음과 같습니다.

* 3.1) 이미지를 Grayscale로 바꾼다.
* 3.2) 이미지를 80 x 80으로 리스케일링한다.
* 3.3) 마지막 4개의 프레임을 쌓아 네트워크에 input으로 넣을 80 x 80 x 4의 입력 어레이를 생성한다.

![네트워크](https://drive.google.com/uc?id=13zyMr6_jQfpAmhaEu0ea7o-R33hvG1VN)

최종적으로 출력되는 레이어는 게임에서 수행할 수 있는 액션의 수와 차원이 동일합니다. 

각 스탭에서 네트워크는 ϵ greedy policy를 활용해 가장 높은 Q value에 해당하는 작업을 수행합니다.  


## 4) 마치며

더 코드를 뜯어보며, 다른 곳에 어떻게 활용할 수 있는지 생각해 봐야겠습니다.
머신러닝을 통해 깰 수 없을 것 같이 보이는 다양한 게임을 깨 보면 재밌지 않을까요?

udacity에서는 다음과 같은 책도 추천하고 있습니다.

* [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)
* [Neural Networks And Deep Learning](http://neuralnetworksanddeeplearning.com/)
* [The Deep Learning Textbook ](http://www.deeplearningbook.org/)